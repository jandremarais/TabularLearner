# Deep Learning \label{chp:dl}

## Introduction

Deep learning is a broadly used term.
To many, the difference between a classical neural network and a deep neural network lies merely in the number of network layers.
Even a network with two hidden layers is sometimes referred to as 'deep'.
From this perspective, deep (multilayer) neural networks were already proposed by Alexey Ivakhnenko in 1965 (reference needed here).
Still the boom in deep learning research may be said to have started only a bit more than a decade ago, since effective training of a multi-layer network was only made possible after a breakthrough presented in the paper by Hinton *et al.* (2006) (edit the reference).

From that time, tons of contributions have been made to the neural network field. 
New types of layers, more effective training strategies, and novel approaches to guard against overfitting have been proposed.
A more general definition of *deep learning* therefore encompasses all modern developments in the neural network space.
Because of the large amount of deep learning contributions during the past decade, note that in this chapter we restrict attention to developments that form a basis for deep learning in the context of tabular data.

Conceptual categorisation of modern contributions to the deep learning field is a difficult task.
Perhaps one way to order some of the more important developments, is to use the aim or effect of their implementation as criterion.
Since representation learning is such an important characteristic of NNs, many deep learning developments focus on this aspect. 
As mentioned before, neural networks are able to approximate any function.
However, learning algorithms are not necessarily able to find these solutions, therefore we need more efficient ways to learn from the available data. 
An example is the class of neural networks called *autoencoders* (\Sref{sec:ae}).
These neural networks are mostly used as unsupervised learning methods with the aim of aiding us to learn more robust data representations, which may subsequently be transferred to supervised learning algorithms.
We have mentioned in Chapter 1 that the process of transferring knowledge from one network to another is called transfer learning. Note that more detail regarding transfer learning is provided in \Sref{sec:tl}.



More regularisation:

Since practically all deep learning developments are (either explicitly or implicitly) geared toward preventing an overfit, much of the remainder of the chapter following  is devoted to proposals that fall in this category.
In (\Sref{sec:dropout}) and (\Sref{sec:bnorm}) we consider the use of layers to facilitate regularisation.
More specifically, in (\Sref{sec:dropout}) we discuss the so-called *dropout* method, whereas (\Sref{sec:bnorm}) is devoted to the process of *batch normalisation* .

The use of different types of layers provides an alternative way of making deeper neural networks more effective, facilitating a focus on the more important set of features, and to enable NNs to process discrete input. 
Hence note that the use of *skip-connections* , *attention modules*  and *embedding layers* are discussed in (\Sref{sec:resnet}), (\Sref{sec:attention}) and (\Sref{sec:embeds}).

Turning our attention to the training data at hand, note that one may consider artificially enlarging the training dataset in order to avoid an overfit. 
This approach, called *data augmentation* is described in \Sref{sec:daug}.
With regard to optimisation of the neural network, a method has been proposed which provides a way of automatically finding a good learning rate.
This leads to a drastic reduction in the number of training iterations needed.
The aforementioned training method (referred to as the *one-cycle policy*) is discussed in (\Sref{sec:onecyc}).

We conclude the chapter with a discussion on the interpretability of neural networks in (\Sref{sec:interp}).

## Autoencoders \label{sec:ae}

The basic autoencoder is a neural network which is trained to attempt to reconstruct its inputs.
The simplest form of an autoencoder is a neural network with one hidden layer and with an output layer having the same size as the input layer. 
This architecture is illustrated in \autoref{fig:ae}).
The linear layer transforming the input to the hidden layer is referred to as the encoder, *viz.* $\boldsymbol{z}=f(\boldsymbol{x})$, and the layer producing the output from the hidden layer is called the decoder, *viz.* $\boldsymbol{x}'=g(\boldsymbol{z})$.
The autoencoder can be trained in the same way as any other neural network, however the loss function to be minimised is called *reconstruction loss*. 
The reconstruction loss function measures the difference between the reconstructed and actual input, hence the MSE loss function is a common choice in the case of continuous data.

```{r, fig.cap="A simple single hidden layer autoencoder with four-dimensional inputs and with two neurons in the hidden layer.\\label{fig:ae}", out.height="30%", fig.align='center'}
knitr::include_graphics('figures/ae.png')
```
Technically, autoencoders belong to the self- (or semi-) supervised class of methods, although many still think of it as unsupervised.
It is unsupervised in the sense that it does not require labelling, but it is supervised in the sense that it does predict an ouput.

If the number of neurons in the hidden layer of an autoencoder is greater than or equal to the number of input features, it is possible  to perfectly reconstruct $\boldsymbol{x}$ from $\boldsymbol{z}$, *i.e.* $\boldsymbol{x}'=\boldsymbol{x}$.
This is however not a very useful model.
Instead, autoencoders are typically built with some type of constraint imposed.
A common option is to restrict the number of neurons in the hidden layer to be smaller than the number of input features.
This forces the autoencoder to capture only the most useful properties of the data in the hidden representation and can thus effectively be used as a way of dimensionality reduction [@Hinton2006a].
Indeed, if there are no non-linear activation functions after the linear layers, one can show that this autoencoder is equivalent to application of *Principal Component Analysis* (PCA) to the inputs.

There is no restriction to the number or the size of the layers used in the encoder and decoder, and if activation functions are used, one can potentially learn a more powerful non-linear generalisation of PCA.
However, if the encoder and decoder networks are allowed too much capacity, the autoencoder may reconstruct the inputs without learning any useful properties of the data.
It is difficult to verify whether or not an autoencoder has learned a useful latent representation of the data.
One way to evaluate the representation is to use the features extracted by the encoder in a supervised learning task and to then compare its performance to that of a model using only the raw data as inputs.
Autoencoders have also been used to initialise the weights of a supervised learning network.
That is, the learned weight of the autoencoder are use to initialise the weights of the supervised learning network (of the same size) [@Larochelle2009].

An autoencoder can learn to perfectly copy the input if the encoder and decoder have enough capacity, even though the latent representation is of a smaller diemsion than the inputs, and thus will not learn useful features of the data.
Therefore we may want to consider other types of contraints to impose.

A *denoising autoencoder* (DAE) [@Vincent2008] first adds noise to its inputs before it is passed to the encoder.
Thus in order to minimise the reconstruction loss, the DAE should learn how to reconstruct the original input from a corrupted version of itself.
[@Alain2014] shows that DAEs are able to learn useful structures of the data.
The choice of the type of noise added to the inputs depends on the data types.
One may block out inputs with zeros if zeros have no other meaning in the data, or one might want to add Gaussian noise to continuous outputs as long as it falls within the true range of the features.
If  too much noise is added in the corruption step, the DAE might not learn anything useful.
The type and amount of noise to be used are factors that practitioners typically experiment with.

Another type of autoencoder is a *Variational Autoencoder* (VAE) [@Kingma2013].
Instead of encoding the data into a latent vector representation, a VAE encodes the data as a Gaussian distribution.
In the decoding step, observations are then sampled from the learned distribution before passing it to the fully connected layers.
This is an interesting direction for future research but not in the scope of this work.

## Pretraining and Transfer Learning \label{sec:tl}

Unsupervised learning played a key part in the rise of deep learning (debatedly starting with [@Hinton2006]).
It made it easier to train deeper neural networks with a process called *pretraining*.
Pretraining is based on the assumption that representations learned from one task can sometimes be useful for accomplishing other tasks in the same input domain.
For example, we can use a DAE to learn the structure of the data from unlabelled data and then use what it has learned as features or initialisation for a supervised learning task with the same type of inputs.
We can also do supervised pretraining by first training the network to estimate one target variable and then use those weights or features when training to estimate a different target variable, for example.

Unsupervised pretraining for supervised learning is very common in NLP [@Devlin2018, @Howard2018] and supervised pretraining for supserivsed learning is very common in computer vision [@Yosinski2015, @He2015].
It is not yet clear theoretically why pretraining works.
It may be that using the pretrained weights as initialisation to the supervised model provides a better starting position on the loss surface and thus have regulartory effects [@Goodfellow2016, Ch. 14].
Pretraining is most effective when there is little data available for the supervised task but a lot of data available for the pretraining task.

The process of transferring what is learned doing one task (for example in pretraining) to another task (supervised learning for examples) is called *transfer learning*. When done effectively, pretraining and transfer learning can together dramatically reduce the number of training samples and computational power need to train a model.

[@Zeiler2014] provides an insigtful investigation of the what a CNN learns when trained on image data. 
From observing the types of features extracted for a trained image model (\autoref{fig:zeil}) it becomes clear why they are also effective on other image datasets. 
The learned filters seemed to look for generic image features like edges and color gradients which are useful for most computer vision tasks.
[@Howard2018] describes a good practice when doing transfer learning via gradual unfreezing and discriminative fine-tuning.

```{r, fig.cap="Visualising the first layer convolutional filters leared by a neural network in a large image dataset. \\label{fig:zeil}", out.height="50%", fig.align = "center"}
knitr::include_graphics("figures/zeiler_filters.jpeg")
```

## More Regularisation

In \Sref{sec:basicreg} we discussed basic regularisation methods for neural networks.
There are many other ways of combatting overfitting.
The following section discusses two regularisation techniques proven to be extremely powerful in almost every application of deep learning.

### Dropout \label{sec:dropout}

*Dropout* consists of randomly setting the output of a hidden neuron to zero with probability $p$.
See \autoref{fig:dropout} for an illustration of how dropout effects the connections between neurons.
The neurons which are set to zero do not contribute to the forward pass and do not participate in backpropogation. 
Every time an input is presented, the neural network samples a different set of neurons to be dropped out. 

This technique ensures that a neuron does not rely on the signals of a particular set of other neurons,
It is therefore forced to learn more robust features that are useful in conjunction with many different random subsets of the other units [@Krizhevsky2012].

In the original paper they used $p=0.5$ but the optimal selection will depend on the use case and is typically found by experimentation.
At test time, no units are dropped out and their output is multiplied by $1-p$ to compensate for the fact that all of the units are now active. 
Dropout does tremendously well to combat overfitting, but it slows down the covergence time of training.

There are also parallels to be drawn between dropout and ensembling approaches [@Hinton2012].
Since, at each training iteration a unique set of neurons are active which may then each be viewed as unique models.
During training these models are combined - similar to the process of ensembling models.

```{r, fig.align="center", fig.cap="The effect dropout has on connections between neurons.\\label{fig:dropout}", out.width="80%"}
knitr::include_graphics('figures/dropout.png')
```


### Data Augmentation \label{sec:daug}

Recall that our aim with predictive models is to generalise well to an unseen test set.
In an ideal world we would train a model on all possible variations of the data to capture all interactions and relationships.
This is not possible in the real world.
Such a dataset is not available and would be infinitely large.
And even if it were, machine learning will then be unecessary since all the possible observations are observable and one could simply use a lookup or a search when predicting outcomes.

In reality we have a finite subset of the full data distribution to train on.
Any new samples with unique feature combinations will likely improve the models generalisablity.
If the collection of new samples is not available, we can try to artificially create more through *data augmentation*.

This is a standard approach especially in computer vision applications. For example, from a single image, we can rotate it, flip it horizontally, shift it in any direction, crop it, and many other transformations without destroying the semantic content of the image.
But by doing so we are artificially increasing the size of the training set to help with overfitting.
In \autoref{fig:aug} we see how a single image of a cat is turned into eight images by doing random transformations of the original image.
In all of these images it is still possible to tell that it is an image of a cat.
Of course this is not as effective as genuine new data samples, but it is a very effective and efficient substitute [@Perez2017]. 
Data augmentation consistently leads to improved generalisation.

```{r, fig.cap="Example data augmentations for images.\\label{fig:aug}", out.width="80%", fig.align = "center"}
knitr::include_graphics('figures/augmentations.png')
```

Data augmentation can be formalised by the *Vicinal Risk Minimisation* principle [@Chapelle2001] where human knowledge is required to describe a vicinity around each observation in the training data so that artificial examples can be drawn from the vicinity distribution of the training sample to enlarge it.
In image classification one can define the vicinity of an image as the set of its horizontal reflections and minor rotations, for example.

Note that data augmentation is dataset and domain dependent and that the augmentations should preserve the semantic content or the signal in the observation.
For example, taking too small crops of an image will ignore the context and may make it impossible to tell what is inside.
And augmentations that are suitable for image data does not necessarily make sense for text data.

## Modern Architectures

In the following sections we highlight some of the recently developed neural network layers and modules. 
These operations were designed to make training more robust and efficient, to help learn more robust representations and be able to model more useful features, among others.

### Normalisation \label{sec:bnorm}

One of the things that complicate the training of neural networks is the fact that hidden layers have to adapt to the continuously changing distribution of its inputs. 
The inputs to each layer are affected by the paramaters of all its preceding layers and a small change in a preceding layer can lead to a much bigger difference in output as the network becomes deeper. 
When the input distribution to a learning system changes, it is said to experience covariate shift [@Shimodaira2000].

Using ReLUs, carefull weight initialisation and small learning rates can help a network to deal with the internal covariate shift. 
However, a more effective way would be to ensure that the distribution of non-linearity inputs remains more stable while training the network.
[@Ioffe2015] proposed *batch normalisation* to do just that.

A batch normalisation layer normalises its inputs to a fixed mean and variance (similar to how the inputs of the network is normalised) and therefore it can be applied before any hidden layer in a network to prevent internal covariate shift. 
The addition of this layer dramatically accelerates the training of deep neural networks, also because it can be used with higher learning rates. 
It also helps with regularisation [@Ioffe2015], therefore in some cases dropout is not necessary.

The batch normalising transform over a batch of univariate inputs, $x_{1}, \dots,x_{n}$ is done by the following steps:

1. Calculate the mini-batch mean, $\mu$, and variance, $\sigma^{2}$:

    $$
    \begin{aligned}
    \mu &= \frac{1}{n}\sum_{i=1}^{n}x_{i}\\
    \sigma^{2}&=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-u)^{2}
    \end{aligned}
    $$

2. Normalise the inputs, 

    $$
    \hat{x}_{i} = \frac{x_{i}-\mu}{\sqrt{\sigma^{2}+\epsilon}},
    $$
    where $\epsilon$ is a constant to ensure numerical stability.

3. Scale and shift the values,

    $$
    y_{i}=\gamma\hat{x}_{i}+\beta,
    $$
    where $\gamma$ and $\beta$ are the only two learnable paramaters of a batch normalisation layer.

The reason for the scale and shift step is to allow the layer to represent the identity transform if the normalised inputs are not suitable for the following layer, *i.e.* the scale and shift step will reverse the normalisation step if $\gamma=\sqrt{\sigma^{2}+\epsilon}$ and $\beta=\mu$. Batch normalisation has now become a standard when training deep CNNs.

### Skip-connections \label{sec:resnet}

Residual Networks became very popular after it was used to win one of the ImageNet competiions [@He2015].
The residual connection layer can simply be formalised as
$$
y=F(x)+x,
$$
*i.e.* combining the input to the layer(s) with the output of the layer(s).
Here, the combination is by addition, but other ways can also be used, like multiplication or concatenation.
The idea is also illustrated in \autoref{fig:skip_conn}.

```{r, fig.align="center", fig.cap="Diagram conceptualising a skip-connection.\\label{fig:skip_conn}", out.width="80%"}
knitr::include_graphics('figures/skip_conn.png')
```

The DenseNet [@Huang2016] is another well-known network making use of skip connections.
In a DenseNet each layer is connected to every other layer in the network.
There are multiple benefits to using skip-connections: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and may reduce the number of parameters required [@Huang2016].
These benefits show in the empirical evaluations.
It is interesting to note that one can draw a parallel between ResNets and boosting methods since both are approaches to fitting a model to the residual [@Huang2017].

### Embeddings \label{sec:embeds}

An embedding is a layer that maps a discrete input to a numeric vector representation.
It was first used in NLP in order to represent words as numbers so that it can be processed by a numeric model.
For instance the word "woman" may be represented by the vector $[1,3,5]$ and the word "man" by $[2,4,6]$.
The goal is to map discrete inputs to a meaningful vector space where items with similar meaning exist close to each other.
In contrast to using a one-hot encoded representation of words, where all words equally far apart.
See \autoref{fig:emb} for an illustration of a such space[^emb_src].

```{r, fig.cap='Learned word embeddings in a 2-dimensional space.\\label{fig:emb}', fig.align="center", out.width="90%"}
knitr::include_graphics('figures/embeds.png')
```

[^emb_src]: https://www.shanelynn.ie/get-busy-with-word-embeddings-introduction/

Initially the mappings were configured independenty of the neural network with approaches based on co-occurences [@Mikolov2013].
The real breakthrough came when the mappings were defined as learnable layers in the network (as done in all of the latest NLP research [@Howard2018, @Devlin2018]).
Thus they can be tuned just like any other parameter in the network.
The parameters of the embedding function (or layer) are first randomly initialised and then gets tuned along with the rest of the neural network during training.

An embedding operation can either be viewed as a table lookup or a matrix multiplication of the discrete input in a one-hot encoded form, *i.e.* $\boldsymbol{e}=W\boldsymbol{x}$ where $\boldsymbol{x}$ is a discrete input in one-hot encoded form and $W:k\times p$ is the matrix containing the embedding where $p$ is the number discrete categories and $k$ is the embedding size.
The embedding layer can be reused by all input features with the same input type, making it more efficient and reducing the memory footprint of the model.

### Attention \label{sec:attention}

Attention is one of the standout breakthroughs made in deep learning in recent times; especially playing an integral part to NLP's success and other sequence related tasks [@Vaswani2017, @Devlin2018].
It was first popularised in neural machine translation [@Bahdanau2014] and now almost used ubiquitously in natural language processing applications.
It was also found useful in computer vision applications, like image captioning [@Xu2015], and in audio processing [@Duong2016].

The main idea of an attention module is to force a layer to only focus on a certain subset of its inputs at different stages of computation.
For example, in image captioning, one may use a RNN to sequentially output words describing the image.
With the use of an attention model, the network is restricted to only look at certain parts of an image at every step, avoiding having to look at the full image everytime.
This is illustrated in \autoref{fig:imattn}.
Notice how the network focusses on the bird part of the image when predicting the word "bird" and "flying" and then focussing on the water part of the image when predicting words like "body" and "water".
Similarly, when used in machine translation, we can visualise the attention weights to see how the network focusses on a different subset of words for predicting each word in the target language (\autoref{fig:nlpattn}).

```{r, fig.cap="Attention applied to image captioning.\\label{fig:imattn}", out.width="90%", fig.align='center'}
knitr::include_graphics('figures/img_attention.jpeg')
```

```{r, fig.cap="Attention applied to machine translation.\\label{fig:nlpattn}", out.width="90%", fig.align='center'}
knitr::include_graphics('figures/word_attn.png')
```

In an ideal world, an attention module should not be necessary and we would want the network to learn if certain parts of images are not required to look at.
However, currently this built-in prior proves to be extremely helpful with current learning algorithms.

The core of an attention module can be summarised with these equations:
$$
\begin{aligned}
\boldsymbol{z}&=f(\boldsymbol{x})\\
\boldsymbol{\alpha} &=\text{softmax}(f_{att}(\boldsymbol{x}))\\
\boldsymbol{y}&=\boldsymbol{z}\otimes\boldsymbol{\alpha}
\end{aligned}
$$
$\boldsymbol{z}$ it the ordinary activations produced by a layer $f$ given input $\boldsymbol{x}$.
$\boldsymbol{\alpha}$ are the weights produced by the attention layer, $f_{att}$ after a logit transform so that the weights sum to one.
The output of the attention module, $\boldsymbol{y}$, is then an elementwise multiplication, $\otimes$, of $\boldsymbol{z}$ and $\boldsymbol{\alpha}$, however other combinatorial operations can also be used.
More detail can be found in [@Xu2015].
We will elaborate on the concepts of *Self-Attention* [@Cheng2016a] and *Multi-Head Attention* [@Vaswani2017] in \Cref{sec:tab_att}.

## Super-Convergence \label{sec:onecyc}

The process of setting the hyperparameters for learning algorithms are hard.
It requires expertise, extensive trial and error and is more of an art form than a science.
There are many hyperparameters to consider: learning rate, batch size, momentum, and weight decay, to name a few.
Although the right parameters make a huge difference in training time and performance, there are no simple and easy ways to find them.
Doing a grid search or a random search in the parameter space is computationally very expensive.

In this section we summarise the work done in [@Smith2018] to give guidance on finding and setting the learning rate, batch size and weight decay.
The approach suggested is based on finding the right balance between overfitting and underfitting by using the right hyperparameters.
By looking at the validation loss at early stages of training, one will find clues to which parameters are optimal.
This eliminates the necessity of running complete grid or random searches.

By following the hyperparameter selection approach discussed next, one can achieve tremendous improvements in training time and accuracy.
If done correctly one may exploit a property called *super-convergence* [@Smith2017], where neural networks can be trained an order of magnitude faster than with standard training methods.
\autoref{fig:supercon} plots the test loss over each training iteration of the proposed hyperparameter strategy compared to a fixed learning rate policy.
We see that with the proposed method, the model achieves a better accuracy than the standard approach and in eigthth of the training iterations.
Next we discuss how to select hyperparameters to achieve super-convergence.

![Reduced training iterations and improved performance with super-convergence principle.\label{fig:supercon}](figures/supercon.png)

Firstly note that the effects of the learning rate, batch size and other regularisation techinques, like weight decay, are tightly coupled and should be tuned together.
We start with setting the learning rate.
In \Cref{chp:nn} we showed that the learning rate is an important parameter and that one can either keep it fixed during training or decrease it by some factor when it looks necessary.
[@Smith15] suggests using *cyclical learning rates* (CLR).
A CLR cycle consists of two stages, one where the learning rate linearly increases during training up until a certain maximum and one where the learning rate decreases linearly back to a chosen minimum.
One needs to specify the number of epochs in each stage, also known as the step size and the minimum and maximum learning rate.
One reason why CLR is believed to work is that variable learning rate allows the model to better dodge saddle points on the loss surface.

In his most reason work, [@Smith2018], the author suggest using only a single cycle of CLR, called the *1cycle* policy.
The learning rate setting of the 1cycle policy is illustrated in \autoref{fig:onecyc}.

![The learning rate schedule of the 1Cycle policy.\label{fig:onecyc}](figures/onecyc.png)

Clearly one also needs to set the bounds of the learning rate, however luckily, the author designed a learning rate range test to help one do just that.
In the learning rate range test, training starts with a small learning rate which is slowly increased linearly throughout a pre-training run.
Typically the training loss will slowly start to decrease as the learning rate increases, up until a certain point at which the learning rate becomes to large and the loss starts to increase.
The learning rate at this point is the largest learning rate one can use during training.
The lower bound of the learning rate can be set to a factor of 10 times less than the max.
See \autoref{fig:range_test} for example output of the range test and how to deterimin the learning rate bounds for the 1Cycle policy.
One should be careful of too small a stepsize since that increases the rate of the learning rate increase which might make the training process unstable.

![An example output of a learning rate range test.\label{fig:range_test}](figures/range_test.png)

Note that large learning rates also acts as a form regularisation [@Smith15].
Thus if one uses large learning rate, one might want to reduce some of the other regularisation controls.
Along with the learning rate [@Smith2018] suggests and shows that larger batch sizes allow for training with larger learning rates and thus convergence can be reached quicker.
So when setting the batch size one would choose the largest possible that fits into memory.

The weight decay is another important value to be set properly along with the 1Cycle policy.
Since it is a regularisation parameter it is important that it is in balance with the learning rate.
The proposed way of setting the weight decay is through a grid search.
Once can do a few short training runs at differnt weight decays and see if the validation loss gives any indication of which one is better.
Another option is to test different weight decays during the learning rate range test and see how they behave.
Then we will look for the weigth decay that produces the most stable loss and allows the use of the largest learning rate.
In our experiments we follow these suggestions to find the best hyperparamters for each model and dataset.

## Model Interpretation \label{sec:interp}

Although Deep Learning is now the state-of-the-art for many machine learning tasks, it is still trailing behind other algorithms in terms of model interpretability.
But keep in mind this is not an unusual trade-off; between prediction performance and model interpretability.
Deep neural networks are occasionally referred to as "black boxes" since it is very difficult to interpret what is going on inside the stacks of linear and non-linear layers.
This is one of deep learning's greatest criticisms and is a large reason why it cannot be used in some production environments. 
For example, in the clinical domain, model transparency is of utmost importance, given that predictions might be used to affect real-world medical decision-making and patient treatments [@Shickel2017].
Fortunately, some work has been done to gain insights from neural networks.
We discuss the topic briefly in the following sections.

### Neural Network Specific \label{sec:interp_ns}

We have showed in \Sref{sec:rep} that it is possible to inspect activations and weights of layers at different levels of the network.
If the network is small, one might gain insight to what the network has learned or why it is making certain decisions.
However, most useful neural networks are at least three layers deep, making its activations and weights more complex to interpret.

When fully convolutional networks are used, there are ways to visualise which parts of the inputs were important in making a certain decision.
These visualisations are called class activation maps [@Zhou2016] ([@Selvaraju2017] suggests a similar approach).
But they cannot be used with fully connected layers and only with fully convolutional networks.

Another common interpretation tool, in order to gain insight into what specific neurons are looking for, is to rank the inputs by the magnitude of their activation at that neuron.
Then, if one would be able to manually spot similarities between the highest ranked inputs, one would have a potential description of the pattern that triggers that neuron.
This approach is called *Activation Maximisation* [@Erhan2009].
This process is manual and not guarranteed to produce useful insights, especially when there are many neurons to investigate.

An interesting take on model interpretation is shown in [@Frosst2017] and [@Che2016].
Their ideas are based on the process of *knowledge distillation* [@Hinton2015] and leverages the fact that decision trees are easier to interpret.
Knowledge distillation is the process of transferring the knowledge from one model (or an ensemble of models) to another model by training the target model to estimate the predictions of the source model (or ensemble).
[@Frosst2017] and [@Che2016] used a soft-decision tree and boosted trees respectively to learn the mapping between the input and the neural network predictions.
Once the tree based methods are trained, the usual intepretation tools of tree based models, like feature importance of evaluating how a sample traverses the tree, can be used to attempt to understand what the neural network is doing.
It is interesting that in both publications the authors note that the tree based models trained on the neural network predictions achieved a better performance that the ones trained on the actual targets - indicating the value knowledge distillation can add.

### Model Agnostic \label{sec:interp_ma}

Besides interpretation tools specifically designed for neural networks, one can also make use of model agnostic tools, *i.e.* ones that can be used with any machine learning model.
*The Permutation importance algorithm* computes feature importance scores for each input feature.
The importance measure of a feature is deterimend by the model's sensitivity to a random permutation of the values of that feature.
The expectation is that when a feature with a strong signal is shuffled before passed to the model, the performance of the model will drop significantly.
On the other hand, if a feature has little effect on the target predictions, shuffling its values will not have a major impact on the model's performance.

This technique was first introduced by [@Breiman2001] for the Random Forest algorithm but can also be generalised to other models.
The steps for calculating the feature importances by the permuation importance algorithm are as follows:

1. Train a neural network on a dataset with $p$ input features.
2. Evaluate the network on a validation set to obtain a performance metric $m_{0}$.
3. For each of the $p$ input features, do:
    - create a copy of the validation set and randomly shuffle the feature in this copy.
    - Evaluate the neural network on this version of the validation set to obtain $m_{j}$, $j=1,\dots,p$.
4. Rank the features based on $m_{j}-m_{0}$ (if a bigger $m$ is better).

Permutation importance only produces sensical results on the assumption that the features are independent.
Permuting the feature independently creates examples that never occurr in real life and the importance of features in that invalid space may be misleading.
The test however can still be useful to identify inputs that are not important, *i.e.* not used by the model.
If randomly permuting a feature does not affect the model performance at all, it may be a good indication that the model is not dependent on it.

*Partial Dependence Plots* can be used to visaulise the relationship between input features and the target.
This is another post-training interpretation tool and was first used in [@Friedman2001].
Once a neural network is trained, we can evaluate what effect a change in any input feature has on a single prediction, by observing the change in the prediction.
Say for example we want to see how feature $X_{1}$ influences the model prediction.
By taking a single observation from the data we can evaluate how the model prediction changes by changing the value of $X_{1}$ to other possible values of $X_{1}$.
And since this behaviour will most like vary for different observations, this process should be repeated for a subset of observations from the dataset.
The average effect on the predictions at different values of $X_{1}$ can then be calculated along with standard errors of these effects.

Other model agnostic interpretation tools, like SHAP values [@Lundberg2017], are also available.
Here we only discussed the basic approaches.
In the next two chapters we will show examples of how these model interpretation techniques can be implemented.
