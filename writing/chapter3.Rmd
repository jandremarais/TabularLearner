# Deep Learning \label{chp:dl}

## Introduction
- Modern Neural Network Approaches
- story of combatting overfitting
- Recent advancements in deep learning which could be useful to applying in tabular data

## Autoeoncdoers


An autoencoder takes an input and first transforms it into some (smaller) latent representation using the part of the network called the encoder.
From the latent representation the second part of the network, called the decoder, tries to reconstruct the input by doing some transformation.
Both the encoder and the decoder networks are NNs in their own right and thus usually consist of either fully conncected layers or convolutional layers (or both).

During training a reconstruction loss is minimised.
A reconstruction loss measures the distance between the reconstruction of the input based on the latent representation and the actual input.

Autoencoders technically belong the self (or semi) supervised class of methods, although many still think of it as unsupervised.
It is unsupervised in the sense that it does not require labelling, but it is stll supervised in the sense that it predicts an ouput; the input and thus self-supervised.

A denoising autoencoder (DAE) is a variant of the vanilla autoencoder.
A DAE also learns to reconstruct the input vector, but in this case from a corrupted version thereof.
So during training, before an input is sent through the encoder, it first get injected with random noise.
However, the output of the decoder is still being compared to the original input and thus the DAE is supposed to learn how to remove noise from the input - therefore, denoising.

[@Miotto2016] used a stacked denoising autoencoder to learn patient representations from EHR data.
They found that these representations were useful features in predicting future health states of patients.
By using these learned representations as input significantly improved the performance of predictive models compared to those only using the raw inputs.

See also [@Vincent2008].

https://arxiv.org/pdf/1803.09820.pdf

## Combatting Overfitting

### Transfer Learning

The major critique against DNNs are that they require a huge amount of training data and that they take extremely long to train. 
This is somewhat true, however, *transfer learning* provides an effective solution to these problems. 
Recall that DNNs are examples of representation learning algorithms. 
Consider the case where a CNN was sucessfully trained on ImageNet.
For any input image, each layer of the CNN produces some feature representation of the input image. 
(Not sure where Zeiler paper is going to be discussed). 

### Dropout

Overfitting can be reduced by using dropout [@Hinton2012] to prevent complex co-adaptions on the training data. Dropout consists of setting the output of a hidden unit to zero with a probability $p$ (in the original paper they used $p=0.5$). The units which are set to zero do not contribute to the forward pass and do not participate in backpropogation. Every time an input is presented, the neural network samples a different set of units to be dropped out. 

This technique ensures that a unit does not rely on the presence of a particular set of other units. It is therefore forced to learn more robust features that are useful in conjunction with many different random subsets of the other units [@Krizhevsky2012].

At test time, no units are dropped out and their output is multiplied by $1-p$ (make sure) to compensate for the fact that all of the units are now active. Dropout does tremendously well to combat overfitting, but it slows down the covergence time of training.

+ in the original paper they also compare the technique to ensebmling

Interesetingly, [@haldar2018] found that dropout was not effective in their application.
They pinned it down to dropout producing invalid input scenarious that distracted the model.
Therefore they opted for hand crafted noise shapes taking into account the distribution of the relevant feature. 

### Data Augmentation

As mentioned before, our aim with predictive models is to generalise well to an unseen test set.
In an ideal world we would train a model on all possible variations of the data to capture all interactions and relationships.
This is not possible in the real world.
Such a dataset is not available and would be infinitely large.

In reality we have a finite subset of the full data distribution to train on.
Any new samples with unique feature combinations will likely improve the models generalisablity.
If the collection of new samples is not available, we can try to artificially create more.

This is a standard approach especially in computer vision applications. For example, from a single image, we can rotate it, flip it horizontally, shift it any direction, crop it, and many other transformations without destroying the semantic content of the image.
But by doing so we are artificially increasing the size of the training set to help with overfitting.
Of course this is not as effective as genuine new data samples, but it is a very effective and efficient substitute [@Perez2017].

## Modern Architectures

### Normalisation

One of the things that complicate the training of neural networks is the fact that hidden layers have to adapt to the continuously changing distribution of its inputs. The inputs to each layer are affected by the paramaters of all its preceding layers and a small change in a preceding layer can lead to a much bigger difference in output as the network becomes deeper. When the input distribution to a learning system changes, it is said to experience covariate shift [@Shimodaira2000].

Using ReLUs, carefull weight initialisation and small learning rates can help a network to deal with the internal covariate shift. However, a more effective way would be to ensure that the distribution of non-linearity inputs remains more stable while training the network. [@Ioffe2015] proposed *batch normalisation* to do just that.

A batch normalisation layer normalises its inputs to a fixed mean and variance (similar to how the inputs of the network is normalised) and therefore it can be applied before any hidden layer in a network to prevent internal covariate shift. The addition of this layer dramatically accelerates the training of DNNs, also because it can be used with higher learning rates. It also helps with regularisation [@Ioffe2015], therefore in some cases dropout is not necessary.

The batch normalising transform over a batch of univariate inputs, $x_{1}, \dots,x_{n}$ is done by the following steps:

1. Calculate the mini-batch mean, $\mu$, and variance, $\sigma^{2}$:

    $$
    \begin{aligned}
    \mu &= \frac{1}{n}\sum_{i=1}^{n}x_{i}\\
    \sigma^{2}&=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-u)^{2}
    \end{aligned}
    $$

2. Normalise the inputs, 

    $$
    \hat{x}_{i} = \frac{x_{i}-\mu}{\sqrt{\sigma^{2}+\epsilon}},
    $$
    where $\epsilon$ is a constant to ensure numerical stability.

3. Scale and shift the values,

    $$
    y_{i}=\gamma\hat{x}_{i}+\beta,
    $$
    where $\gamma$ and $\beta$ are the only two learnable paramaters of a batch normalisation layer.

The reason for the scale and shift step is to allow the layer to represent the identity transform if the normalised inputs are not suitable for the following layer, *i.e.* the scale and shift step will reverse the normalisation step if $\gamma=\sqrt{\sigma^{2}+\epsilon}$ and $\beta=\mu$.


The batch normalisation layer attempts to normalise neuron activations to zero mean and unit variance [@Ioffe2015].
It has become the stanard when training deep CNNs.
Training with normalisation techniques is perturbed by Stochastic Gradient Descent (SGD), stochastic regularisation (like dropout) and the estimation of the normalisation parameters.
Both RNNs and CNNs can stabilise learning via weight sharing, therefore they are less prone to perturbations.
Fully-connected NNs do not have this luxury and shows high variance in the training error when trained with normalisation techniques.

- batchnorm
- selu?

### Residual Networks
- resnet and densenet
- how resnet is essentially boosting

Residual Networks became very popular after it was used to win one of the ImageNet competiions [@He2015].
The residual connection layer can simply be formalised as
$$
y=F(x)+x,
$$
*i.e.* combining the input to the layer(s) with the output of the layer(s).
Here, the combination is by addition, but other ways can also be used, like multiplication or concatenation.
These layers are very useful when training deeper neural networks since they encourage gradient flow through interval layers.

### Embeddings

Like word embeddings.
Learnable mapping of an item to a vector.

### Attention

First proposed in neural machine translation [@Bahdanau2014] and now almost used ubiquitously in natural language processing applications.

## One-cycle Policy
- can include experiment here


[@Smith2018]

+ reduce training time and increase performace

Currently the process of setting the hyper-parameters, including designing the network architecture, requires expertise and extensive trial and error and is based more on serendipity than science. 

Currently there are no simple and easy ways to set hyper-parameters – specifically, learning rate, batch size, momentum, and weight decay.
Grid search or random search is expensive.
Optimal parameters make a huge difference in training time and performance.

Look for clues of overfitting and underfitting to determine best parameters.

The experiments discussed herein indicate that the learning rate, momentum, and regularization are tightly coupled and optimal values must be determined together.

by monitoring validation/test loss early in the training, enough information is available to tune the
architecture  and  hyper-parameters  and  this  eliminates  the  necessity  of  running  complete  grid  or
random searches.

Underfitting is when the machine learning model is unable to reduce the error for either the test or
training set.  The cause of underfitting is an under
capacity
of the machine learning model; that is,
it is not powerful enough to fit the underlying complexities of the data distributions.  Overfitting
happens  when  the  machine  learning  model  is  so  powerful  as  to  fit  the  training  set  too  well  and
the generalization error increases.

The takeaway is that
achieving the horizontal part of the test loss is the goal of hyper-
parameter tuning

The art of setting the network’s hyper-parameters amounts to ending up at the balance point between
underfitting and overfitting

If the learning rate (LR) is too small, overfitting can occur.  Large learning rates help to regularize
the training but if the learning rate is too large, the training will diverge.

To use CLR, one specifies minimum and maximum learning rate boundaries and a stepsize. The
stepsize is the number of iterations (or epochs) used for each step and a cycle consists of two such
steps – one in which the learning rate linearly increases from the minimum to the maximum and
the other in which it linearly decreases. 

n the LR range test, training starts with a small learning rate which is slowly increased linearly
throughout a pre-training run.  This single run provides valuable information on how well the net-
work can be trained over a range of learning rates and what is the maximum learning rate.  When
starting with a small learning rate, the network begins to converge and, as the learning rate increases,
it eventually becomes too large and causes the test/validation loss to increase and the accuracy to
decrease.  The learning rate at this extrema is the largest value that can be used as the learning rate
for the maximum bound with cyclical learning rates but a smaller value will be necessary when
choosing a constant learning rate or the network will not begin to converge.

the amount of regularization must be balanced for each dataset
and architecture

Contrary to this early work, this Section recommends using a larger batch size when using
the 1cycle learning rate schedule, which is described in the above

Weight decay is one form of regularization and it plays an important role in training so its value needs
to be set properly.  The important point made above applies; that is, practitioners must balance the
various forms of regularization to obtain good performance. the interested reader can see kuka et al. (2017) for a review of regularization methods.

1.  Learning rate (LR): Perform a learning rate range test to a “large” learning rate.  The max
LR depends on the architecture (for the shallow 3-layer architecture, large is 0.01 while for
resnet, large is 3.0), you might try more than one maximum.  Using the 1cycle LR policy
with a maximum learning rate determined from an LR range test, a minimum learning rate
as a tenth of the maximum appears to work well but other factors are relevant, such as the
rate of learning rate increase (too fast and increase will cause instabilities).

2.  Total batch size (TBS): A large batch size works well but the magnitude is typically con-
strained by the GPU memory. If your server has multiple GPUs, the total batch size is the
batch size on a GPU multiplied by the number of GPUs. If the architecture is small or your
hardware permits very large batch sizes, then you might compare performance of different
batch sizes.  In addition, recall that small batch sizes add regularization while large batch
sizes add less, so utilize this while balancing the proper amount of regularization. It is often
better to use a larger batch size so a larger learning rate can be used.

3.  Momentum:  Short runs with momentum values of 0.99, 0.97, 0.95, and 0.9 will quickly
show the best value for momentum. If using the 1cycle learning rate schedule, it is better to
use a cyclical momentum (CM) that starts at this maximum momentum value and decreases
with increasing learning rate to a value of 0.8 or 0.85 (performance is almost independent
of the minimum momentum value).  Using cyclical momentum along with the LR range
test stabilizes the convergence when using large learning rate values more than a constant
momentum does.

4.  Weight decay (WD): This requires a grid search to determine the proper magnitude but
usually  does  not  require  more  than  one  significant  figure  accuracy.   Use  your  knowl-
edge of the dataset and architecture to decide which values to test.  For example, a more
complex dataset requires less regularization so test smaller weight decay values. A  shallow  architecture  requires  more  regularization  so  test  largerweight decay values.

## Model Interpretation

Although Deep Learning is now the state-of-the-art for many machine learning tasks, it still trailing behind other algorithms in terms of model interpretability.
But keep in mind this is not an unusual trade-off; between prediction performance and model interpretability.
DNNs are occasionally referred to as "black boxes" since it is very difficult to interpret what is going on inside the stacks of linear and non-linear layers.
This one of deep learning's greatest criticisms and is large reason why it cannot be used in some production environments. 
For example, in the clinical domain, model transparency is of utmost importance, given that predictions  might be used to affect real-world medical decision-making and patient treatments [@Shickel2017].

Fortunately, some work has been done to gain insights from NNs.

### Model Agnostic

#### Permutation Importance

[@haldar2018] notes that the permutation test only produces sensical results on the assumption that the features are independent.
Permuting the feature independently created examples that never occurred in real life, and the importance of features in that invalid space sent us in the wrong direction. 
The test however is somewhat useful in determining features that were not pulling their weight.
If randomly permuting a feature did not affect the model performance at all, it was a good indication that the model is probably not dependent on it.

+ Partial Dependece
+ SHAP

### Neural Network Specific

- Distilling Neural Networks, i.e. training a decision tree on train neural network generated data. https://arxiv.org/pdf/1711.09784.pdf
- Mimic leanring [@Che2016]
- Plotting embeddings in lower dimensional space with PCA or t-sne
- evaluate which inputs get activated by a certain unit.
- 