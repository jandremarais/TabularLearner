# Conclusion \label{chp:conc}

In this study we set out to investigate a relatively under-explored area of deep learning, *viz*. the application of deep learning approaches to tabular datasets.
We reviewed the best approaches for this task, and through empirical work, aimed to gain a better understanding of each of the proposed methodologies.
In order to explore deep learning for tabular data, we provided an overview of neural networks and discussed modern advancements in the deep learning field.
Some of the modern proposals could then be identified as potentially useful in the context of tabular data.

This chapter provides a summary of our work.
In \Sref{sec:summary} the contribution of each chapter is discussed, and in \Sref{sec:lim}, some limitations and avenues towards possible improvements are indicated.

We conclude the thesis with a section on promising future directions for research in the field of deep learning for tabular data  (\Sref{sec:future}).

## Summary \label{sec:summary}

In \Cref{chp:intro}, the motivation and the objectives of the study were described.
It was stated that deep learning for tabular data is an important topic, but in our opinion, also one that has not yet received sufficient attention in the literature.
Hence the main issues that need to be addressed in order to make progress in the field, were highlighted.
The chapter also provided an overview of the fundamentals of Statistical Leaning Theory, including various learning paradigms and loss functions; as well as aspects of optimisation and overfitting.
The SLT framework served as background for the problems we aimed to solve.

The focus in \Cref{chp:nn} included the core concepts in neural networks, and neural network training.
Neurons, layers, activation functions and network architectures were described in order to gain an understanding of the mechanics of neural networks.
With regard to neural network training, the backpropogation and stochastic gradient descent algorithms were introduced, and illustrated by way of a few examples.
The chapter included a brief look at basic regularisation methods for neural networks, as well as a section on representation learning.
The aim of the latter section was to facilitate insight into the way that a neural network learns from data, and into what concepts are actually learned. 
In summary therefore, the contribution of the chapter was to equip us with the fundamentals of neural networks in order to grasp the ideas underlying modern developments in deep learning.

The topic in \Cref{chp:dl} was deep learning. 
The aim of the chapter was to provide insight into the methodologies enabling deep learning to be successful in the NLP and computer vision fields.
This provided a good starting point for contemplating ways of improving deep learning for tabular data.
The chapter started with an introduction to autoencoders and the concept of using them for unsupervised pretraining in transfer leaning.
We also discussed the use of data augmentation and dropout as highly effective regularisation techniques.
This was followed by a review of the more modern layers and architecture designs in deep learning, which included normalisation layers, skip connections, embeddings and the attention mechanism.
The chapter included a section on the concept of superconvergence.
Here we discussed the 1cycle policy and more effective hyperparameter selection as ways in which superconvergence may be achieved.
We concluded with a brief discussion (and examples) of both neural network specific and model agnostic tools that may be used to interpret deep neural networks. 

\Cref{chp:td} was devoted to the topic of deep learning for tabular data.
This chapter entailed a review of recent contributions with regard to the application of deep neural networks to tabular data.
As stated in \Cref{chp:intro}, the review was guided by the main challenges posed by tabular data in this context. 
We explored various ways of preprocessing continuous features, and of optimally embedding and presenting categorical features.
We also investigated approaches towards encouraging networks to learn better feature interactions.
These included the use of attention modules and cross features.
In terms of training deeper neural networks for more complext feature learning, the SeLU activation function was discussed, along with the use of skip connections.
Subsequently, we described several approaches towards making deep neural networks more sample efficient.
In this section we focused on denoising autoencoders, and on data augmentation techniques such as swap noise and mixup augmentation.
The penultimate section described ways of interpreting neural networks for tabular data, and provided an illustration of interpretation by means of knowledge distillation.
We concluded with an empirical investigation of superconvergence in the context of tabular data.
Hence \Cref{chp:td} contributed to the technical understanding necessary for tackling challenges in deep learning for tabular data.

Our empirical work is summarised in \Cref{chp:exp}.
The experiments reported on, complement the exploratory study of deep learning for tabular data.
We attempted to answer three main questions, *viz.* which approach to use for input representation, for inferring feature interactions, and for enhancing sample efficiency.
In the input representation experiments, we evaluated the effect of entity embedding sizes on the performance of neural networks.
Here we experimented with the attention mechanism, along with the use of skip connections and of the SeLU activation function. 
Finally, in the sample efficiency section, we attempted to gage the sensitivity of neural networks to the number of training samples.
We also tested the use of unsupervised pretraining towards alleviating this sensitivity.

We also looked at the mixup and swap noise data augmentation approaches as means to avoid overfitting.

## Limitations \label{sec:lim}

There were various aspects that limited the impact of this study.
Two technical limitations were experienced:

+ **Access to large compute:** Deep learning techniques are notorious for the amount of computing power they require. There was limited access to cloud computing providers on which some of the experiments were done. However, in most cases, we only had access to a small personal machine without a graphical processing unit (GPU). This significantly increased the running time of the experiments and hindered rapid iteration of different approaches. In future work we would make sure that sufficient computing power is available for experimenting in this field.
  
+ **Access to quality code:** Many of the recent developments discussed in this work had no official implementation at the time of writing. This forced us to rewrite many of the code in order to validate the results claimed in the literature and to be able to use it in our data and models. Sometimes technical details were left out of the original papers and we had to improvise to get a working example. By doing it this way we risked using code for our experiments that might not have been exactly what the authors intendend.

The other limitations to this study were:

+ **Experiments on a single dataset:** Due to the technical limitations described above, we only had the capacity to process a single dataset for our experiments. This made our results inconclusive, but still adds value as an exploratory study. There is also a possibility that this dataset was in a sense to easy to solve and that we could not demonstrate the full power of the deep learning approaches. This study would have benefitted from experiments on datasets with different properties and with different tasks. We published all of our code online if anyone wants to build on this work.

+ **Based on pre-prints:** Deep learning is such a fast developing area of research and in an attempt to keep this work relevant, pre-prints of publications were cited. Pre-prints are not peer-reviewed and subject to change. We did our best to critically evaluate the work cited and to confirm findings with experiments. Although we tried to keep up with the deluge of publications, there remains a possibility that new publications arised during the post-research phase of this work.

## Promising Future Directions \label{sec:future}

Throughout this thesis we have identified promising research directions which, however, did not fit into the scope of this work.
Besides repeating the empirical work on a wider variety of datasets, we propose the following to be a valuable research path to follow:

**Generative models for tabular data**.
Here we specifically refer to *Variational Autoencoders* (VAE) [@Kingma2013] and *Generative Adversarial Networks* (GAN) [@Goodfellow2014].
A VAE provides a probabilistic manner for describing an observation in a latent space.
That is, instead of using a single value to describe an attribute in the latent space, like the standard autoencoder, a VAE uses a probability distribution as a description of a latent space attribute.
The greatest value we think VAEs can add is as a method of unsupervised pretraining.
When using DAEs for unsupervised pretraining, it needs to be injected with some noise.
However, the best noise schemes we found, were swap noise and blank-out, but as mentioned before, neither makes complete sense in the tabular dataset environment.
A VAE can provide a more robust way of doing unsupervised learning because it decoding function needs to learning to process probabilistic output and thus it is not reliant on noise injection.
In addition, once trained, a VAE can be used to generate new samples by sampling from the latent probabilistic distribution, which may serve as a way to generate more training samples.

We also believe that GANs offer a good alternative to generating new training samples.
A GAN consists of two neural networks, a generator ($G$) and a discriminator ($D$).
The task of $G$ is: given random noise as input, generate artificial samples of the data that are indistinguishable from the genuine samples.
The task $D$ is to attempt to discriminate between the two.
GANs have shown tremendous value in data synthesis, especial in the domains of computer vision and speech synthesis, producing life-like faces [@Karras2017] and voices [@Donahue2018].
We think GANs can achieve similar success in data synthesis for tabular data and can thus also be used to artificially enlarge training datasets for supervised learning.
