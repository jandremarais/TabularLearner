# Conclusion \label{chp:conc}

In this study we set out to investigate a relatively under-explored area of deep learning, *viz*. the application of deep learning approaches to tabular datasets.
We reviewed the best approaches for this task, and through empirical work, aimed to gain a better understanding of each of the proposed methodologies.
In order to explore deep learning for tabular data, we provided an overview of neural networks and discussed modern advancements in the deep learning field.
Some of the modern proposals could then be identified as potentially useful in the context of tabular data.

This chapter provides a summary of our work.
In \Sref{sec:summary} the contribution of each chapter is discussed, and in \Sref{sec:lim}, some limitations and avenues towards possible improvements are indicated.

We conclude the thesis with a section on promising future directions for research in the field of deep learning for tabular data  (\Sref{sec:future}).

## Summary \label{sec:summary}

In \Cref{chp:intro}, the motivation and the objectives of the study were described.
It was stated that deep learning for tabular data is an important topic, but in our opinion, also one that has not yet received sufficient attention in the literature.
Hence the main issues that need to be addressed in order to make progress in the field, were highlighted.
The chapter also provided an overview of the fundamentals of Statistical Leaning Theory, including various learning paradigms and loss functions; as well as aspects of optimisation and overfitting.
The SLT framework served as background for the problems we aimed to solve.

The focus in \Cref{chp:nn} included the core concepts in neural networks, and neural network training.
Neurons, layers, activation functions and network architectures were described in order to gain an understanding of the mechanics of neural networks.
With regard to neural network training, the backpropogation and stochastic gradient descent algorithms were introduced, and illustrated by way of a few examples.
The chapter included a brief look at basic regularisation methods for neural networks, as well as a section on representation learning.
The aim of the latter section was to facilitate insight into the way that a neural network learns from data, and into what concepts are actually learned. 
In summary therefore, the contribution of the chapter was to equip us with the fundamentals of neural networks in order to grasp the ideas underlying modern developments in deep learning.

The topic in \Cref{chp:dl} was deep learning. 
The aim of the chapter was to provide insight into the methodologies enabling deep learning to be successful in the NLP and computer vision fields.
This provided a good starting point for contemplating ways of improving deep learning for tabular data.
The chapter started with an introduction to autoencoders and the concept of using them for unsupervised pretraining in transfer leaning.
We also discussed the use of data augmentation and dropout as highly effective regularisation techniques.
This was followed by a review of the more modern layers and architecture designs in deep learning, which included normalisation layers, skip connections, embeddings and the attention mechanism.
The chapter included a section on the concept of superconvergence.
Here we discussed the 1cycle policy and more effective hyperparameter selection as ways in which superconvergence may be achieved.
We concluded with a brief discussion (and examples) of both neural network specific and model agnostic tools that may be used to interpret deep neural networks. 

\Cref{chp:td} was devoted to the topic of deep learning for tabular data.
This chapter entailed a review of recent contributions with regard to the application of deep neural networks to tabular data.
As stated in \Cref{chp:intro}, the review was guided by the main challenges posed by tabular data in this context. 
We explored various ways of preprocessing continuous features, and of optimally embedding and presenting categorical features.
We also investigated approaches towards encouraging networks to learn better feature interactions.
These included the use of attention modules and cross features.
In terms of training deeper neural networks for more complext feature learning, the SeLU activation function was discussed, along with the use of skip connections.
Subsequently, we described several approaches towards making deep neural networks more sample efficient.
In this section we focused on denoising autoencoders, and on data augmentation techniques such as swap noise and mixup augmentation.
The penultimate section described ways of interpreting neural networks for tabular data, and provided an illustration of interpretation by means of knowledge distillation.
We concluded with an empirical investigation of superconvergence in the context of tabular data.
Hence \Cref{chp:td} contributed to the technical understanding necessary for tackling challenges in deep learning for tabular data.

Our empirical work is summarised in \Cref{chp:exp}.
The experiments reported on, complement the exploratory study of deep learning for tabular data.
We attempted to answer three main questions, *viz.* which approach to use for input representation, for inferring feature interactions, and for enhancing sample efficiency.
In the input representation experiments, we evaluated the effect of entity embedding sizes on the performance of neural networks.
with regard to feature interactions, we experimented with the attention mechanism, along with use of the SeLU activation function, and of skip connections. 
Finally, in the sample efficiency section, we attempted to gage the sensitivity of neural networks to the number of training samples.
We tested the use of unsupervised pretraining towards alleviating this sensitivity, and investigated the use of mixup- and swap noise data augmentation as a means to prevent overfitting.

## Limitations \label{sec:lim}

There are various aspects that limited the impact of this study.
In this regard, there were two main obstacles which we needed to overcome:

+ **Access to large compute**: Deep learning techniques are notorious for the computing power they require. 
We had limited access to cloud computing providers on which some of the experiments were done. 
Therefore most experiments were conducted on a small personal machine without a graphical processing unit (GPU).
This significantly increased the running time of the experiments, and hindered rapid execution of the various approaches. 
In future one would want to ensure sufficient computing power to be available, enabling higher quality empirical work.
  
+ **Access to quality code**: At the time of writing, many of the recent developments discussed in this thesis were not accompanied by any official implementation. 
This forced us to rewrite much of the code that was used to validate the results reported in the literature in order to be able to apply it to our data and models. 
Sometimes important technical details were omitted from the original papers.
This called for some improvisation in order to obtain working examples. 
Unfortunately this approach brings with it the risk of unintentionally departing from the original intention of some of the proposed methodologies.

Further limitations were:

+ **Experiments on a single dataset**: Due to the technical limitations mentioned above, we only had capacity to process a single dataset in our empirical study. 
This rendered our findings to be inconclusive, although we believe there still to be value in our exploratory study.
The possibility remains that the strength of the signal in the Adult data does not make the use of this particular dataset amenable to  demonstrating the full power of deep learning approaches. 
Naturally, repeating experimentation on datasets with different properties, and to applications with different tasks, would have facilitated clearer recommendations with regard to the questions that arise during implementation of deep learning for tabular data.
Such an extention is possible, since we have made all or our code available on-line.

+ **Based on pre-prints**: Deep learning is such a fast developing area of research and in an attempt to keep this work relevant, pre-prints of publications were cited. 
Pre-prints are of course not peer-reviewed and subject to change. 
We did our best to critically evaluate the work cited, and to confirm findings with our own experiments.
Although we tried to keep up with the deluge of publications that are currently available, there remains a possibility that new publications arised during the post-review phase of the study.

## Future Directions \label{sec:future}

With a view to future research directions, in this final section we would like to point the reader to the potential of using *generative models* in the context of deep learning for tabular data.
More specifically, it might be worthwhile to study variational autoencoders [@Kingma2013] and *Generative Adversarial Networks* (GANs) [@Goodfellow2014].
We propose studying VAEs as a means to perform more effective unsupervised pretraining, and we believe GANs to offer a good alternative to generating new training samples.
In conclusion, therefore, a brief over of VAEs and GANs follows below.

VAEs provide a probabilistic manner of describing observations in a latent space.
That is, instead of using a single value to describe an attribute in the latent space, as is the cases with standard autoencoders, VAEs use probability distributions to describe a latent space attribute.
When using DAEs for unsupervised pretraining, we have seen that they need to be injected with some noise.
However, we have seen that neither of the two best noise schemes that we could find (*viz.* swap noise and blank-out), makes complete sense in the tabular dataset environment.
We postulate that VAEs may provide a more robust way of doing unsupervised pretraining, since its decoding function learns probabilistic output. 
Thus it is not reliant on noise injection.
In addition, once trained, the latent probabilistic distribution of VAE can be used to generate additional training samples.

GANs consist of two neural networks, *viz.* a generator ($G$) and a discriminator ($D$).
Given random noise as input, the task given to $G$ is to generate artificial samples of the data that are indistinguishable from a set of genuine training samples.
The task assigned to $D$ is to attempt to discriminate between the artificial and genuine samples.
GANs have shown tremendous value in data synthesis, especial in the domains of computer vision and speech synthesis, producing lifelike faces [@Karras2017] and voices [@Donahue2018].
We postulate that GANs may be able to achieve similar successes in data synthesis for tabular data, which may be used to artificially enlarge training datasets for supervised learning.
