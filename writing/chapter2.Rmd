# Neural Networks \label{chp:nn}

## Introduction

Not unlike most supervised machine learning models, a neural network (NN) is a function which maps inputs to outputs, *i.e.* $f:\boldsymbol{x}\to y.$
The structure of $f$ is often loosely compared to the structure of the human brain. 
Oversimplified, the brain consists of a collection of interconnected neurons. 
Each neuron can generate and receive signals. 
A received signal may be described as an input to a neuron, whereas a sent signal may be described as an output from that neuron.
If two neurons are connected, it means that the output from the one neuron serves as input to the other.
In a very simple model of the brain, one may argue that a neuron receives several signals, which it weighs and combines, and if the combined value of the inputs is higher than a certain threshold, the neuron sends a output signal to the next neuron.
\autoref{fig:neurons} (a) provides a schematic of a biological neuron.

An artifical neural network tries to mimic this model of the human brain - it is set up to consist of several layers of connected units (or neurons).
With exception of units in the first and final layers, each unit outputs a weighted combination of its inputs, combined with a simple non-linear transformation.
In each layer of the neural network, the input is passed through each of the neurons. 
In turn, their output is passed to the next layer.

The transformation at each neuron is controlled by a set of parameters, also known as weights. 
Training a neural network involves tuning these weights in order to obtain some desired output. 
During training, the neural network receives as input a set of training data. 
The neural network weights are then learned in such a way that, when given a new set of inputs, the output predicted by the neural network matches the corresponding response of interest as closely as possible.
The process of using the training data to tweak the weights is done by means of an optimisation algorithm called Stochastic Gradient Descent (SGD).

Although recently there has been plenty of excitement around neural networks, it is well known that they were invented many years ago.
The development of neural networks dates back at least as far as the invention of perceptrons in [@rosenblatt1962].
It is also interesting to compare modern neural networks with the Projection Pursuit Regression algorithm in statistics [@friedman1981].
Only recently a series of breakthroughs allowed neural networks to be more effective, leading to the renewed interest in the field.

The aim of this chapter is to provide an overview of neural networks, emphasising the basic structure (\Sref{sec:structure}) and the way in which they are trained (\Sref{sec:training}).
This is done with a view to discuss modern neural network structures and training policies in \Cref{chp:dl}, which in turn will help us shed light on Deep Learning for tabular data.
This chaper includes a section on representation and manifold learning (\Sref{sec:rep}) in an attempt to understand what a NN is actually doing.
Finaly, the chapter concludes with an example implementation of a vanilla NN trained on tabular data (\Sref{sec:vanilla_example}).

## The Structure of a Neural Network \label{sec:structure}

### Neurons and Layers

In basic terms, a neural network processes an input $boldsymbol{x}$ by sending it through a series of layers. 
The neurons in each layer apply some transformation to their inputs, resulting in a set of outputs which are again passed on to the next layer of neurons. 
Eventually, the final layer produces the neural network output. 
In this section we provide more detail regarding the neural network structure. 
We start with a description of the operations inside each neuron, and follow with a discussion of the way in which the neurons may be connected in layers in order to form a complete neural network structure.
Our discussion is based upon a simple regression example.

Suppose we are in pursuit of a function which is able to estimate some continuous target, $y$, given a $p$-dimensional input $\boldsymbol{x}$, *e.g.* estimating the taxi fare from features such as distance travelled, time elapsed and number of passengers.
A single neuron may act as such a function.
It models $y$ by computing a weighted average of the input features.
This operation is illustrated in \autoref{fig:neurons} (b).

```{r, fig.cap="Comparison of a biological (a) and an artificial (b) neuron[^imcred].\\label{fig:neurons}"}
# library(png)
library(jpeg)
library(grid)
library(gridExtra)
img1 <- rasterGrob(as.raster(readJPEG("figures/biological_neuron.jpg")), interpolate = FALSE)
img2 <- rasterGrob(as.raster(readJPEG("figures/single_neuron.jpg")), interpolate = FALSE)
t1 <- textGrob('(a)')
t2 <- textGrob('(b)')
grid.arrange(img1, img2, t1, t2, ncol = 2, heights=9:1)
```

[^imcred]: Image credit: https://www.jeremyjordan.me/intro-to-neural-networks/

In equation form, this function can be written as:
$$
w_{1}\cdot x_{1} + w_{2}\cdot x_{2}+ \dots + w_{p}\cdot x_{p}+b=y,
$$
where $\{w_{k}\}_{k=1}^{p}$, are the weights applied to each of the inputs $\{x_{k}\}_{k=1}^{p}$ and $b$ the constant bias term.
Clearly, this equation is simply the very common linear model and thus also can be written as:
$$
\boldsymbol{w}^{\intercal}\boldsymbol{x} + b=y,
$$
where $\boldsymbol{x}=[x_{1}\quad x_{2}\quad \dots\quad x_{p}]^{\intercal}$ is the input, $\boldsymbol{w}=[w_{1}\quad w_{2}\quad \dots\quad w_{p}]^{\intercal}$ the weights and $y$ the output. 
We may compress the above equation to $\boldsymbol{w}^{\intercal}\boldsymbol{x}=y$, where $\boldsymbol{x}$ includes the bias term and the weight vector $\boldsymbol{w}$ a unit element, *i.e.* $\boldsymbol{x}=[1\quad x_{1}\quad\dots\quad\ x_{p}]^{\intercal}$ is the input, $\boldsymbol{w}=[b\quad w_{1}\quad \dots\quad w_{p}]^{\intercal}$.

The weights convey the importance of each input features in predicting the target.
The larger $|w_{k}|$ is, the greater is the contribution of $x_{k}$ towards the output.
If $w_{k}=0$, $x_{k}$ has no influence on the target.
However the weights are unknown and therefore we need to estimate them.

In linear regression this is done by means of the method of ordinary least squares.
Since a neural network consists of many inter-conncected neurons, an alternative estimation procedure is required. 
This is the topic of the next section.

Often a linear model will be too rigid to model a certain response of interest. 
In order to fit a more flexible model, we may add more neurons.
Consider the use of two neurons, where the second neuron accepts the same input as the first neuron, but uses a different set of weights.
Thus we have two different outputs produced by the two neurons, *i.e.*. $z_{1}=\boldsymbol{w}_{1}^{\intercal}\boldsymbol{x}$ and $z_{2}= \boldsymbol{w}_{2}^{\intercal}\boldsymbol{x}$.
In order to produce a final estimate from the initial two estimates, *viz.* $z_{1}$ and $z_{2}$, they are passed to a third neuron.
That is, $y=\boldsymbol{w}_{3}^{\intercal}\boldsymbol{z}$, where $\boldsymbol{z}=[z_{1}\quad z_{2}]^{\intercal}$.
\autoref{fig:simple_nn} illustrates this pipepline in network form.

```{r, fig.cap="A simple neural network accepting $p$-sized inputs, with one hidden layer which has two neurons.\\label{fig:simple_nn}", out.width="200px", align='center'}
knitr::include_graphics('figures/simple_nn.png')
```

The first two neurons each received all $p$ inputs and each produced a single output. 
These two outputs were received by the third neuron, and combined in order to produce the final output, *viz.* $y$. 
The operations performed by the first two neurons may be expressed as $\boldsymbol{z}=W\boldsymbol{x}^{\intercal}$, where
$$
 W=\begin{bmatrix}
 \boldsymbol{w}_{1}^{\intercal} \\
 \boldsymbol{w}_{2}^{\intercal}
 \end{bmatrix}=
 \begin{bmatrix}
 w_{10} & w_{11} & w_{12} & \dots & w_{1p}\\
 w_{20} & w_{21} & w_{22} & \dots & w_{2p}\\
 \end{bmatrix} \quad \text{and}\quad \boldsymbol{z}=[z_{1}\quad z_{2}]^{\intercal}.
$$

The collection of these two neurons is what is called a layer.
Since our third neuron (which is also a layer but with a single neuron) receives the output of this layer as input, it is possible to express the complete input-output relationship in one equation, *i.e.*
$$
y=\boldsymbol{w}_{3}^{\intercal}\boldsymbol{z}=\boldsymbol{w}_{3}^{\intercal}W\boldsymbol{x}.
$$
Note here that the weights from the first layer, $W$, and the third neuron, $\boldsymbol{w}_{3}$, can be collapsed into a single vector $\boldsymbol{w}$, effectively reducing all of the neuron operations back into a single neuron representation and thus the fitted model is still linear.
In order to fit a non-linear model, a non-linear transformation function is applied to the output of each layer. 
This function is called an *activation function*. 

Subsequently the neural network equation can be written as
$$
y=a_{2}\left(\boldsymbol{w}_{3}^{\intercal}a_{1}(W\boldsymbol{x})\right),
$$
where $a_{1}$ denotes the activation function applied after the first (linear) layer, and where $a_{2}$ is the the activation function applied after the final layer.
                           
The introduction of non-linear activation functions serves to enlarge the class of functions that can be approximated by the network.

### Activation Functions

The activation function, $a(\cdot)$, was usually chosen to be the sigmoid function, $a(v)=\frac{1}{1+e^{-v}}$

In the previous section, we introduced activation functions, which are simple non-linear functions of its input. 
These are usually applied after a fully connected layer (linear transformation) and are crucial for the flexibility of a deep neural network. 
We also mentioned that the sigmoid activation, which was originally the go-to activation, is currently not the most popular choice.
Another activation function originally thought to work well was, $a(x)=\tanh(x)$. 
However, by far the most common activation function used at the time of writing is the Rectified Linear Units (ReLU) non-linearity.
Its definition is much simpler than its name and is defined as $a(x)=\max(0,x)$. 
It was introduced in [@Krizhevsky2012] and they showed that using ReLUs in their CNNs reduced the number of training iterations to reach the same point by a factor of 6 compared to using $\tan(x)$.
The ReLU limits the gradient vanishing problem as its derivative is always one when x is positive.
Gradient vanishing problem?

There are a plethora of proposals for activation functions, since any simple non-linear (differentiable?) function can be used. 
Some of the recent most popular choices are exponential linear units (ELUs) [@Clevert2015] and scaled exponential linear units (SELUs) [@Klambauer2017]. 
The choice of activation function usually influences the convergence time and some might protect the training procedure from overfitting in some cases. 
The different activation functions can be experimented with, however it would be sufficient in most cases to use ReLUs. 
The other mentioned proposals have inconsistent gains over ReLUs and therefore it remains the standard choice.

However, very recently [@Ramachandran2017] used automated search techniques to discover novel activation functions. 
The exhaustive and reinforcement learning based searched identified a few promising novel activation functions on which the authors then did further empirical evaluations. 
They found that the so-called *Swish* activation function,

$$
a(x)=x\cdot\sigma(\beta x),
$$
where $\beta$ is a constant (can also be a trainable parameter), gave the best empirical results. 
It consistently matched or outperformed ReLU's on deep networks applied to the domains of image classification and machine translation.
\autoref{fig:activation_functions} shows how the activation functions introduces non-linearity.

![Plots of various activation functions.\label{fig:activation_functions}](figures/activation_functions.pdf)

### Size of the Network

The number of units in the hidden layer, $M$, is also a value to be decided on.
Too few units will not allow the network enough flexibility to model complex relationships and too many takes longer to train and increases the chance of overfitting. 
$M$ is mostly chosen by experimentation. 
A good starting point would be to choose a large value and training the network with regularisation (discussed shortly).

The difference between the above discussed neural networks and current state-of-the-art deep learning methods, is the number and type of hidden layers. 
The following section discusse the popular activation functions used in DNNs.

The units in $\boldsymbol{Z}$ are called hidden since they are not directly observed. The aim of this transformation is to derive features, $\boldsymbol{Z}$, so that the classes become linearly separable in the derived feature space [@Lecun2015].
Many more of these hidden layers (combination of linear and non-linear transformations) can be used to derive features to input into the final classifier.

+ comment on number and size of layers
+ lead into modern architectures
+ lead into parameter optimisation

## Training a Neural Network \label{sec:training}

### Optimisation \label{sec:optimisation}

As mentioned before, fitting a linear regression model can be reduced to finding the optimal weights to minimise the MSE function (with or without weight decay). In fact, typically model training procedures can be described as the search for its internal parameters that minimises or maximises some *objective function*. Therefore statistical learning and optimisation are closely related. Optimisation refers to the task of either minimising or maximising some function $J(x)$ by altering $x$. The function we want to optimise is called the objective function. When we are minimising the objective function, we may also refer to the objective function as the *cost* or *loss function*. These terms will be used interchangeably throughout the remainder of the thesis.

As mentioned in the previous section, parameter estimation (or optimisation) of a linear (or logistic regression) model is usually done using OLS or maximum likelihood estimation (MLE). In this section, however, we discuss an alternative parameter estimation method which is also relevant for the optimisation of neural networks.

Consider the MSE loss function:

$$
\begin{aligned}
L&=\sum_{i=1}^{N}L_{i}\\
&=\sum_{i=1}^{N}\sum_{k=1}^{K}(y_{ik}-f_{k}(\boldsymbol{x}_{i}))^{2}\\
&=\sum_{i=1}^{N}\sum_{k=1}^{K}(y_{ik}-\boldsymbol{w}_{k}^{T}\boldsymbol{x}_{i})^{2},
\end{aligned}
$$
where $f_{k}(\cdot)$ in this case is the linear model used to predict the $k$-th class posterior probability. Although the MSE loss is mostly used in a regression setup and not really well suited for classification, we make use of it here for illustration purposes.

To find the weights, $\boldsymbol{w}$, that minimise $L$, we can follow a process of iterative refinement. That is, starting with a random initialisation of $\boldsymbol{w}$, one iteratively updates the values such that $L$ decreases. The updating steps are repeated until the loss converges. In order to minimise $L$ with respect to $\boldsymbol{w}$, we calculate the gradient of the loss function at the point $L(\boldsymbol{x};\boldsymbol{w})$. The gradient (or slope) of the loss function indicates the direction in which the function has the steepest rate of increase. Therefore, once we have determined this direction, we can update the weights by a step in the opposite direction - thereby reaching a smaller value of $L$.

The gradient of $L_{i}$ is computed by obtaining the partial derivative of $L_{i}$ with respect to $\boldsymbol{w}_{k}$, *i.e.*:

$$
\frac{\partial L_{i}}{\partial\boldsymbol{w_{k}}}=-2(y_{ik}-\boldsymbol{w}_{k}^{T}\boldsymbol{x}_{i})\boldsymbol{x}_{i}.
$$
After obtaining the above ${N}$ partial derivatives, an update at the $(r+1)$-th iteration may be obtained as follows:

$$
\boldsymbol{w}_{k}^{(r+1)}=\boldsymbol{w}_{k}^{(r)}-\gamma\sum_{i=1}^{n}\frac{\partial L_{i}}{\partial\boldsymbol{w_{k}^{(r)}}},
$$
where $\gamma$ is called the *learning rate* and determines the size of the step taken toward the optimal direction. One typically would like to set the learning rate small enough so that one does not overshoot the minimum, but large enough to limit the number of iterations before convergence. This value can be determined via a line search but is not always ideal since this may render the training time of DNNs too long. Another option is to reduce the learning rate after every fixed number of iterations. More detail regarding the implication of the learning rate will be given in \Cref{chp:dnn}.

The procedure of repeatedly evaluating the gradient of the objective function and then performing a parameter update, is called *gradient descent* [Cauchy, 1847]. Gradient descent forms the basis of the optimisation procedure for neural networks.

Note that a weight update is made by evaluating the gradient over a set of observations, $\{\boldsymbol{x}_{i},i=1,\dots,n\}$. One of the advantages of gradient descent is that at an iteration, the gradient need not be computed over the complete training dataset, *i.e.* $n\le N$. When updates are iteratively determined by using subsets of the data, the process is called *mini-batch gradient descent*. This is extremely helpful in large-scale applications, since it obviates computation of the full loss function over the entire dataset. This leads to faster convergence, because of more frequent parameter updates, and allows processing of data sets that are too large to fit into a computer's memory. The choice regarding batch size depends on the available computation power. Typically a batch consists of 64, 128 or 256 data points, since in practice many vectorised operation implementations work faster when their inputs are sized in powers of 2. The gradient obtained using mini-batches is only an approximation of the gradient of the full loss but it seems to be sufficient in practice [@Li2014]. Note at this point that the collection of iterations needed to make one sweep through the training data set is called an *epoch*.

The extreme case of mini-batch gradient descent is when the batch size is selected to be 1. This is called *Stochastic Gradient Descent* (SGD). Recently SGD has been used much less, since it is more efficient to calculate the gradient in larger batches compared to only using one example. However, note that it remains common to use the term SGD when actually referring to mini-batch gradient descent. Gradient descent in general has often been regarded as slow or unreliable but it works well for optimising DNNs. SGD will most probably not find even a local minimum of the objective function. It typically however finds a very low value of the cost function quickly enough to be useful.

### Optimisation Example

To illustrate the SGD algorithm, consider the linear model in a classification context. Suppose we are given a training data set with two-dimensional inputs and only two possible classes. Let the data be generated in the same way as described in [@Hastie2009, pp. 16-17]. 

We want to fit a linear regression model to the data such that we can classify an observation to the class with the highest predicted score. In the binary case it is only necessary to model one class probability and then assign an observation to that class if the score exceeds some threshold (usually 0.5), otherwise it is assigned to the other class. Therefore the decision boundary is given by $\{\boldsymbol{x}:\boldsymbol{x}^{T}\hat{\boldsymbol{w}}=0.5\}$.

The example is illustrated in \autoref{fig:sgd}. The colour shaded regions represent the parts of the input space classified to the respective classes, as determined by the decision boundary based upon OLS parameter estimates. Gradient descent was applied to the determine the optimal weights using a learning rate of 0.001. Since the total number of training observations are small, it is not necessary to use SGD. In \autoref{fig:sgd}, the dashed lines represent the decision boundary defined by the gradient descent parameter estimates at different iterations. We observe that initially the estimated decision boundary is far from the OLS solution, but as the update iterations proceed, the decision boundary is rotated and translated until finally matching the OLS line. It took 29 iterations for the procedure to reach convergence.

```{r, cache=TRUE,eval=FALSE, fig.cap = "Plots of the gradient descent example. (a) The data points in input space. The shades in the background represent the class division in input space, with the decision boundary determined by linear least squares estimation. The dashed lines represent the decision boundaries learned at different iterations. (b) The loss calculated at each iteration. \\label{fig:sgd}"}
# generate the data
set.seed(1)

K <- 2
m <- lapply(list(c(1, 0), c(0, 1), c(3, 3))[1:K], 
            function(a) mvrnorm(n = 10, mu = a, Sigma = diag(2)))

X <- lapply(m, function(b) {
  t(sapply(1:100, function(a) {
    mvrnorm(n = 1, mu = b[sample(10, 1), ], Sigma = diag(2)/5)
  }))
})

D <- data.frame(X = do.call("rbind", X), Y = rep(0:(K-1), each = 100))
D <- cbind(X.0 = 1, D)
#D[, -3] <- scale(D[, -3])

# SGD
lin_model <- function(x, b) sum(x * b)
L <- function(y, yhat) sum((y - yhat)^2)

set.seed(125)
B <- mvrnorm(1, mu = c(0, 0, 0), Sigma = diag(3))
B_mat <- B
yhat <- apply(D[, -3], 1, function(a) lin_model(a, B))
loss <- L(D$Y, yhat)
lr <- 0.001
for(i in 1:200) {
  gradient <- c(-2*t(as.matrix(D[, -4])) %*% (D$Y - yhat))
  B_new <- B - lr * gradient
  
  yhat <- apply(D[, -4], 1, function(a) lin_model(a, B_new))
  loss <- c(loss, L(D$Y, yhat))
  
  if(abs(loss[i+1] - loss[i]) < 0.00001) {
    return()
  } else {
    B <- B_new
    B_mat <- rbind(B_mat, B_new)
  }
  #B <- B_new
}

x <- as.matrix(D[, -4])
Bhat <- solve(t(x)%*%x)%*%t(x)%*%D$Y

d_bounds <- t(apply(B_mat, 1, function(a) c(slope = -a[2]/a[3], intercept = (0.5-a[1])/a[3])))
rownames(d_bounds) <- NULL
d_bounds <- data.frame(iteration = 0:(nrow(d_bounds)-1), d_bounds)
d_bounds <- d_bounds[c(1, 5, 10, 30), ]

xlims <- range(D$X.1) * 1.1
ylims <- range(D$X.2) * 1.1

shade_x <- seq(xlims[1], xlims[2], len = 100)
shade_y <- -shade_x*Bhat[2]/Bhat[3] + (0.5 - Bhat[1])/Bhat[3]
shade_y[shade_y>ylims[2]] <- ylims[2]

db_coords <- t(sapply(1:nrow(d_bounds), function(a) {
  ycut <- xlims[2] * d_bounds[a, "slope"] + d_bounds[a, "intercept"]
  if(ycut < ylims[2]) {
    c(xlims[2], ycut)
  } else {
    c((ylims[2] - d_bounds[a, "intercept"])/ d_bounds[a, "slope"], ylims[2])
  }
}))
colnames(db_coords) <- c("dx", "dy")
d_bounds <- cbind(d_bounds, db_coords)

library(latex2exp)
# Plot
p <- D %>% 
  ggplot() + 
  geom_point(aes(X.1, X.2, color = factor(Y)), show.legend = FALSE)+
  #coord_fixed() +
  theme(panel.background = element_rect(fill = "white")) +
  geom_abline(data = d_bounds, aes(slope = slope, intercept = intercept), linetype = "dashed") +
  geom_ribbon(data = data.frame(shade_x, shade_y),
            aes(x = shade_x, ymin = ylims[1], ymax = shade_y), alpha = 0.15, fill = "red") +
  geom_ribbon(data = data.frame(shade_x, shade_y),
            aes(x = shade_x, ymin = shade_y, ymax = ylims[2]), alpha = 0.15, fill = "blue") +
  scale_y_continuous(expand = c(0, 0), name = TeX("$X_2$"), breaks = NULL) + 
  scale_x_continuous(expand = c(0, 0), name = TeX("$X_1$"), breaks = NULL) +
  geom_text(data = d_bounds, aes(dx, dy, label = paste0("i=", iteration)), hjust = c(1, rep(0, 3)), 
            vjust = c(0, rep(1, 3)), nudge_y = c(0, rep(-0.02, 3)), nudge_x = c(-0.02, rep(0, 3))) +
  panel_border(colour = "black")

loss_data <- data.frame(loss = loss, iteration = 0:(length(loss) - 1)) 
library(ggthemes)
p_loss <- loss_data %>% 
  ggplot(aes(iteration, loss)) + 
  geom_line(color = "blue") +
  theme(axis.ticks.y = element_blank()) +
  # theme(panel.background = element_rect(fill = "white"),
  #       axis.ticks.x = element_blank()) +
  geom_segment(data = loss_data[c(1,5,10,30), ], aes(x = iteration, xend = iteration, yend = 0), linetype = "dashed") +
  scale_y_continuous(expand = c(0, 0), labels = NULL) + 
  scale_x_continuous(expand = c(0.05, 0), breaks = c(0, 4, 9, 29))

plot_grid(p, p_loss, labels = c("(a)", "(b)"), nrow = 1, align = "vh", rel_widths = c(3,2))
```


### Backpropogation

In \Cref{sec:optimisation} we discussed how to fit a linear model using the Stochastic Gradient Descent optimisation procedure. 
Currenlty, SGD is the most effective way of training deep networks. 
To recap, SGD optimises the parameters $\theta$ of a networks to minimise the loss,

$$
\theta = \arg\min_{\theta}\frac{1}{N}\sum_{i=1}^{N}l(\boldsymbol{x}_{i}, \theta).
$$
With SGD the training proceeds in steps and at each step we consider a mini-batch of size $n\le N$ training samples. 
The mini-batch is used to approximate the gradient of the loss function with respect to the paramaters by computing, 

$$
\frac{1}{n}\frac{\partial l(\boldsymbol{x}_{i},\theta)}{\partial \theta}.
$$
Using a mini-batch of samples instead of one at a time produces a better estimate of the gradient over the full training set and it is computationally much more efficient.

This section discusses the same procedure, but applied to a simple single hidden layer neural network. 
This is made possible by the *backpropogation* algorithm. 
Note, this process extends naturally to the training of deeper networks.

The neural network described in the previous section has a set of unknown adjustable weights that defines the input-output function of the network. 
They are the $\alpha_{0m}, \boldsymbol{\alpha}_{m}$ paramters of the linear function of the inputs, $\boldsymbol{X}$, and the $\beta_{0k}, \boldsymbol{\beta}_{k}$ paramaters of the linear transformation of the derived features, $\boldsymbol{Z}$. 
Denote the complete set of parameters by $\theta$. 
Then the objective function for regression can be chosen as the sum-of-squared-errors:

$$
L(\theta) = \sum_{k=1}^{K}\sum_{i=1}^{N}\left(y_{ik}-f_{k}(\boldsymbol{x}_{i})\right)^{2}
$$
and for classification, the cross-entropy:

$$
L(\theta) = -\sum_{i=1}^{N}\sum_{k=1}^{K}y_{ik}\log f_{k}(\boldsymbol{x}_{i}),
$$
with corresponding classifier $G(\boldsymbol{x})=\arg\max_{k}f_{k}(\boldsymbol{x})$.
Since the neural network for classification is a linear logistic regression model in the hidden units, the paramaters can be estimated by maximum likelihood. 
According to @Hastie2009 [p. 395], the global minimiser of $L(\theta)$ is most likely an overfit solution and we instead require regularisation techniques when minimising $L(\theta)$.

Therefore, one rather uses gradient descent and backpropogation to minimise $L(\theta)$.
This is possible because of the modular nature of a neural network, allowing the gradients to be derived by iterative application of the chain rule for differentiation. 
This is done by a forward and backward sweep over the network, keeping track only of quantities local to each unit.

In detail, the backpropogation algorithm for the sum-of-squared error objective function,

$$
\begin{aligned}
L(\theta)&=\sum_{i=1}^{N}L_{i}\\
&=\sum_{i=1}^{N}\sum_{k=1}^{K}(y_{ik}-f_{k}(\boldsymbol{x}_{i}))^{2},
\end{aligned}
$$
is as follows. The relevant derivatives for the algortihm are:

$$
\begin{aligned}
\frac{\partial L_{i}}{\partial \beta_{km}}&=-2(y_{ik}-f_{k}(\boldsymbol{x}_{i}))g_{k}'(\boldsymbol{\beta}_{k}^{T}\boldsymbol{z}_{i})z_{mi},\\
\frac{\partial L_{i}}{\partial \alpha_{ml}}&=-\sum_{k=1}^{K}2(y_{ik}-f_{k}(\boldsymbol{x}_{i}))g_{k}'(\boldsymbol{\beta}_{k}^{T}\boldsymbol{z}_{i})\beta_{km}\sigma'(\boldsymbol{\alpha}_{m}^{T}\boldsymbol{x}_{i})x_{il}.
\end{aligned}
$$

Given these derivatives, a gradient descent update at the $(r+1)$-th iteration has the form,

$$
\begin{aligned}
\beta_{km}^{(r+1)}&=\beta_{km}^{(r)}-\gamma_{r}\sum_{i=1}^{N}\frac{\partial L_{i}}{\partial \beta_{km}^{(r)}},\\
\alpha_{ml}^{(r+1)}&=\alpha_{ml}^{(r)}-\gamma_{r}\sum_{i=1}^{N}\frac{\partial L_{i}}{\partial \alpha_{ml}^{(r)}},
\end{aligned}
$$
where $\gamma_{r}$ is called the learning rate. Now write the gradients as

$$
\begin{aligned}
\frac{\partial L_{i}}{\partial \beta_{km}}&=\delta_{ki}z_{mi},\\
\frac{\partial L_{i}}{\partial \alpha_{ml}}&=s_{mi}x_{il}.
\end{aligned}
$$
The quantities, $\delta_{ki}$ and $s_{mi}$ are errors from the current model at the output and hidden layer units respectively. 
From their definitions, they satify the following,

$$
s_{mi}=\sigma'(\boldsymbol{\alpha}_{m}^{T}\boldsymbol{x}_{i})\sum_{k=1}^{K}\beta_{km}\delta_{ki},
$$
which is known as the backpropogation equations.
Using this, the weight updates can be made with an algortihm consisting of a forward and a backward pass over the network. 
In the forward pass, the current weights are fixed and the predicted values $\hat{f}_{k}(\boldsymbol{x}_{i})$ are computed. 
In the backward pass, the errors $\delta_{ki}$ are computed, and then backpropogated via the backpropogation equations to give obtain $s_{mi}$. 
These are then used to update the weights.

Backpropogation is simple and its local nature (each hidden unit passes only information to and from its connected units) allows it to be implented efficiently in parallel. 
The other advantage is that the computation of the gradient can be done on a batch (subset of the training set) of observations. 
This allows the network to be trained on very large datasets. 
One sweep of the batch learning through the entire training set is known as an epoch. 
It can take many training epochs for the objective function to converge. 

### Learning Rate

The convergence times also depends on the learning rate, $\gamma_{r}$. 
There are no easy ways for determining $\gamma_{r}$. 
A small learning rate slows downs the training time, but is safer against overfitting and overshooting the optimal solution.
With a large learning rate, convergence will be reached quicker, but the optimal solution may not have been found. 
One could do a line search of a range of possible values, but this usually takes too long for bigger networks. 
One possible strategy for effective training is to decrease the learning rate every time after a certain amount of iterations.

Recently, in (https://arxiv.org/abs/1711.00489) (no bibtex entry), the authors found that, instead of learning rate decay, one can alternatively increase the batch size during training. They found that this method reaches equivalent test acccuracies compared to learning rate decay after the same amount of epochs. 
But their method requires fewer parameter updates.

### Basic Regularisation

There are many ways to prevent overfitting in deep neural networks. 
The simplest strategies for single hidden layer networks are by early stopping and weight decay. 
Stopping the training process early can prevent overfitting. When to stop can be determined by a validation set approach. 
Weight decay is the addition of a penalty term, $\lambda J(\theta)$, to the objective function, where,

$$
J(\theta)=\sum_{km}\beta^{2}_{km} + \sum_{ml}\alpha^{2}_{ml}.
$$
This is exactly what is done in ridge regression [@Hastie2009, Ch. 4]. 
$\lambda \ge 0$ and larger values of $\lambda$ tends to shrink the weights towards zero. 
This helps with the generalisation ability of a neural network, but recently more effective techniques to combat overfitting in DNNs have been developed. 
These are dicussed in \Cref{sec:over}.

It is common to standardise all inputs to have mean zero and standard deviation of one. 
This ensures that all input features are treated equally. 
Now we have covered all of the basics for simple (1-layer) neural networks.

+ move regularisation to next chapter
+ lead into modern learning policies
+ lead into what it is learning

## Representation Learning \label{sec:rep}

We are now familiar with the mathematical operations of basic layers, how they are connected and how their weights are tweaked to minimise a loss function.
In this section we will discuss why this works and what the NN is actually doing to model the data.
The central idea is that of a data *representation* [@Bengio2013] and that at each layer of the network the data is transformed into a higher-level abstraction of itself.
Understanding and interpreting NNs remains a challenge [@Frosst2017], but the notion of learning an optimal data representation allows us to gain a deeper intuition of the inner mechanics of NNs.

Machine learning models are very sensitive to the form and the properties of the input given to it.
Thus a large part of building machine learning models is to find the best way of representing the raw data to make it easier for the models to extract useful information.
This is typically a laborious manual task of creating, analyzing, evaluating and selecting appropriate features[^feature_engineering], and requires practitioner expertise and domain knowledge.
This *feature engineering* process is more trial-and-error than a systematic recipe.
Therefore if one can effectively automate this process, it will save a lot of time and raise the performance ceiling of models.
Automatically learning representations of the data that make it easier to extract useful information for classifiers or other predictors is called representation learning [@Bengio2013].

[^feature_engineering]: http://blog.kaggle.com/2014/08/01/learning-from-the-best/

A NN can be viewed from the perspective of representation learning.
Consider a classification task.
Since the final layer of a NN is a linear model, in order for the network to produce accuracte predictions, the previous layers should be able to project the data into a space where the classes are linearly separable.
Thus the network learns a representation of the data that is optimal for classification.

Each of the simple but non-linear modules of a NN transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level.
With the composition of enough such transformations, very complex functions can be learned [@Lecun2015].
These transformations can emphasise (and create) features that are important for discrimination and drop those which are redundant.

Let us work through a simple example to illustrate the representations learned by a NN.
Consider a dataset with two classes; the two curves on a plane shown in \autoref{fig:simple_dataset}.
Clearly, the observations from the two classes are not linearly separable in their raw form.
Thus if we fit a single layer NN (*i.e.* only an output layer) to this data, we will get an unsatisfactory decision boundary, since the decision boundary can only be linear, as shown in \autoref{fig:simple_dataset_simpleNN}.
However, if fit a two-layer NN, where the hidden layer has two neurons and a sigmoid acitvation, to the same dataset the decision boundary perfectly separates the two classes.
This is shown in \autoref{fig:simple_dataset_complexNN}.

![Simple dataset with two linearly inseparable classes.\label{fig:simple_dataset}](figures/simple_dataset.pdf)

![Decision boundary of 1-layer NN.\label{fig:simple_dataset_simpleNN}](figures/simple_dataset_simpleNN.pdf)

![Decision boundary of 2-layer NN.\label{fig:simple_dataset_complexNN}](figures/simple_dataset_complexNN.pdf)

Since the hidden layer consists of only two neurons, we are able to plot the output from the hidden layer after the raw data has passed through it.
This is depicted in \autoref{fig:simple_dataset_complexNN_rep}.
This shows how the hidden layer projected the input data into a space where the observations from the two classes are linearly separable.
Which leaves it to the final layer to find the best hyperplane between them.

![Hidden representation of 2-layer NN.\label{fig:simple_dataset_complexNN_rep}](figures/simple_dataset_complexNN_rep.pdf)

Of course this is a very simple example, but the same concepts apply to more complicated datasets and models.
However, even though it is technically possible to separate any arrangement of points with a sufficiently large network[^colah], in reality it can become quite challenging to find such representations.
This is where the need for more data, regularisation, smarter optimsation procedures and architecture design arises.
Without the aforementioned, it is likely that the network will get stuck in a sub-optimal local minima, not being able to find the optimal representation of the data.
In the following chapters we explore the approaches available to find optimal represenations of tabular data for classification and regression tasks.

[^colah]: http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/

## Vanilla MLP on Tabular Data \label{sec:vanilla_example}

In this chapter we have discussed the basics of NNs and how to train them.
To compliment this discussion we will run through a relatively simple but real-world classification task on tabular data here.
The aim of this is example is to solidfy the understanding of the basics, compare the expected beahviour and the behaviour in practise for each of the discussed concepts and to highlight where sticking with the basics comes up short.

We are going to use the Adult dataset.
We are going to see how we process the features.
How we are going to choose the learning rate and batch size and weight decay and number of epochs.
How we are going to choose the size of the layers and the depth of the network.
Different activation functions?

This acts as a baseline to which we can compare the network with advanced components.

Identify things we can improve upon.
Better preprocessing of the data. 
One-hot encode creates sparse input and requires large weight matrices.
Continuous variables requires zero mean and unit variance normalisation - but is this the best way?
We are naively combining very different types of features in one layer.
The dataset is relatively small.
We struggle to find the right balance between overfitting and underfitting.
Fixed learning rate.
Can't interpret it.






