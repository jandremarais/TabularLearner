# Neural Networks \label{chp:nn}

## Introduction

Not unlike most supervised machine learning models, an artificial neural network is a function which maps inputs to outputs, *i.e.* $f:\boldsymbol{x}\to y.$
The structure of $f$ is often loosely compared to the structure of the human brain. 
Oversimplified, the brain consists of a collection of interconnected neurons. 
Each neuron can generate and receive signals. 
A received signal may be described as an input to a neuron, whereas a sent signal may be described as an output from that neuron.
If two neurons are connected, it means that the output from the one neuron serves as input to the other.
In a very simple model of the brain, one may argue that a neuron receives several signals, which it weighs and combines, and if the combined value of the inputs is higher than a certain threshold, the neuron sends an output signal to the next neuron.
\autoref{fig:neurons} (a) provides a schematic of a biological neuron.

An artifical neural network tries to mimic this model of the human brain: it is set up to consist of several layers of connected units (or neurons).
With exception of units in the first and final layers, each unit outputs a weighted combination of its inputs, combined with a simple non-linear transformation.
In each layer of the neural network, the input is passed through each of the neurons. 
In turn, their output is passed to the next layer.

The transformation at each neuron is controlled by a set of parameters, also known as weights. 
Training a neural network involves tuning these weights in order to obtain some desired output. 
During training, the neural network receives as input a set of training data. 
The neural network weights are then learned in such a way that, when given a new set of inputs, the output predicted by the neural network matches the corresponding response of interest as closely as possible.
The process of using the training data to tweak the weights is done by means of an optimisation algorithm called Stochastic Gradient Descent (SGD).

Although recently there has been plenty of excitement around advances in neural networks, it is well known that they were invented many years ago.
The development of neural networks dates back at least as far as the invention of perceptrons in [@rosenblatt1962].
It is also interesting to compare modern neural networks with the Projection Pursuit Regression algorithm in statistics [@friedman1981].
Only recently a series of breakthroughs caused neural networks to become more effective, leading to the renewed interest in the field.

The aim of this chapter is to provide an overview of neural networks, emphasising their basic structure (\Sref{sec:structure}) and the way in which they are trained (\Sref{sec:training}).
This is done with a view to discuss modern neural network structures and training policies in \Cref{chp:dl}, which in turn will help us shed light on Deep Learning for tabular data. In (\Sref{sec:basicreg}) and (\Sref{sec:lr}), regularisation for neural networks and the use of adaptive learning rates are discussed. These are necessary components for regulating the generalisation performance of NNs, as well as for keeping the required training time at bay.
The chapter concludes with a section on representation learning (\Sref{sec:rep}), which is an important topic toward understanding the inner workings of neural networks.

## The Structure of a Neural Network \label{sec:structure}

### Neurons and Layers

In basic terms, a neural network processes an input $\boldsymbol{x}$ by sending it through a series of layers. 
The neurons in each layer apply some transformation to their inputs, resulting in a set of outputs which are again passed on to the next layer of neurons. 
Eventually, the final layer produces the neural network output. 
In this section we provide more detail regarding the neural network structure. 
We start with a description of the operations inside each neuron, followed by a discussion of the way in which the neurons may be connected in layers in order to form a complete neural network structure.
Our discussion is based upon a simple regression example.

Suppose we are in pursuit of a function which is able to estimate some continuous target, $y$, given a $p$-dimensional input $\boldsymbol{x}$, *e.g.* estimating the taxi fare from features such as distance travelled, time elapsed and number of passengers.
A single neuron may act as such a function.
It models $y$ by computing a weighted average of the input features.
This operation is illustrated in \autoref{fig:neurons} (b).

```{r, fig.cap="Comparison of a biological (a) and an artificial (b) neuron[^imcred].\\label{fig:neurons}"}
# library(png)
library(jpeg)
library(grid)
library(gridExtra)
img1 <- rasterGrob(as.raster(readJPEG("figures/biological_neuron.jpg")), interpolate = FALSE)
img2 <- rasterGrob(as.raster(readJPEG("figures/single_neuron.jpg")), interpolate = FALSE)
t1 <- textGrob('(a)')
t2 <- textGrob('(b)')
grid.arrange(img1, img2, t1, t2, ncol = 2, heights=9:1)
```

[^imcred]: Image credit: https://www.jeremyjordan.me/intro-to-neural-networks/

In equation form, this function may be written as
$$
w_{1}\cdot x_{1} + w_{2}\cdot x_{2}+ \dots + w_{p}\cdot x_{p}+b=y,
$$
where $\{w_{k}\}_{k=1}^{p}$, are the weights applied to each of the inputs $\{x_{k}\}_{k=1}^{p}$ and where $b$ denotes the constant bias term.
Clearly, this equation is simply the very common linear model and thus also can be written as
$$
\boldsymbol{w}^{\intercal}\boldsymbol{x} + b=y,
$$
where $\boldsymbol{x}=[x_{1}\quad x_{2}\quad \dots\quad x_{p}]^{\intercal}$ represents the input, and where resepctively $\boldsymbol{w}=[w_{1}\quad w_{2}\quad \dots\quad w_{p}]^{\intercal}$ and $y$ denote the weights and output.
We may of course compress the above equation to $\boldsymbol{w}^{\intercal}\boldsymbol{x}=y$, where $\boldsymbol{w}$ is amended to include the bias term, and where $\boldsymbol{x}$ is amended to include a 1, *i.e.* $\boldsymbol{x}=[1\quad x_{1}\quad\dots\quad\ x_{p}]^{\intercal}$ is the input, and $\boldsymbol{w}=[b\quad w_{1}\quad \dots\quad w_{p}]^{\intercal}$ is the weight vector.

The weights convey the importance of each input feature in predicting the target.
Larger values of $|w_{k}|$ indicate greater contributions of $x_{k}$ toward the output.
If $w_{k}=0$, $x_{k}$ has no influence on the target.
However the weights are unknown and need to be estimated.

As discussed in Chapter 1, in linear regression this is done by means of OLS.
However, since a neural network consists of many inter-conncected neurons, an alternative estimation procedure is required. 
This is the topic of the next section.

Often a linear model will be too rigid to model a certain response of interest. 
In order to fit a more flexible model, we may add more neurons.
Consider the use of two neurons, $z{1}$ and $z{2}$, where the second neuron ($z{2}$) accepts the same input as the first neuron ($z{1}$), but uses a different set of weights.
Thus we have two different outputs produced by the two neurons, *i.e.* $z_{1}=\boldsymbol{w}_{1}^{\intercal}\boldsymbol{x}$ and $z_{2}=\boldsymbol{w}_{2}^{\intercal}\boldsymbol{x}$.
In order to produce a final estimate from the initial two estimates, *viz.* $z_{1}$ and $z_{2}$, they are passed to a third neuron.
That is, $y=\boldsymbol{w}_{3}^{\intercal}\boldsymbol{z}$, where $\boldsymbol{z}=[z_{1}\quad z_{2}]^{\intercal}$.
\autoref{fig:simple_nn} illustrates this pipepline in network form.

```{r, fig.cap="A simple neural network accepting $p$-sized inputs, with one hidden layer consisting of two neurons.\\label{fig:simple_nn}", out.width="200px", fig.align='center'}
knitr::include_graphics('figures/simple_nn.png')
```

The first two neurons each received all $p$ inputs and each produced a single output. 
These two outputs were received by the third neuron, and combined in order to produce the final output, *viz.* $y$. 
The operations performed by $z{1}$ and $z{2}$ may be expressed as $\boldsymbol{z}=W\boldsymbol{x}^{\intercal}$, where
$$
 W=\begin{bmatrix}
 \boldsymbol{w}_{1}^{\intercal} \\
 \boldsymbol{w}_{2}^{\intercal}
 \end{bmatrix}=
 \begin{bmatrix}
 w_{10} & w_{11} & w_{12} & \dots & w_{1p}\\
 w_{20} & w_{21} & w_{22} & \dots & w_{2p}\\
 \end{bmatrix} \quad \text{and}\quad \boldsymbol{z}=[z_{1}\quad z_{2}]^{\intercal}.
$$

The collection of $z{1}$ and $z{2}$ is called a layer.
Since our third neuron (which is also a layer but with a single neuron) receives the output of this layer as input, it is possible to express the complete input-output relationship in one equation, *i.e.*
$$
y=\boldsymbol{w}_{3}^{\intercal}\boldsymbol{z}=\boldsymbol{w}_{3}^{\intercal}W\boldsymbol{x}.
$$
Note here that the weights from the first layer, *viz.* $W$, and the weights from the third neuron, *viz.* $\boldsymbol{w}_{3}$, may be collapsed into a single vector $\boldsymbol{w}$, effectively reducing all of the neuron operations to a single neuron representation. Therefore the fitted model is still linear.
In order to fit a non-linear model, a non-linear transformation function has to be applied to the output of each layer. 
This function is called an *activation function*. 

Incorporating an activation function, the neural network equation may be written as
$$
y=a_{2}\left(\boldsymbol{w}_{3}^{\intercal}a_{1}(W\boldsymbol{x})\right),
$$
where $a_{1}$ denotes the activation function applied after the first (linear) layer, and where $a_{2}$ denotes the activation function applied after the final layer.
                           
The introduction of non-linear activation functions serves to enlarge the class of functions that can be approximated by the network. That is, activiation functions enable the network to learn complex non-linear relationships between inputs and outputs.
Next, we briefly discuss various activation functions.

### Activation Functions

Since any simple non-linear and differentiable function can be used as activation function, there are plenty of activation functions to choose from.  
Originally, the *sigmoid* activation function, *viz.* $\text{sigmoid}(x)=\frac{1}{1+e^{-x}}$, was a common choice [@Rumelhart1988].
The S-shape of the sigmoid activation function, and its range between 0 an 1 is illustrated in \autoref{fig:activation_functions} (a).
Note that the reason why the sigmoid function fell out of favour in terms of its use as activation function in neural networks is because of issues related to the gradient based optimisation procedure of NNs. For example, gradient weight updates that veer to far in different directions are caused by the values of sigmoid activations that are not centered around zero. 
Some other issues with the sigmoid activiation function are discussed in more detail in \Cref{sec:training}. 

```{r, fig.cap="Plots of various activation functions (a) and their local derivatives (b).\\label{fig:activation_functions}", fig.subcap=c('',''), out.width='50%', fig.show='hold'}
knitr::include_graphics(c('figures/activation_functions.pdf', 'figures/activation_functions_derivative.pdf'))
```

The hyperbolic tangent or *tanh* activation function, on the other hand, does return outputs centered around zero.
It takes the form $\text{tanh}(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$ and its shape is illustrated in \autoref{fig:activation_functions} (a).
However, the problem with both the sigmoid and tanh activation functions is that they may lead to saturated gradients during training.
To see this, consider the tails of the sigmoid and tanh functions, which indicate that the gradients of both these functions tend to zero as $|x|\to\infty$. 
During training, this may cause weight updates to be nearly zero, resulting in the network getting stuck at a certain point in the parameter space.
Furthermore, the maximum gradient of the sigmoid activation function turns out to be only 0.25 (at $x=0.5$). The nature of the chain rule therefore causes lower layers in the network to train much slower than higher layers.
The tanh activation function typically have larger derivatives than the sigmoid function. Thus it is not as susceptible to this vanishing gradient problem. However, it is still not immune to it.
The local derivatives of the activation functions discussed in this section are illustrated in \autoref{fig:activation_functions} (b). The form of these functions will become more apparent in \Cref{sec:training}.

To date, the most popular choice in activation function is the *Rectified Linear Units* (ReLU) non-linearity.
It is defined as:
$$
\text{relu}(x)=\begin{cases}x\quad \text{if }x>0\\ 0\quad\text{otherwise}\end{cases}.
$$ 

The shape of the ReLU activiation and of its derivative are illustrated in \autoref{fig:activation_functions} (a) and (b), respectively.
Use of the ReLU alleviates the gradient vanishing problem as the derivative of this function is always 1 (for positive $x$ values).
This results in significantly shorter steps to convergence, as found by the authors in [@Krizhevsky2012].
However, ReLUs may suffer from the "dead ReLU" problem, which is, if a ReLU neuron gets clamped to zero, then its weights will get zero gradient and may remain permanently "dead" during training.
This sparsity of the activations is what some believe is the reason for the effectiveness of ReLUs [@Sun2014].
If dead activations needs to be avoided, alternative activations functions to apply include PReLUs [@He2015] and Leaky ReLUs [@Maas2013]. 

Selecting an appropriate activation function for a specific task typically involves a trial-and-error process.
For general tasks, the use of ReLUs after each hidden layer may suffice and has to some extent become standard practice.
In the context of classification, for each class, it is often useful to produce outputs between 0 and 1 as estimates of conditional class probabilities. Therefore one often uses a sigmoid activation function on the output layer.
In the context of (single label) multiclass classifcation, it is also desirable for these ouputs to sum to 1, therefore in this case, an activation function called *softmax* is typically used.
Note that the softmax activation, *viz.* $\text{softmax}(\boldsymbol{x})_{k}=\frac{e^{x_{k}}}{\sum_{l=1}^{K}x_{l}}$ is simply the logit transformation introduced in \Sref{sec:SLT}.
In a regression context, we mostly omit the use of an activation function on the output layer.

In our empirical work, we will experiment with the use of the different activation functions discussed in this section, and compare their performance to that of *Scaled Exponential Linear Units* (SELUs) [@Klambauer2017], where the latter is supposed to facilitate more effective training of deeper neural networks.
Note that the depth of a neural network refers to its number of hidden layers, whereas the width of a layer refers to the number of neurons it consists of.
Selecting network depth and layer width is the topic of the next section.

### Size of the Network \label{sec:netsize}

The network depth and the width of its hidden layers (*i.e.* the size of the network) are hyperparameters of the model.
They control the ability of a neural network to model complex functions, which is also often referred to as its flexibility.
In statistical learning it is well known that increasing the flexibility of a model is typically only beneficial up to a certain point, whereafter a further increase in flexibility will be detrimental to its prediction performance on new unseen data cases.
Suboptimal test performance due to a too flexible model is known as *overfitting*.
Appropriate selection of the flexibility of the model is also important in the case of neural networks.
The challenge is to find a network size which is large enough to capture all the complexities in the data, but small enough to avoid overfitting. In addition, whereas more layers facilitate a more flexible fit, larger networks require more time and more hardware capacity in order to train them.

Currently the best way of finding the optimal size of a network for a given problem is by means of experimentation.
Note therefore that for many of the components of neural networks in deep learning, hyperparameter values are selected through a process of trial-and-error. Whereas appropriate specification of the size of a neural network is certainly important, in \Sref{sec:basicreg}, we will see why tuning the size of a network is not necessarily the best way to control overfitting.

Theoretically, according to the universal approximation theorem [@Cybenko1989], a neural network with a single hidden layer and with a finite number of neurons can approximate any continuous function. This begs the question: why are additional hidden layers required?
As stated in [@Ba2013], although a neural network can represent any function, it does not mean that the available learning algorithm is able to find these optimal weights. Moreover, it may be the case that the number of neurons needed in order for a single hidden layer network to represent a specific function of interest, is infeasibly large.
By choosing deeper networks we are assuming that the function we are trying to learn is composed of several simpler functions.
Incorporating this prior belief has empirically been shown to be useful [@Goodfellow2016, pp.197-198]. This is especially true in the case of tasks that may be partitioned into smaller subtasks, for example in computer vision.

In our empirical work we investigate the effect of network depth and layer width on the generalisation performance of neural networks.
We also analyse why networks used in the case of tabular data are typically much shallower than in the case of computer vision or NLP applications. For some additional insight in the matter, later on in this chapter we view the problem of the specification of the network size from a representation learning perspective. 

## Training a Neural Network \label{sec:training}

Briefly, basic training of a neural network entails the following four steps:

1. Initialisation: random numbers are assigned to the network parameters.
2. Forward propagation: the input is passed through the network layers in order to produce an output.
3. Error calculation: the predicted output is compared to the true output, and the difference measures by means of an appropriate objective function.
4. Backward propagation: the gradients of the objective function with respect to the weights are obtained, and the network weights are updated accordingly.

The above steps are typically repeated until the loss function is found to converge. Note however that convergence may require many training epochs. The following four subsections are each devoted to a discussion of one of the aforementioned steps.

### Weight Initialisation

Taining a neural network starts with a weight initialisation step.
In order to think about sensible initialisation, first note that we expect the number of positive and negative weights of a well trained neural network to be equal.
If we initialise all weights to be zero, each neuron will compute the same output. Consequently, each neuron will produce the same gradient and undergo the same weight update.
Hence we want to initialise the weights as small as possible, but each weight should be unique.
Sampling the initial weights from the standard normal distribution seems to be a natural choice. However it turns out that the variance of the outputs from randomly initialised neurons grow as the number of inputs increase.
Therefore an option is scale the weight vector generated from the standard norm distribution by the square root of its number of inputs. Such a scaling step will normalise the variance of the output, ensuring that all neurons in the network initially have approximately the same output distribution. In addition, it serves to improve the rate of convergence when training the network.

Another option for weight initialisation is the proposal by He *et al.* (2015b), which was made specifically in the context of using ReLU activation functions.

### Optimisation \label{sec:optimisation}

We have briefly seen in Chapter 1 that there is a connection between statistical learning and optimisation. 
Optimisation refers to the task of altering $x$ in order to either minimise or maximise some *objective function* $J(x)$. 
When we are minimising the objective function, the latter is often also referred to as the *loss function*, or the *cost*. 
In the remainder of the thesis, note that these different terms for the loss function will be used interchangeably.

As mentioned in Chapter 1, parameter estimation (or optimisation) of a linear (or logistic regression) model is usually done using OLS or maximum likelihood estimation (MLE). 
In this section, however, we discuss an alternative parameter estimation method which is also relevant in the optimisation of neural networks.

Therefore consider the MSE loss function:
$$
\begin{aligned}
L&=\sum_{i=1}^{N}L_{i}\\
&=\sum_{i=1}^{N}\sum_{k=1}^{K}(y_{ik}-f_{k}(\boldsymbol{x}_{i}))^{2}\\
&=\sum_{i=1}^{N}\sum_{k=1}^{K}(y_{ik}-\boldsymbol{w}_{k}^{T}\boldsymbol{x}_{i})^{2},
\end{aligned}
$$
where in this case $f_{k}(\cdot)$ denotes the linear model used to predict the $k$-th class posterior probability. 
Note that although the MSE loss is mostly used in regression and not really well suited for classification, we make use of it here for illustration purposes.

In order to find the weights $\boldsymbol{w}$ that minimise $L$, we follow a process of iterative refinement. 
That is, starting with a random initialisation of $\boldsymbol{w}$, one iteratively updates the values such that $L$ decreases. 
The updating steps are repeated until the loss converges.
To minimise $L$ with respect to $\boldsymbol{w}$, we calculate the gradient of the loss function at the point $L(\boldsymbol{x};\boldsymbol{w})$.
The gradient (or slope) of the loss function indicates the direction in which the function has the steepest rate of increase. 
Once we have determined this direction, we can update the weights by a step in the opposite direction - thereby reaching a smaller value of $L$.

The gradient of $L_{i}$ is computed by obtaining the partial derivative of $L_{i}$ with respect to $\boldsymbol{w}_{k}$, *i.e.*:
$$
\frac{\partial L_{i}}{\partial\boldsymbol{w_{k}}}=-2(y_{ik}-\boldsymbol{w}_{k}^{T}\boldsymbol{x}_{i})\boldsymbol{x}_{i}.
$$

The above gradient is obtained for the loss at each data point, whereafter an update of the weight vector at the $(r+1)$-th iteration may be obtained as
$$
\boldsymbol{w}_{k}^{(r+1)}=\boldsymbol{w}_{k}^{(r)}-\gamma\sum_{i=1}^{n}\frac{\partial L_{i}}{\partial\boldsymbol{w_{k}^{(r)}}},
$$

where $\gamma$ determines the size of the step taken towards the optimal direction and is called the *learning rate*. Of course $\gamma$ needs to be specified by the user. One typically would like to set the learning rate small enough so that one does not overshoot the minimum, but large enough to limit the number of iterations before convergence. 
The learning rate is a crucial parameter when training neural networks. Its significance is discussed in \Sref{sec:lr}.

The procedure of repeatedly evaluating the gradient of the objective function, followed by a parameter update, forms the basis of the optimisation procedure for neural networks and is called *gradient descent* (Cauchy, 1847). 

Note that a weight update is made by evaluating the gradient over a subset of the training observations, *viz.* 
$\{\boldsymbol{x}_{i},i=1,\dots,n\}$. 
One of the advantages of gradient descent is that during each iteration, the gradient need not be computed over the complete training dataset, *i.e.* $n\le N$.
When updates are iteratively determined using subsets of the training data, the process is called *mini-batch gradient descent*.
Of course the gradient obtained using mini-batches is only an approximation of the gradient of the full loss but it seems to be sufficient in practice [@Li2014]. 
The option of using mini-batch gradient descent is extremely helpful in large-scale applications, since it obviates computation of the loss function over the entire training dataset. 
This leads to faster convergence, because of more frequent parameter updates, and allows processing of data sets that are too large to fit in a computer's memory. 
A choice regarding batch size depends on the computation power available.
Typically a batch consists of 64, 128 or 256 data points, since in practice many vectorised operation implementations work faster when their inputs are sized in powers of 2. 
Note at this point that the collection of iterations needed to make one sweep through the training dataset is called an *epoch*.

An extreme case of mini-batch gradient descent is when the batch size is selected to be 1.
This is called *Stochastic Gradient Descent* (SGD). 
Recently SGD has been used much less, since it is more efficient to calculate the gradient in larger batches of training data cases.
However, note that it remains common to use the term SGD when actually referring to mini-batch gradient descent.
The use of gradient descent in general has often been regarded as slow or unreliable. SGD will most probably not even find a local minimum of the objective function, however it typically finds a very low value of the cost function quickly enough to be useful.
Thus gradient descent has been proven to be efficient for optimising neural networks.

### Optimisation Example

In order to illustrate the SGD algorithm, we consider the linear model in a binary classification context, *i.e.* $K=2$.
Also in our example, suppose the training data are generated in the way described in [@Hastie2009, pp. 16-17], where
the inputs are two-dimensional, *i.e.* $p=2$. Suppose we want to fit a linear regression model to the training data and classify an observation to the class with the highest predicted score. 
Of course in the binary classification it is only necessary to model one class probability: an observation is then assigned to the corresponding class if the score exceeds some threshold (usually 0.5).
Therefore the decision boundary is given by $\{\boldsymbol{x}:\boldsymbol{x}^{T}\hat{\boldsymbol{w}}=0.5\}$.

Optimisation of the regression weights by means of gradient descent is illustrated in \autoref{fig:sgd}. 
The colour-shaded regions represent the regions of the input space classified to the respective classes, as determined by the decision boundary based upon the OLS parameter estimates.
Since the number of training observations are small, it was not necessary to make use of mini-batch gradient descent.
Note that the learning rate was set equal to 0.001. 
The decision boundaries defined by the gradient descent parameter estimates at different iterations are represented by the dashed lines in \autoref{fig:sgd}.
Initially the estimated decision boundary lies far from the OLS solution. However, after convergence (29 iterations later), the gradient descent line matches the OLS line. 

```{r, cache=TRUE,eval=TRUE, fig.cap = "Plots of the gradient descent example. (a) The training data points in input space. The shades in the background represent the class division in input space, with the decision boundary determined by linear least squares estimation. The dashed lines represent the gradient descent decision boundaries at different iterations. (b) The loss function at each iteration. \\label{fig:sgd}"}
library(MASS)
library(cowplot)
# generate the data
set.seed(1)

K <- 2
m <- lapply(list(c(1, 0), c(0, 1), c(3, 3))[1:K], 
            function(a) mvrnorm(n = 10, mu = a, Sigma = diag(2)))

X <- lapply(m, function(b) {
  t(sapply(1:100, function(a) {
    mvrnorm(n = 1, mu = b[sample(10, 1), ], Sigma = diag(2)/5)
  }))
})

D <- data.frame(X = do.call("rbind", X), Y = rep(0:(K-1), each = 100))
D <- cbind(X.0 = 1, D)
#D[, -3] <- scale(D[, -3])

# SGD
lin_model <- function(x, b) sum(x * b)
L <- function(y, yhat) sum((y - yhat)^2)

set.seed(125)
B <- mvrnorm(1, mu = c(0, 0, 0), Sigma = diag(3))
B_mat <- B
yhat <- apply(D[, -3], 1, function(a) lin_model(a, B))
loss <- L(D$Y, yhat)
lr <- 0.001
for(i in 1:200) {
  gradient <- c(-2*t(as.matrix(D[, -4])) %*% (D$Y - yhat))
  B_new <- B - lr * gradient
  
  yhat <- apply(D[, -4], 1, function(a) lin_model(a, B_new))
  loss <- c(loss, L(D$Y, yhat))
  
  if(abs(loss[i+1] - loss[i]) < 0.00001) {
    return()
  } else {
    B <- B_new
    B_mat <- rbind(B_mat, B_new)
  }
  #B <- B_new
}

x <- as.matrix(D[, -4])
Bhat <- solve(t(x)%*%x)%*%t(x)%*%D$Y

d_bounds <- t(apply(B_mat, 1, function(a) c(slope = -a[2]/a[3], intercept = (0.5-a[1])/a[3])))
rownames(d_bounds) <- NULL
d_bounds <- data.frame(iteration = 0:(nrow(d_bounds)-1), d_bounds)
d_bounds <- d_bounds[c(1, 5, 10, 30), ]

xlims <- range(D$X.1) * 1.1
ylims <- range(D$X.2) * 1.1

shade_x <- seq(xlims[1], xlims[2], len = 100)
shade_y <- -shade_x*Bhat[2]/Bhat[3] + (0.5 - Bhat[1])/Bhat[3]
shade_y[shade_y>ylims[2]] <- ylims[2]

db_coords <- t(sapply(1:nrow(d_bounds), function(a) {
  ycut <- xlims[2] * d_bounds[a, "slope"] + d_bounds[a, "intercept"]
  if(ycut < ylims[2]) {
    c(xlims[2], ycut)
  } else {
    c((ylims[2] - d_bounds[a, "intercept"])/ d_bounds[a, "slope"], ylims[2])
  }
}))
colnames(db_coords) <- c("dx", "dy")
d_bounds <- cbind(d_bounds, db_coords)

library(ggplot2)
library(dplyr)
library(latex2exp)
# Plot
p <- D %>% 
  ggplot() + 
  geom_point(aes(X.1, X.2, color = factor(Y)), show.legend = FALSE)+
  #coord_fixed() +
  theme(panel.background = element_rect(fill = "white")) +
  geom_abline(data = d_bounds, aes(slope = slope, intercept = intercept), linetype = "dashed") +
  geom_ribbon(data = data.frame(shade_x, shade_y),
            aes(x = shade_x, ymin = ylims[1], ymax = shade_y), alpha = 0.15, fill = "red") +
  geom_ribbon(data = data.frame(shade_x, shade_y),
            aes(x = shade_x, ymin = shade_y, ymax = ylims[2]), alpha = 0.15, fill = "blue") +
  scale_y_continuous(expand = c(0, 0), name = TeX("$X_2$"), breaks = NULL) + 
  scale_x_continuous(expand = c(0, 0), name = TeX("$X_1$"), breaks = NULL) +
  geom_text(data = d_bounds, aes(dx, dy, label = paste0("i=", iteration)), hjust = c(1, rep(0, 3)), 
            vjust = c(0, rep(1, 3)), nudge_y = c(0, rep(-0.02, 3)), nudge_x = c(-0.02, rep(0, 3))) +
  panel_border(colour = "black")

loss_data <- data.frame(loss = loss, iteration = 0:(length(loss) - 1)) 
library(ggthemes)
p_loss <- loss_data %>% 
  ggplot(aes(iteration, loss)) + 
  geom_line(color = "blue") +
  theme(axis.ticks.y = element_blank()) +
  # theme(panel.background = element_rect(fill = "white"),
  #       axis.ticks.x = element_blank()) +
  geom_segment(data = loss_data[c(1,5,10,30), ], aes(x = iteration, xend = iteration, yend = 0), linetype = "dashed") +
  scale_y_continuous(expand = c(0, 0), labels = NULL) + 
  scale_x_continuous(expand = c(0.05, 0), breaks = c(0, 4, 9, 29))

plot_grid(p, p_loss, labels = c("(a)", "(b)"), nrow = 1, align = "vh", rel_widths = c(3,2))
```


### Backpropagation \label{sec:backprop}

Currently, SGD is the most effective way of training deep neural networks. Recall that in \Cref{sec:optimisation} we described how to fit a linear model using the SGD optimisation procedure. We have seen that SGD optimises the parameters $\theta$ of a network to minimise the loss function. That is,
$$
\theta = \arg\min_{\theta}\frac{1}{N}\sum_{i=1}^{N}l(\boldsymbol{x}_{i}, \theta).
$$

Using SGD, training a neural network involves several iterations. During each iteration we consider a mini-batch consisting of $n\le N$ training examples. 
The mini-batch is used to approximate the gradient of the loss function with respect to the parameters via the following derivative: 
$$
\frac{1}{n}\frac{\partial l(\boldsymbol{x}_{i},\theta)}{\partial \theta}.
$$

Using a mini-batch of training examples instead of one training example at a time produces a better estimate of the gradient over the full training set, and is computationally much more efficient.

In this section we discuss the same procedure, but applied to a simple single hidden layer NN for multiclass classification, which may be decomposed as follows:
$$
\begin{aligned}
f_{k}(\boldsymbol{x})&=g_{k}(\boldsymbol{\beta}^{\intercal}_{k}\boldsymbol{z}), \quad k=1,\dots,K\\
z_{m}&=\sigma(\boldsymbol{\alpha}_{m}^{\intercal}\boldsymbol{x}),\quad m=1,\dots, M
\end{aligned}
$$
where $\sigma(\cdot)$ is the sigmoid activation and where $g(\cdot)$ is the softmax activation.
Here there are two sets of unknown adjustable weights that defines the input-output function of the network. 
They are the parameters of the linear function of the inputs, *viz.* $\boldsymbol{\alpha}_{m}=(\alpha_{0m}, \alpha_{1m},\dots,\alpha_{pm})$, and also the parameters of the linear transformation of the derived features, *viz.* $\boldsymbol{\beta}_{k}=(\beta_{0k}, \beta_{1k},\dots,\beta_{mk})$.
If we denote the complete set of parameters by $\theta$, then recall that the objective function for regression may be chosen to be the sum of squared errors, *i.e.* we have
$$
L(\theta) = \sum_{k=1}^{K}\sum_{i=1}^{N}\left(y_{ik}-f_{k}(\boldsymbol{x}_{i})\right)^{2},
$$

whereas in the context of classification, the loss function may be specified as the so-called *cross-entropy*.
The latter loss function is defined as follows:
$$
L(\theta) = -\sum_{i=1}^{N}\sum_{k=1}^{K}y_{ik}\log f_{k}(\boldsymbol{x}_{i}),
$$

with the corresponding classifier denoted by $G(\boldsymbol{x})=\arg\max_{k}f_{k}(\boldsymbol{x})$.
Since a neural network for classification takes the form of a linear logistic regression model in the hidden units, note that the parameters may be estimated using maximum likelihood. 
According to @Hastie2009 [p. 395] however, the global minimiser of $L(\theta)$ is most likely an overfit solution. Therefore we require regularisation techniques when minimising $L(\theta)$.
Furthermore, as the size of the network increases, MLE soon becomes intractable.

Therefore, one rather uses gradient descent and the *backpropogation* algorithm [@Rumelhart1988] to minimise $L(\theta)$.
This is possible because of the modular nature of a neural network, allowing the gradients to be derived through iteration of the chain rule for differentiation. In broad terms, the iterative calculation of derivatives occur during a forward and backward sweep over the network, keeping track only of quantities local to each unit.

In more detail, the backpropogation algorithm for the sum-of-squared error objective function, previously given as
$$
\begin{aligned}
L(\theta)&=\sum_{i=1}^{N}L_{i}\\
&=\sum_{i=1}^{N}\sum_{k=1}^{K}(y_{ik}-f_{k}(\boldsymbol{x}_{i}))^{2},
\end{aligned}
$$
is as follows. 
We start by obtaining the derivatives required in order to implement gradient descent. For this example, following the chain rule, the relevant derivatives are

$$
\begin{aligned}
\frac{\partial L_{i}}{\partial \beta_{km}}&=-2(y_{ik}-f_{k}(\boldsymbol{x}_{i}))g_{k}'(\boldsymbol{\beta}_{k}^{T}\boldsymbol{z}_{i})z_{mi},\\
\frac{\partial L_{i}}{\partial \alpha_{ml}}&=-\sum_{k=1}^{K}2(y_{ik}-f_{k}(\boldsymbol{x}_{i}))g_{k}'(\boldsymbol{\beta}_{k}^{T}\boldsymbol{z}_{i})\beta_{km}\sigma'(\boldsymbol{\alpha}_{m}^{T}\boldsymbol{x}_{i})x_{il}.
\end{aligned}
$$

Given these derivatives, a gradient descent update at the $(r+1)$-th iteration takes the form
$$
\begin{aligned}
\beta_{km}^{(r+1)}&=\beta_{km}^{(r)}-\gamma_{r}\sum_{i=1}^{N}\frac{\partial L_{i}}{\partial \beta_{km}^{(r)}},\\
\alpha_{ml}^{(r+1)}&=\alpha_{ml}^{(r)}-\gamma_{r}\sum_{i=1}^{N}\frac{\partial L_{i}}{\partial \alpha_{ml}^{(r)}}.
\end{aligned}
$$

We may now rewrite the gradients as follows:
$$
\begin{aligned}
\frac{\partial L_{i}}{\partial \beta_{km}}&=\delta_{ki}z_{mi},\\
\frac{\partial L_{i}}{\partial \alpha_{ml}}&=s_{mi}x_{il}.
\end{aligned}
$$
Note that the quantities $\delta_{ki}$ and $s_{mi}$ are errors from the current model at the output and hidden layer units respectively. 
From their definitions it can be seen that
$$
s_{mi}=\sigma'(\boldsymbol{\alpha}_{m}^{T}\boldsymbol{x}_{i})\sum_{k=1}^{K}\beta_{km}\delta_{ki},
$$
which is known as the so-called *backpropagation equations*.
Using the above set of equations, weight updates proceed by means of an algortihm consisting of a forward and a backward pass over the network. 
In the forward pass, the current weights are fixed and the predicted values $\hat{f}_{k}(\boldsymbol{x}_{i})$ are computed. 
In the backward pass, the errors $\delta_{ki}$ are computed, and then channelled via the backpropogation equations in order to specify $s_{mi}$. 
These values are then used to update the weights. 

We have seen that backpropagation is a simple algorithm.
It can easily be implemented on any size network and differentiable layers.
Its local nature (each hidden unit only passes information to and from its connected units) allows it to be implented efficiently in parallel. Another advantage is that the computation of the gradient can be done on a batch of training observations. 
This allows the network to be trained on very large datasets. 

Having discussed basic NN training, we now turn to a discussion of two components that are very important in ensuring neural networks to be useful. Basic ways to regularise a neural network, and the use of adaptive learning rates are discussed in the following two sections.

## Basic Regularisation \label{sec:basicreg}

In \Sref{sec:netsize} the importance of selecting an appropriate size for a neural network was emphasised. It may seem that smaller networks should generally be preferred in order to prevent an overfit.
However smaller networks are harder to train with local methods such as gradient descent because their loss functions have poor local minima that are easy to converge to. In contrast, local minima of larger networks are typically smaller [@Choromanska2014].

There are indeed more effective ways of regularising neural networks. We briefly discuss these methods in the remainder of this section, and in Chapter 3. One of the preferred ways of preventing overfitting in neural networks is by using L1 or L2 regularisation, *i.e.* by adding a penalty term to the objective function, where the penalty term is proportional to the magnitude of the NN weights.
The role of the penalty term is to encourage the weight estimates to be small.
This is of course the same strategy as the one followed when regularising the least squares linear model by means of Ridge Regression or the Lasso [@Hastie2009, Ch. 4].
The difference between L1 and L2 regularisation lies in the form of the penalty term, which is $\lambda |w|$ in L1 regularisation, and $\frac{1}{2}\lambda w^{2}$ in the case of L2 regularisation.
Specification of the $\lambda$ parameter is important since it determines the severity of the penalty of large NN weight values.
Note that the "$\frac{1}{2}$" in front of the L2 penalty is added for the sake of convenience since it renders the derivative of the penalty term equal to $\lambda w$. 
The latter form implies a linear decay of the weights towards zero, *i.e.* $w'=w-\lambda w$, which is also known as *weight decay*. 
In neural network applications it has been shown that L2 regularisation typically outperforms L1 regularisation and is therefore preferred (Reference?)

An alternative way of preventing a neural network to overfit is so-called *early termination* of the training process. That is, one refrains from training the network until the training loss converges. Since the training loss after convergence is not equivalent to the test loss, the loss function on a validation set should be monitored during training. Early termination involves stopping the training process as soon as the validation loss stops decreasing. 

> As mentioned before, the learning rate also plays a big part in finding the optimal weights.
> Next we discuss how we can tune the learning rate to train faster and to find better local minima.
> More advanced regularisation techniques are discussed in \Cref{chp:dl}.

## Adaptive Learning Rates \label{sec:lr}

Although in the neural network literature it is known that a more optimal learning rate may reduce the time to train the network and improve its test performance, optimal specification of this parameter is not such an easy task. 
A small learning rate slows downs the training time, but is safer against overfitting and overshooting the optimal solution.
With a large learning rate, convergence may be reached quicker, but the optimal solution may not have been found. 
Toward appropriate specification of the learning rate one may consider doing a line search over a range of possible values. However in the case of large networks such an approach is typically too time consuming. 

In contrast to a once off specification of the learning rate, an alternative approach is to allow the learning rate to be adapted during the training process. A popular approach is to decrease the learning rate by a fraction after a fixed number of epochs, or as soon as the validation loss starts to converge (*cf.* for example He *et al.*, 2015a). The intuition is that larger steps may be afforded while the weight estimates are still far away from an optimal position on the loss surface, while smaller steps need to gradually be taken once closer to the optimal weight vector value in order to take care not to overshoot it.
Note that an adaptive learning rate requires one to also tune the rate of decrease and the time steps of each decrease during training.
Fortunately it is believed that neural network learning algorithms are not very sensitive to these choices.

In addition, there are ways of manipulating the learning rates at a local level, as opposed to the aforementioned global methods.
Adagrad [@Duchi2011] is an adaptive learning rate method which increases the learning rate at neurons with small gradients and *vice versa*. 
Adam [@Kingma2014] is the most commonly used weight update approach.
It also uses the magnitude of the gradient to control each weight update, in addition to the previous iteration's gradients and it combines them in a smooth fashion.
This resembles the physical property of momentum [@Bengio2012].
For more detail, the reader can refer to the cited publications as it falls out of the scope of this work.

## Representation Learning \label{sec:rep}

We are now familiar with the mathematical operations of basic layers, how they are connected and how their weights are tweaked to minimise a loss function.
In this section we discuss why this works and what the neural network is actually doing to model the data.
The central idea is that of a *data representation* [@Bengio2013] and that at each layer of the network the data is transformed into a higher-level abstraction of itself.
Understanding and interpreting neural networks remains a challenge [@Frosst2017], but the notion of learning an optimal data representation allows us to gain a deeper intuition of the inner mechanics of neural networks.

Machine learning models are very sensitive to the form and properties of the input it is presented with.
Thus a large part of constructing machine learning models is to find the best way of representing the raw data in order to simplify the extraction of useful information.
This *feature engineering* process typically is a laborious manual task which entails creating, analyzing, evaluating and selecting appropriate features[^feature_engineering]. There is therefore unfortunately no systematic recipe for feature engineering. Instead, it is a trial-and-error process which requires practitioner expertise and domain-specific knowledge. 
In representation learning, the idea is to find a way of effectively automating the feature engineering process. That is, the goal is to automatically learn representations of the data that make it easier to extract useful information for classifiers or other predictors [@Bengio2013]. Such automation seems to have the potential of saving a lot of time and raising the performance ceiling of machine learning models.

[^feature_engineering]: http://blog.kaggle.com/2014/08/01/learning-from-the-best/

Importantly, a neural network may be viewed from the perspective of representation learning.
To see this, consider a classification task.
Since the final layer of a neural network is a linear model, in order for the network to produce accuracte predictions, the previous layers should be able to project the data into a space where the classes are linearly separable.
Thus the network needs to learn a representation of the data that is optimal for classification.

Starting with the raw input, each of the simple (but non-linear) modules of a neural network transforms the data representation at one level into a representation at a higher, slightly more abstract level. Each transformation may create and/or emphasise features that are important for discrimination, and drop those which are redundant. When a sufficient number of such transformations are combined, very complex functions may be learned by a neural network [@Lecun2015].

In order to illustrate the data representations learned by a neural network, consider the following simple example. 
Suppose we have available a dataset with two classes, *viz.* the two curves on a plane as displayed in \autoref{fig:simple_dataset}.
In their original form, clearly the observations from the two classes are not linearly separable.
If we fit a single layer neural network to these data (*i.e.* a network with only an output layer), the resulting decision boundary can only be linear (as shown in \autoref{fig:simple_dataset_simpleNN}) and will thus be unsatisfactory. 
However, if we fit a two-layer neural network to the same dataset (where the hidden layer has two neurons and a sigmoid activation), the resulting decision boundary perfectly separates the two classes (as shown in \autoref{fig:simple_dataset_complexNN}).

![Simple dataset with two linearly inseparable classes.\label{fig:simple_dataset}](figures/simple_dataset.pdf)

![Decision boundary of a single-layer neural network.\label{fig:simple_dataset_simpleNN}](figures/simple_dataset_simpleNN.pdf)

![Decision boundary of a two-layer neural network.\label{fig:simple_dataset_complexNN}](figures/simple_dataset_complexNN.pdf)

Since the hidden layer consists of only two neurons, we are able to plot the output from the hidden layer after the raw data has passed through it. This is depicted in \autoref{fig:simple_dataset_complexNN_rep}.
From \autoref{fig:simple_dataset_complexNN_rep it can be seen how the hidden layer projects the input data into a space where the observations from the two classes are linearly separable, which then leaves it to the final layer to find the best hyperplane between the two classes.

![Hidden representation of a two-layer neural network.\label{fig:simple_dataset_complexNN_rep}](figures/simple_dataset_complexNN_rep.pdf)

Although the above example represents a very simple data setup and neural network architechture, the same concepts apply to more complicated datasets and models.
It should however be noted that although it is technically possible to separate any arrangement of points with a sufficiently large network[^colah], in reality it can become quite challenging to find such representations.
This is where the need for more data, regularisation, smarter optimisation procedures and architecture design arises.
Without the aforementioned, it is likely that the network will get stuck in a sub-optimal local minimum, unable to find the optimal representation of the data.
In the chapters to follow we explore the approaches available to find optimal representations for tabular data in regression and classification contexts.

[^colah]: http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/






