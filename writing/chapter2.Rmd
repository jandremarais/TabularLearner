# Neural Networks \label{chp:nn}

## Introduction

Not unlike most supervised machine learning models, an artificial neural network is a function which maps inputs to outputs, *i.e.* $f:\boldsymbol{x}\to y.$
The structure of $f$ is often loosely compared to the structure of the human brain. 
Oversimplified, the brain consists of a collection of interconnected neurons. 
Each neuron can generate and receive signals. 
A received signal may be described as an input to a neuron, whereas a sent signal may be described as an output from that neuron.
If two neurons are connected, it means that the output from the one neuron serves as input to the other.
In a very simple model of the brain, one may argue that a neuron receives several signals, which it weighs and combines, and if the combined value of the inputs is higher than a certain threshold, the neuron sends an output signal to the next neuron.
\autoref{fig:neurons} (a) provides a schematic of a biological neuron.

An artifical neural network tries to mimic this model of the human brain: it is set up to consist of several layers of connected units (or neurons).
With exception of units in the first and final layers, each unit outputs a weighted combination of its inputs, combined with a simple non-linear transformation.
In each layer of the neural network, the input is passed through each of the neurons. 
In turn, their output is passed to the next layer.

The transformation at each neuron is controlled by a set of parameters, also known as weights. 
Training a neural network involves tuning these weights in order to obtain some desired output. 
During training, the neural network receives as input a set of training data. 
The neural network weights are then learned in such a way that, when given a new set of inputs, the output predicted by the neural network matches the corresponding response of interest as closely as possible.
The process of using the training data to tweak the weights is done by means of an optimisation algorithm called Stochastic Gradient Descent (SGD).

Although recently there has been plenty of excitement around advances in neural networks, it is well known that they were invented many years ago.
The development of neural networks dates back at least as far as the invention of perceptrons in [@rosenblatt1962].
It is also interesting to compare modern neural networks with the Projection Pursuit Regression algorithm in statistics [@friedman1981].
Only recently a series of breakthroughs caused neural networks to become more effective, leading to the renewed interest in the field.

The aim of this chapter is to provide an overview of neural networks, emphasising their basic structure (\Sref{sec:structure}) and the way in which they are trained (\Sref{sec:training}).
This is done with a view to discuss modern neural network structures and training policies in \Cref{chp:dl}, which in turn will help us shed light on Deep Learning for tabular data.
The chapter concludes with a section on representation and manifold learning (\Sref{sec:rep}), which are important topics toward understanding the inner workings of neural networks.

## The Structure of a Neural Network \label{sec:structure}

### Neurons and Layers

In basic terms, a neural network processes an input $\boldsymbol{x}$ by sending it through a series of layers. 
The neurons in each layer apply some transformation to their inputs, resulting in a set of outputs which are again passed on to the next layer of neurons. 
Eventually, the final layer produces the neural network output. 
In this section we provide more detail regarding the neural network structure. 
We start with a description of the operations inside each neuron, followed by a discussion of the way in which the neurons may be connected in layers in order to form a complete neural network structure.
Our discussion is based upon a simple regression example.

Suppose we are in pursuit of a function which is able to estimate some continuous target, $y$, given a $p$-dimensional input $\boldsymbol{x}$, *e.g.* estimating the taxi fare from features such as distance travelled, time elapsed and number of passengers.
A single neuron may act as such a function.
It models $y$ by computing a weighted average of the input features.
This operation is illustrated in \autoref{fig:neurons} (b).

```{r, fig.cap="Comparison of a biological (a) and an artificial (b) neuron[^imcred].\\label{fig:neurons}"}
# library(png)
library(jpeg)
library(grid)
library(gridExtra)
img1 <- rasterGrob(as.raster(readJPEG("figures/biological_neuron.jpg")), interpolate = FALSE)
img2 <- rasterGrob(as.raster(readJPEG("figures/single_neuron.jpg")), interpolate = FALSE)
t1 <- textGrob('(a)')
t2 <- textGrob('(b)')
grid.arrange(img1, img2, t1, t2, ncol = 2, heights=9:1)
```

[^imcred]: Image credit: https://www.jeremyjordan.me/intro-to-neural-networks/

In equation form, this function may be written as
$$
w_{1}\cdot x_{1} + w_{2}\cdot x_{2}+ \dots + w_{p}\cdot x_{p}+b=y,
$$
where $\{w_{k}\}_{k=1}^{p}$, are the weights applied to each of the inputs $\{x_{k}\}_{k=1}^{p}$ and where $b$ denotes the constant bias term.
Clearly, this equation is simply the very common linear model and thus also can be written as
$$
\boldsymbol{w}^{\intercal}\boldsymbol{x} + b=y,
$$
where $\boldsymbol{x}=[x_{1}\quad x_{2}\quad \dots\quad x_{p}]^{\intercal}$ represents the input, and where resepctively $\boldsymbol{w}=[w_{1}\quad w_{2}\quad \dots\quad w_{p}]^{\intercal}$ and $y$ denote the weights and output.
We may of course compress the above equation to $\boldsymbol{w}^{\intercal}\boldsymbol{x}=y$, where $\boldsymbol{w}$ is amended to include the bias term, and where $\boldsymbol{x}$ is amended to include a 1, *i.e.* $\boldsymbol{x}=[1\quad x_{1}\quad\dots\quad\ x_{p}]^{\intercal}$ is the input, and $\boldsymbol{w}=[b\quad w_{1}\quad \dots\quad w_{p}]^{\intercal}$ is the weight vector.

The weights convey the importance of each input feature in predicting the target.
Larger values of $|w_{k}|$ indicate greater contributions of $x_{k}$ toward the output.
If $w_{k}=0$, $x_{k}$ has no influence on the target.
However the weights are unknown and need to be estimated.

As discussed in Chapter 1, in linear regression this is done by means of OLS.
However, since a neural network consists of many inter-conncected neurons, an alternative estimation procedure is required. 
This is the topic of the next section.

Often a linear model will be too rigid to model a certain response of interest. 
In order to fit a more flexible model, we may add more neurons.
Consider the use of two neurons, $z{1}$ and $z{2}$, where the second neuron ($z{2}$) accepts the same input as the first neuron ($z{1}$), but uses a different set of weights.
Thus we have two different outputs produced by the two neurons, *i.e.* $z_{1}=\boldsymbol{w}_{1}^{\intercal}\boldsymbol{x}$ and $z_{2}=\boldsymbol{w}_{2}^{\intercal}\boldsymbol{x}$.
In order to produce a final estimate from the initial two estimates, *viz.* $z_{1}$ and $z_{2}$, they are passed to a third neuron.
That is, $y=\boldsymbol{w}_{3}^{\intercal}\boldsymbol{z}$, where $\boldsymbol{z}=[z_{1}\quad z_{2}]^{\intercal}$.
\autoref{fig:simple_nn} illustrates this pipepline in network form.

```{r, fig.cap="A simple neural network accepting $p$-sized inputs, with one hidden layer consisting of two neurons.\\label{fig:simple_nn}", out.width="200px", fig.align='center'}
knitr::include_graphics('figures/simple_nn.png')
```

The first two neurons each received all $p$ inputs and each produced a single output. 
These two outputs were received by the third neuron, and combined in order to produce the final output, *viz.* $y$. 
The operations performed by $z{1}$ and $z{2}$ may be expressed as $\boldsymbol{z}=W\boldsymbol{x}^{\intercal}$, where
$$
 W=\begin{bmatrix}
 \boldsymbol{w}_{1}^{\intercal} \\
 \boldsymbol{w}_{2}^{\intercal}
 \end{bmatrix}=
 \begin{bmatrix}
 w_{10} & w_{11} & w_{12} & \dots & w_{1p}\\
 w_{20} & w_{21} & w_{22} & \dots & w_{2p}\\
 \end{bmatrix} \quad \text{and}\quad \boldsymbol{z}=[z_{1}\quad z_{2}]^{\intercal}.
$$

The collection of $z{1}$ and $z{2}$ is called a layer.
Since our third neuron (which is also a layer but with a single neuron) receives the output of this layer as input, it is possible to express the complete input-output relationship in one equation, *i.e.*
$$
y=\boldsymbol{w}_{3}^{\intercal}\boldsymbol{z}=\boldsymbol{w}_{3}^{\intercal}W\boldsymbol{x}.
$$
Note here that the weights from the first layer, *viz.* $W$, and the weights from the third neuron, *viz.* $\boldsymbol{w}_{3}$, may be collapsed into a single vector $\boldsymbol{w}$, effectively reducing all of the neuron operations to a single neuron representation. Therefore the fitted model is still linear.
In order to fit a non-linear model, a non-linear transformation function has to be applied to the output of each layer. 
This function is called an *activation function*. 

Incorporating an activation function, the neural network equation may be written as
$$
y=a_{2}\left(\boldsymbol{w}_{3}^{\intercal}a_{1}(W\boldsymbol{x})\right),
$$
where $a_{1}$ denotes the activation function applied after the first (linear) layer, and where $a_{2}$ denotes the activation function applied after the final layer.
                           
The introduction of non-linear activation functions serves to enlarge the class of functions that can be approximated by the network. That is, activiation functions enable the network to learn complex non-linear relationships between inputs and outputs.
Next, we briefly discuss various activation functions.

### Activation Functions

Since any simple non-linear and differentiable function can be used as activation function, there are plenty of activation functions to choose from.  
Originally, the *sigmoid* activation function, *viz.* $\text{sigmoid}(x)=\frac{1}{1+e^{-x}}$, was a common choice [@Rumelhart1988].
The S-shape of the sigmoid activation function, and its range between 0 an 1 is illustrated in \autoref{fig:activation_functions} (a).
Note that the reason why the sigmoid function fell out of favour in terms of its use as activation function in neural networks is because of issues related to the gradient based optimisation procedure of NNs. For example, gradient weight updates that veer to far in different directions are caused by the values of sigmoid activations that are not centered around zero. 
Some other issues with the sigmoid activiation function are discussed in more detail in \Cref{sec:training}. 

```{r, fig.cap="Plots of various activation functions (a) and their local derivatives (b).\\label{fig:activation_functions}", fig.subcap=c('',''), out.width='50%', fig.show='hold'}
knitr::include_graphics(c('figures/activation_functions.pdf', 'figures/activation_functions_derivative.pdf'))
```

The hyperbolic tangent or *tanh* activation function, on the other hand, does return outputs centered around zero.
It takes the form $\text{tanh}(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$ and its shape is illustrated in \autoref{fig:activation_functions} (a).
However, the problem with both the sigmoid and tanh activation functions is that they may lead to saturated gradients during training.
To see this, consider the tails of the sigmoid and tanh functions, which indicate that the gradients of both these functions tend to zero as $|x|\to\infty$. 
During training, this may cause weight updates to be nearly zero, resulting in the network getting stuck at a certain point in the parameter space.
Furthermore, the maximum gradient of the sigmoid activation function turns out to be only 0.25 (at $x=0.5$). The nature of the chain rule therefore causes lower layers in the network to train much slower than higher layers.
The tanh activation function typically have larger derivatives than the sigmoid function. Thus it is not as susceptible to this vanishing gradient problem. However, it is still not immune to it.
The local derivatives of the activation functions discussed in this section are illustrated in \autoref{fig:activation_functions} (b). The form of these functions will become more apparent in \Cref{sec:training}.

To date, the most popular choice in activation function is the *Rectified Linear Units* (ReLU) non-linearity.
It is defined as:
$$
\text{relu}(x)=\begin{cases}x\quad \text{if }x>0\\ 0\quad\text{otherwise}\end{cases}.
$$ 

The shape of the ReLU activiation and of its derivative are illustrated in \autoref{fig:activation_functions} (a) and (b), respectively.
Use of the ReLU alleviates the gradient vanishing problem as the derivative of this function is always 1 (for positive $x$ values).
This results in significantly shorter steps to convergence, as found by the authors in [@Krizhevsky2012].
However, ReLUs may suffer from the "dead ReLU" problem, which is, if a ReLU neuron gets clamped to zero, then its weights will get zero gradient and may remain permanently "dead" during training.
This sparsity of the activations is what some believe is the reason for the effectiveness of ReLUs [@Sun2014].
If dead activations needs to be avoided, alternative activations functions to apply include PReLUs [@He2015] and Leaky ReLUs [@Maas2013]. 

Selecting an appropriate activation function for a specific task typically involves a trial-and-error process.
For general tasks, the use of ReLUs after each hidden layer may suffice and has to some extent become standard practice.
In the context of classification, for each class, it is often useful to produce outputs between 0 and 1 as estimates of conditional class probabilities. Therefore one often uses a sigmoid activation function on the output layer.
In the context of (single label) multiclass classifcation, it is also desirable for these ouputs to sum to 1, therefore in this case, an activation function called *softmax* is typically used.
Note that the softmax activation, *viz.* $\text{softmax}(\boldsymbol{x})_{k}=\frac{e^{x_{k}}}{\sum_{l=1}^{K}x_{l}}$ is simply the logit transformation introduced in \Sref{sec:SLT}.
In a regression context, we mostly omit the use of an activation function on the output layer.

In our empirical work, we will experiment with the use of the different activation functions discussed in this section, and compare their performance to that of *Scaled Exponential Linear Units* (SELUs) [@Klambauer2017], where the latter is supposed to facilitate more effective training of deeper neural networks.
Note that the depth of a neural network refers to its number of hidden layers, whereas the width of a layer refers to the number of neurons it consists of.
Selecting network depth and layer width is the topic of the next section.

### Size of the Network \label{sec:netsize}

The network depth and the width of its hidden layers (*i.e.* the size of the network) are hyperparameters of the model.
They control the ability of a neural network to model complex functions, which is also often referred to as its flexibility.
In statistical learning it is well known that increasing the flexibility of a model is typically only beneficial up to a certain point, whereafter a further increase in flexibility will be detrimental to its prediction performance on new unseen data cases.
Suboptimal test performance due to a too flexible model is known as *overfitting*.
Appropriate selection of the flexibility of the model is also important in the case of neural networks.
The challenge is to find a network size which is large enough to capture all the complexities in the data, but small enough to avoid overfitting. In addition, whereas more layers facilitate a more flexible fit, larger networks require more time and more hardware capacity in order to train them.

Currently the best way of finding the optimal size of a network for a given problem is by means of experimentation.
Note therefore that for many of the components of neural networks in deep learning, hyperparameter values are selected through a process of trial-and-error. Whereas appropriate specification of the size of a neural network is certainly important, in \Sref{sec:basicreg}, we will see why tuning the size of a network is not necessarily the best way to control overfitting.

Theoretically, according to the universal approximation theorem [@Cybenko1989], a neural network with a single hidden layer and with a finite number of neurons can approximate any continuous function. This begs the question: why are additional hidden layers required?
As stated in [@Ba2013], although a neural network can represent any function, it does not mean that the available learning algorithm is able to find these optimal weights. Moreover, it may be the case that the number of neurons needed in order for a single hidden layer network to represent a specific function of interest, is infeasibly large.
By choosing deeper networks we are assuming that the function we are trying to learn is composed of several simpler functions.
Incorporating this prior belief has empirically been shown to be useful [@Goodfellow2016, pp.197-198]. This is especially true in the case of tasks that may be partitioned into smaller subtasks, for example in computer vision.

In our empirical work we investigate the effect of network depth and layer width on the generalisation performance of neural networks.
We also analyse why networks used in the case of tabular data are typically much shallower than in the case of computer vision or NLP applications. For some additional insight in the matter, later on in this chapter we view the problem of the specification of the network size from a representation learning perspective. 

## Training a Neural Network \label{sec:training}

### Optimisation \label{sec:optimisation}

We have briefly seen in Chapter 1 that there is a connection between statistical learning and optimisation. 
Optimisation refers to the task of altering $x$ in order to either minimise or maximise some *objective function* $J(x)$. 
When we are minimising the objective function, the latter is often also referred to as the *loss function*, or the *cost*. 
In the remainder of the thesis, note that these different terms for the loss function will be used interchangeably.

As mentioned in Chapter 1, parameter estimation (or optimisation) of a linear (or logistic regression) model is usually done using OLS or maximum likelihood estimation (MLE). 
In this section, however, we discuss an alternative parameter estimation method which is also relevant in the optimisation of neural networks.

Therefore consider the MSE loss function:
$$
\begin{aligned}
L&=\sum_{i=1}^{N}L_{i}\\
&=\sum_{i=1}^{N}\sum_{k=1}^{K}(y_{ik}-f_{k}(\boldsymbol{x}_{i}))^{2}\\
&=\sum_{i=1}^{N}\sum_{k=1}^{K}(y_{ik}-\boldsymbol{w}_{k}^{T}\boldsymbol{x}_{i})^{2},
\end{aligned}
$$
where in this case $f_{k}(\cdot)$ denotes the linear model used to predict the $k$-th class posterior probability. 
Note that although the MSE loss is mostly used in regression and not really well suited for classification, we make use of it here for illustration purposes.

In order to find the weights $\boldsymbol{w}$ that minimise $L$, we follow a process of iterative refinement. 
That is, starting with a random initialisation of $\boldsymbol{w}$, one iteratively updates the values such that $L$ decreases. 
The updating steps are repeated until the loss converges.
In order to minimise $L$ with respect to $\boldsymbol{w}$, we calculate the gradient of the loss function at the point $L(\boldsymbol{x};\boldsymbol{w})$.
The gradient (or slope) of the loss function indicates the direction in which the function has the steepest rate of increase. 
Therefore, once we have determined this direction, we can update the weights by a step in the opposite direction - thereby reaching a smaller value of $L$.

The gradient of $L_{i}$ is computed by obtaining the partial derivative of $L_{i}$ with respect to $\boldsymbol{w}_{k}$, *i.e.*:
$$
\frac{\partial L_{i}}{\partial\boldsymbol{w_{k}}}=-2(y_{ik}-\boldsymbol{w}_{k}^{T}\boldsymbol{x}_{i})\boldsymbol{x}_{i}.
$$
After obtaining the above ${N}$ partial derivatives, an update at the $(r+1)$-th iteration may be obtained as follows:
$$
\boldsymbol{w}_{k}^{(r+1)}=\boldsymbol{w}_{k}^{(r)}-\gamma\sum_{i=1}^{n}\frac{\partial L_{i}}{\partial\boldsymbol{w_{k}^{(r)}}},
$$
where $\gamma$ is called the *learning rate* and determines the size of the step taken towards the optimal direction. 
One typically would like to set the learning rate small enough so that one does not overshoot the minimum, but large enough to limit the number of iterations before convergence. 
The learning rate is a crucial parameter when training neural networks; we will discuss its significance in \Sref{sec:lr}.

The procedure of repeatedly evaluating the gradient of the objective function and then performing a parameter update, is called *gradient descent* (Cauchy, 1847). 
Gradient descent forms the basis of the optimisation procedure for neural networks.

Note that a weight update is made by evaluating the gradient over a set of observations, $\{\boldsymbol{x}_{i},i=1,\dots,n\}$. 
One of the advantages of gradient descent is that at an iteration, the gradient need not be computed over the complete training dataset, *i.e.* $n\le N$.
When updates are iteratively determined by using subsets of the data, the process is called *mini-batch gradient descent*.
This is extremely helpful in large-scale applications, since it obviates computation of the full loss function over the entire dataset. 
This leads to faster convergence, because of more frequent parameter updates, and allows processing of data sets that are too large to fit into a computer's memory. 
The choice regarding batch size depends on the available computation power. 
Typically a batch consists of 64, 128 or 256 data points, since in practice many vectorised operation implementations work faster when their inputs are sized in powers of 2. 
The gradient obtained using mini-batches is only an approximation of the gradient of the full loss but it seems to be sufficient in practice [@Li2014]. 
Note at this point that the collection of iterations needed to make one sweep through the training data set is called an *epoch*.

The extreme case of mini-batch gradient descent is when the batch size is selected to be 1.
This is called *Stochastic Gradient Descent* (SGD). 
Recently SGD has been used much less, since it is more efficient to calculate the gradient in larger batches compared to only using one example. 
However, note that it remains common to use the term SGD when actually referring to mini-batch gradient descent.
Gradient descent in general has often been regarded as slow or unreliable but it works well for optimising neural networks.
SGD will most probably not find even a local minimum of the objective function. 
It typically however finds a very low value of the cost function quickly enough to be useful.

### Optimisation Example

To illustrate the SGD algorithm, consider the linear model in a classification context. 
Suppose we are given a training dataset with two-dimensional inputs and only two possible classes.
Let the data be generated in the same way as described in [@Hastie2009, pp. 16-17]. 

We want to fit a linear regression model to the data such that we can classify an observation to the class with the highest predicted score. 
In the binary case it is only necessary to model one class probability and then assign an observation to that class if the score exceeds some threshold (usually 0.5), otherwise it is assigned to the other class. 
Therefore the decision boundary is given by $\{\boldsymbol{x}:\boldsymbol{x}^{T}\hat{\boldsymbol{w}}=0.5\}$.

The example is illustrated in \autoref{fig:sgd}. 
The colour shaded regions represent the parts of the input space classified to the respective classes, as determined by the decision boundary based upon OLS parameter estimates.
Gradient descent was applied to the determine the optimal weights using a learning rate of 0.001. 
Since the total number of training observations are small, it is not necessary to use SGD. 
In \autoref{fig:sgd}, the dashed lines represent the decision boundary defined by the gradient descent parameter estimates at different iterations.
We observe that initially the estimated decision boundary is far from the OLS solution, but as the update iterations proceed, the decision boundary is rotated and translated until finally matching the OLS line. 
It took 29 iterations for the procedure to reach convergence.

```{r, cache=TRUE,eval=TRUE, fig.cap = "Plots of the gradient descent example. (a) The data points in input space. The shades in the background represent the class division in input space, with the decision boundary determined by linear least squares estimation. The dashed lines represent the decision boundaries learned at different iterations. (b) The loss calculated at each iteration. \\label{fig:sgd}"}
library(MASS)
library(cowplot)
# generate the data
set.seed(1)

K <- 2
m <- lapply(list(c(1, 0), c(0, 1), c(3, 3))[1:K], 
            function(a) mvrnorm(n = 10, mu = a, Sigma = diag(2)))

X <- lapply(m, function(b) {
  t(sapply(1:100, function(a) {
    mvrnorm(n = 1, mu = b[sample(10, 1), ], Sigma = diag(2)/5)
  }))
})

D <- data.frame(X = do.call("rbind", X), Y = rep(0:(K-1), each = 100))
D <- cbind(X.0 = 1, D)
#D[, -3] <- scale(D[, -3])

# SGD
lin_model <- function(x, b) sum(x * b)
L <- function(y, yhat) sum((y - yhat)^2)

set.seed(125)
B <- mvrnorm(1, mu = c(0, 0, 0), Sigma = diag(3))
B_mat <- B
yhat <- apply(D[, -3], 1, function(a) lin_model(a, B))
loss <- L(D$Y, yhat)
lr <- 0.001
for(i in 1:200) {
  gradient <- c(-2*t(as.matrix(D[, -4])) %*% (D$Y - yhat))
  B_new <- B - lr * gradient
  
  yhat <- apply(D[, -4], 1, function(a) lin_model(a, B_new))
  loss <- c(loss, L(D$Y, yhat))
  
  if(abs(loss[i+1] - loss[i]) < 0.00001) {
    return()
  } else {
    B <- B_new
    B_mat <- rbind(B_mat, B_new)
  }
  #B <- B_new
}

x <- as.matrix(D[, -4])
Bhat <- solve(t(x)%*%x)%*%t(x)%*%D$Y

d_bounds <- t(apply(B_mat, 1, function(a) c(slope = -a[2]/a[3], intercept = (0.5-a[1])/a[3])))
rownames(d_bounds) <- NULL
d_bounds <- data.frame(iteration = 0:(nrow(d_bounds)-1), d_bounds)
d_bounds <- d_bounds[c(1, 5, 10, 30), ]

xlims <- range(D$X.1) * 1.1
ylims <- range(D$X.2) * 1.1

shade_x <- seq(xlims[1], xlims[2], len = 100)
shade_y <- -shade_x*Bhat[2]/Bhat[3] + (0.5 - Bhat[1])/Bhat[3]
shade_y[shade_y>ylims[2]] <- ylims[2]

db_coords <- t(sapply(1:nrow(d_bounds), function(a) {
  ycut <- xlims[2] * d_bounds[a, "slope"] + d_bounds[a, "intercept"]
  if(ycut < ylims[2]) {
    c(xlims[2], ycut)
  } else {
    c((ylims[2] - d_bounds[a, "intercept"])/ d_bounds[a, "slope"], ylims[2])
  }
}))
colnames(db_coords) <- c("dx", "dy")
d_bounds <- cbind(d_bounds, db_coords)

library(ggplot2)
library(dplyr)
library(latex2exp)
# Plot
p <- D %>% 
  ggplot() + 
  geom_point(aes(X.1, X.2, color = factor(Y)), show.legend = FALSE)+
  #coord_fixed() +
  theme(panel.background = element_rect(fill = "white")) +
  geom_abline(data = d_bounds, aes(slope = slope, intercept = intercept), linetype = "dashed") +
  geom_ribbon(data = data.frame(shade_x, shade_y),
            aes(x = shade_x, ymin = ylims[1], ymax = shade_y), alpha = 0.15, fill = "red") +
  geom_ribbon(data = data.frame(shade_x, shade_y),
            aes(x = shade_x, ymin = shade_y, ymax = ylims[2]), alpha = 0.15, fill = "blue") +
  scale_y_continuous(expand = c(0, 0), name = TeX("$X_2$"), breaks = NULL) + 
  scale_x_continuous(expand = c(0, 0), name = TeX("$X_1$"), breaks = NULL) +
  geom_text(data = d_bounds, aes(dx, dy, label = paste0("i=", iteration)), hjust = c(1, rep(0, 3)), 
            vjust = c(0, rep(1, 3)), nudge_y = c(0, rep(-0.02, 3)), nudge_x = c(-0.02, rep(0, 3))) +
  panel_border(colour = "black")

loss_data <- data.frame(loss = loss, iteration = 0:(length(loss) - 1)) 
library(ggthemes)
p_loss <- loss_data %>% 
  ggplot(aes(iteration, loss)) + 
  geom_line(color = "blue") +
  theme(axis.ticks.y = element_blank()) +
  # theme(panel.background = element_rect(fill = "white"),
  #       axis.ticks.x = element_blank()) +
  geom_segment(data = loss_data[c(1,5,10,30), ], aes(x = iteration, xend = iteration, yend = 0), linetype = "dashed") +
  scale_y_continuous(expand = c(0, 0), labels = NULL) + 
  scale_x_continuous(expand = c(0.05, 0), breaks = c(0, 4, 9, 29))

plot_grid(p, p_loss, labels = c("(a)", "(b)"), nrow = 1, align = "vh", rel_widths = c(3,2))
```


### Backpropogation \label{sec:backprop}

In \Cref{sec:optimisation} we discussed how to fit a linear model using the Stochastic Gradient Descent optimisation procedure.
Currenlty, SGD is the most effective way of training deep networks. 
To recap, SGD optimises the parameters $\theta$ of a network to minimise the loss,
$$
\theta = \arg\min_{\theta}\frac{1}{N}\sum_{i=1}^{N}l(\boldsymbol{x}_{i}, \theta).
$$
With SGD the training proceeds in steps and at each step we consider a mini-batch of size $n\le N$ training samples. 
The mini-batch is used to approximate the gradient of the loss function with respect to the paramaters by computing, 
$$
\frac{1}{n}\frac{\partial l(\boldsymbol{x}_{i},\theta)}{\partial \theta}.
$$
Using a mini-batch of samples instead of one at a time produces a better estimate of the gradient over the full training set and it is computationally much more efficient.

This section discusses the same procedure, but applied to a simple single hidden layer neural network for multiclass classification, decomposed as:
$$
\begin{aligned}
f_{k}(\boldsymbol{x})&=g_{k}(\boldsymbol{\beta}^{\intercal}_{k}\boldsymbol{z}), \quad k=1,\dots,K\\
z_{m}&=\sigma(\boldsymbol{\alpha}_{m}^{\intercal}\boldsymbol{x}),\quad m=1,\dots, M
\end{aligned}
$$
where $\sigma(\cdot)$ is the sigmoid activation and $g(\cdot)$ the softmax activation.
The neural network has a set of unknown adjustable weights that defines the input-output function of the network. 
They are the parameters of the linear function of the inputs, $\boldsymbol{\alpha}_{m}=(\alpha_{0m}, \alpha_{1m},\dots,\alpha_{pm})$, and the paramaters of the linear transformation of the derived features, $\boldsymbol{\beta}_{k}=(\beta_{0k}, \beta_{1k},\dots,\beta_{mk})$.
Denote the complete set of parameters by $\theta$.
Then the objective function for regression can be chosen as the sum-of-squared-errors:
$$
L(\theta) = \sum_{k=1}^{K}\sum_{i=1}^{N}\left(y_{ik}-f_{k}(\boldsymbol{x}_{i})\right)^{2}
$$
and for classification, the cross-entropy:

$$
L(\theta) = -\sum_{i=1}^{N}\sum_{k=1}^{K}y_{ik}\log f_{k}(\boldsymbol{x}_{i}),
$$
with corresponding classifier $G(\boldsymbol{x})=\arg\max_{k}f_{k}(\boldsymbol{x})$.
Since the neural network for classification is a linear logistic regression model in the hidden units, the paramaters can be estimated by maximum likelihood. 
According to @Hastie2009 [p. 395], the global minimiser of $L(\theta)$ is most likely an overfit solution and we instead require regularisation techniques when minimising $L(\theta)$.
Furthermore, as the network becomes larger, MLE becomes intractable.

Therefore, one rather uses gradient descent and the *backpropogation* algortihm [@Rumelhart1988] to minimise $L(\theta)$.
This is possible because of the modular nature of a neural network, allowing the gradients to be derived by iterative application of the chain rule for differentiation. 
This is done by a forward and backward sweep over the network, keeping track only of quantities local to each unit.

In detail, the backpropogation algorithm for the sum-of-squared error objective function,
$$
\begin{aligned}
L(\theta)&=\sum_{i=1}^{N}L_{i}\\
&=\sum_{i=1}^{N}\sum_{k=1}^{K}(y_{ik}-f_{k}(\boldsymbol{x}_{i}))^{2},
\end{aligned}
$$
is as follows. 
Following the chain-rul, the relevant derivatives for gradient descent are:

$$
\begin{aligned}
\frac{\partial L_{i}}{\partial \beta_{km}}&=-2(y_{ik}-f_{k}(\boldsymbol{x}_{i}))g_{k}'(\boldsymbol{\beta}_{k}^{T}\boldsymbol{z}_{i})z_{mi},\\
\frac{\partial L_{i}}{\partial \alpha_{ml}}&=-\sum_{k=1}^{K}2(y_{ik}-f_{k}(\boldsymbol{x}_{i}))g_{k}'(\boldsymbol{\beta}_{k}^{T}\boldsymbol{z}_{i})\beta_{km}\sigma'(\boldsymbol{\alpha}_{m}^{T}\boldsymbol{x}_{i})x_{il}.
\end{aligned}
$$

Given these derivatives, a gradient descent update at the $(r+1)$-th iteration has the form,
$$
\begin{aligned}
\beta_{km}^{(r+1)}&=\beta_{km}^{(r)}-\gamma_{r}\sum_{i=1}^{N}\frac{\partial L_{i}}{\partial \beta_{km}^{(r)}},\\
\alpha_{ml}^{(r+1)}&=\alpha_{ml}^{(r)}-\gamma_{r}\sum_{i=1}^{N}\frac{\partial L_{i}}{\partial \alpha_{ml}^{(r)}}.
\end{aligned}
$$

Now write the gradients as
$$
\begin{aligned}
\frac{\partial L_{i}}{\partial \beta_{km}}&=\delta_{ki}z_{mi},\\
\frac{\partial L_{i}}{\partial \alpha_{ml}}&=s_{mi}x_{il}.
\end{aligned}
$$
The quantities, $\delta_{ki}$ and $s_{mi}$ are errors from the current model at the output and hidden layer units respectively. 
From their definitions, they satify the following,
$$
s_{mi}=\sigma'(\boldsymbol{\alpha}_{m}^{T}\boldsymbol{x}_{i})\sum_{k=1}^{K}\beta_{km}\delta_{ki},
$$
which is known as the backpropogation equations.
Using this, the weight updates can be made with an algortihm consisting of a forward and a backward pass over the network. 
In the forward pass, the current weights are fixed and the predicted values $\hat{f}_{k}(\boldsymbol{x}_{i})$ are computed. 
In the backward pass, the errors $\delta_{ki}$ are computed, and then backpropogated via the backpropogation equations to obtain $s_{mi}$. 
These are then used to update the weights.
This approach extends naturally to any size network and differentiable layers (functions).

Backpropogation is simple and its local nature (each hidden unit passes only information to and from its connected units) allows it to be implented efficiently in parallel. 
The other advantage is that the computation of the gradient can be done on a batch (subset of the training set) of observations. 
This allows the network to be trained on very large datasets. 

In summary, training a neural network consists of these four steps:

1. Initialise the network weights: give random set of numbers to the network parameters.
2. Forward propogation: pass the input through the network layers to produce an output.
3. Calculate the error: compare the predicted output with the true output and measure the difference using an objective function.
4. Backward propogation: calculate the gradients of the objective function with respect to the weights and update the weights accordingly.

These four steps are typically repeated until convergence of the loss function.
It can take many training epochs for the objective function to converge.

### Weight initialisation

Before one begins training a neural network, one needs to initialise its weights.
We expect a well trained neural network to have an equal number of weights smaller than zero and greater than zero.
If we initialise all weights to be zero, every neuron will compute the same output and therefore prodcue the same gradient and undergo the same weight update.
We still want to initialise the weights as small as possible but each one unique.
Sampling from zero mean and unit variance Gaussian distribution is a natural fit, however, the variance of the outputs from a randomly initialised neuron grows with increasing number of inputs.
Therefore we may scale the weight vector by the square root of its number of inputs to normalise the variance to 1.
This ensures that all neurons in the network initially have approximately the same output distribution and empirically improves the rate of convergence.
[@He2015a] derived a weight initialisation specifically for ReLU neurons.

### Basic Regularisation \label{sec:basicreg}

From \Sref{sec:netsize} it seems that smaller neural networks can be preferred if the data is not complex enough to prevent overfitting.
However, this is untrue and there are much better ways of regularising neural networks, which we discuss shortly and in the next chapter.
Smaller networks are harder to train with local methods such as gradient descent because their loss functions have poor local minima that are easy to converge to.
Whereas the local minima of larger networks are much better in terms of their actual loss [@Choromanska2014].

One of the preferred ways of fighting overfitting in neural networks is by using L1 or L2 regularisation, *i.e.* adding a penalty term proportional to the magnitude of the weights to the objective function.
This is exactly what is done in Ridge Regression and the Lasso [@Hastie2009, Ch. 4].
In L2 regularisation we add the term $\frac{1}{2}\lambda w^{2}$ to the objective and in L1 regularisation the penalty term $\lambda |w|$.
These penalties forces the weights to be small and the $\lambda$ parameter controls the strength of the regularisation.
The "$\frac{1}{2}$" in front of the L2 penalty is added for convenience to make the derivative equal to $\lambda w$.
This makes it then equivalent to decaying each of the weights linearly towards zero: $w'=w-\lambda w$, which is also known as *weight decay*.
In practice, L2 regularisation outperforms L1 regularisaion.

Another basic way of preventing the neural network to overfit is to stop the training process "early" and to not train until the training loss converges.
A converged training loss is not equivalent to an optimal test loss and therefore the test or validation loss should be observerd during training.
One approach is to stop training when the validation loss stops decreasing.
As mentioned before, the learning rate also plays a big part in finding the optimal weights.
Next we discuss how we can tune the learning rate to train faster and to find better local minima.
More advanced regularisation techniques are discussed in \Cref{chp:dl}.

### Adaptive Learning Rates and Annealing \label{sec:lr}

Although the right learning rate can reduce training time and improve performance, there is no silver bullet when it comes to setting the parameter.
A small learning rate slows downs the training time, but is safer against overfitting and overshooting the optimal solution.
With a large learning rate, convergence may be reached quicker, but the optimal solution may not have been found. 
One could do a line search of a range of possible values, but this usually takes too long for bigger networks. 

However, one does not need to keep the learning rate fixed throughout the training process.
A popular approach is to decrease the learning rate by a fraction after a certain number of epochs or as the validation loss starts to converge, as done in [@He2015] for example.
The intuition is that larger steps can be taken when we are still far away from an optimal position on the loss surface, and then gradually take smaller steps once we get closer to not overshoot it.
Note, that this means that we would also need to tune the rate of decrease and the time steps of each decrease during training.
Fortunately, it is believed that the learning algorithms are not that sensitive to this choice.

In addition, there are ways of manipulating the learning rates at a local level, as opposed the aforementioned global methods.
Adagrad [@Duchi2011] is an adaptive learning rate method which increases the learning rate at neurons with small gradients and *vice versa*.
Adam [@Kingma2014] is the most commonly used weight update approach.
It also uses the magnitude of the gradient to control each weight update, in addition to the previous iteration's gradients and it combines them in a smooth fashion.
This resembles the physical property of momentum [@Bengio2012].
For more detail, the reader can refer to the cited publications as it falls out of the scope of this work.

## Representation Learning \label{sec:rep}

We are now familiar with the mathematical operations of basic layers, how they are connected and how their weights are tweaked to minimise a loss function.
In this section we will discuss why this works and what the neural network is actually doing to model the data.
The central idea is that of a data *representation* [@Bengio2013] and that at each layer of the network the data is transformed into a higher-level abstraction of itself.
Understanding and interpreting neural networks remains a challenge [@Frosst2017], but the notion of learning an optimal data representation allows us to gain a deeper intuition of the inner mechanics of neural networks.

Machine learning models are very sensitive to the form and the properties of the input given to it.
Thus a large part of building machine learning models is to find the best way of representing the raw data to make it easier for the models to extract useful information.
This is typically a laborious manual task of creating, analyzing, evaluating and selecting appropriate features[^feature_engineering] requires practitioner expertise and domain knowledge.
This *feature engineering* process is more trial-and-error than a systematic recipe.
Therefore if one can effectively automate this process, it will save a lot of time and raise the performance ceiling of models.
Automatically learning representations of the data that make it easier to extract useful information for classifiers or other predictors is called representation learning [@Bengio2013].

[^feature_engineering]: http://blog.kaggle.com/2014/08/01/learning-from-the-best/

A neural network can be viewed from the perspective of representation learning.
Consider a classification task.
Since the final layer of a neural network is a linear model, in order for the network to produce accuracte predictions, the previous layers should be able to project the data into a space where the classes are linearly separable.
Thus the network learns a representation of the data that is optimal for classification.

Each of the simple but non-linear modules of a neural network transforms the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level.
With the composition of enough such transformations, very complex functions can be learned [@Lecun2015].
These transformations can emphasise (and create) features that are important for discrimination and drop those which are redundant.

Let us work through a simple example to illustrate the representations learned by a neural network.
Consider a dataset with two classes; the two curves on a plane shown in \autoref{fig:simple_dataset}.
Clearly, the observations from the two classes are not linearly separable in their raw form.
Thus if we fit a single layer neural network (*i.e.* only an output layer) to this data, we will get an unsatisfactory decision boundary, since the decision boundary can only be linear, as shown in \autoref{fig:simple_dataset_simpleNN}.
However, if we fit a two-layer neural network, where the hidden layer has two neurons and a sigmoid acitvation to the same dataset, the decision boundary perfectly separates the two classes.
This is shown in \autoref{fig:simple_dataset_complexNN}.

![Simple dataset with two linearly inseparable classes.\label{fig:simple_dataset}](figures/simple_dataset.pdf)

![Decision boundary of 1-layer neural network.\label{fig:simple_dataset_simpleNN}](figures/simple_dataset_simpleNN.pdf)

![Decision boundary of 2-layer neural network.\label{fig:simple_dataset_complexNN}](figures/simple_dataset_complexNN.pdf)

Since the hidden layer consists of only two neurons, we are able to plot the output from the hidden layer after the raw data has passed through it.
This is depicted in \autoref{fig:simple_dataset_complexNN_rep}.
This shows how the hidden layer projected the input data into a space where the observations from the two classes are linearly separable.
Which leaves it to the final layer to find the best hyperplane between them.

![Hidden representation of 2-layer neural network.\label{fig:simple_dataset_complexNN_rep}](figures/simple_dataset_complexNN_rep.pdf)

Of course this is a very simple example, but the same concepts apply to more complicated datasets and models.
However, even though it is technically possible to separate any arrangement of points with a sufficiently large network[^colah], in reality it can become quite challenging to find such representations.
This is where the need for more data, regularisation, smarter optimsation procedures and architecture design arises.
Without the aforementioned, it is likely that the network will get stuck in a sub-optimal local minima, not being able to find the optimal representation of the data.
In the following chapters we explore the approaches available to find optimal represenations of tabular data for classification and regression tasks.

[^colah]: http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/






