# Neural Networks \label{chp:nn}

## Introduction

Not unlike most supervised machine learning models, a neural network (NN) is a function which maps inputs to outputs, *i.e.* $f:\boldsymbol{x}\to y.$
The structure of $f$ is often loosely compared to the structure of the human brain. 
Oversimplified, the brain consists of a collection of interconnected neurons. 
Each neuron can generate and receive signals. 
A received signal may be described as an input to a neuron, whereas a sent signal may be described as an output from that neuron.
If two neurons are connected, it means that the output from the one neuron serves as input to the other.
In a very simple model of the brain, one may argue that a neuron receives several signals, which it weighs and combines, and if the combined value of the inputs is higher than a certain threshold, the neuron sends a output signal to the next neuron.
\autoref{fig:neurons} (a) provides a schematic of a biological neuron.

An artifical neural network tries to mimic this model of the human brain - it is set up to consist of several layers of connected units (or neurons).
With exception of units in the first and final layers, each unit outputs a weighted combination of its inputs, combined with a simple non-linear transformation.
In each layer of the neural network, the input is passed through each of the neurons. 
In turn, their output is passed to the next layer.

The transformation at each neuron is controlled by a set of parameters, also known as weights. 
Training a neural network involves tuning these weights in order to obtain some desired output. 
During training, the neural network receives as input a set of training data. 
The neural network weights are then learned in such a way that, when given a new set of inputs, the output predicted by the neural network matches the corresponding response of interest as closely as possible.
The process of using the training data to tweak the weights is done by means of an optimisation algorithm called Stochastic Gradient Descent (SGD).

Although recently there has been plenty of excitement around neural networks, it is well known that they were invented many years ago.
The development of neural networks dates back at least as far as the invention of perceptrons in [@rosenblatt1962].
It is also interesting to compare modern neural networks with the Projection Pursuit Regression algorithm in statistics [@friedman1981].
Only recently a series of breakthroughs allowed neural networks to be more effective, leading to the renewed interest in the field.

The aim of this chapter is to provide an overview of neural networks, emphasising the basic structure (\Sref{sec:structure}) and the way in which they are trained (\Sref{sec:training}).
This is done with a view to discuss modern neural network structures and training policies in \Cref{chp:dl}, which in turn will help us shed light on Deep Learning for tabular data.
Finaly, the chapter concludes with a section on representation and manifold learning (\Sref{sec:rep}) in an attempt to understand what a neural network is actually doing.

## The Structure of a Neural Network \label{sec:structure}

### Neurons and Layers

In basic terms, a neural network processes an input $boldsymbol{x}$ by sending it through a series of layers. 
The neurons in each layer apply some transformation to their inputs, resulting in a set of outputs which are again passed on to the next layer of neurons. 
Eventually, the final layer produces the neural network output. 
In this section we provide more detail regarding the neural network structure. 
We start with a description of the operations inside each neuron, and follow with a discussion of the way in which the neurons may be connected in layers in order to form a complete neural network structure.
Our discussion is based upon a simple regression example.

Suppose we are in pursuit of a function which is able to estimate some continuous target, $y$, given a $p$-dimensional input $\boldsymbol{x}$, *e.g.* estimating the taxi fare from features such as distance travelled, time elapsed and number of passengers.
A single neuron may act as such a function.
It models $y$ by computing a weighted average of the input features.
This operation is illustrated in \autoref{fig:neurons} (b).

```{r, fig.cap="Comparison of a biological (a) and an artificial (b) neuron[^imcred].\\label{fig:neurons}"}
# library(png)
library(jpeg)
library(grid)
library(gridExtra)
img1 <- rasterGrob(as.raster(readJPEG("figures/biological_neuron.jpg")), interpolate = FALSE)
img2 <- rasterGrob(as.raster(readJPEG("figures/single_neuron.jpg")), interpolate = FALSE)
t1 <- textGrob('(a)')
t2 <- textGrob('(b)')
grid.arrange(img1, img2, t1, t2, ncol = 2, heights=9:1)
```

[^imcred]: Image credit: https://www.jeremyjordan.me/intro-to-neural-networks/

In equation form, this function can be written as:
$$
w_{1}\cdot x_{1} + w_{2}\cdot x_{2}+ \dots + w_{p}\cdot x_{p}+b=y,
$$
where $\{w_{k}\}_{k=1}^{p}$, are the weights applied to each of the inputs $\{x_{k}\}_{k=1}^{p}$ and $b$ the constant bias term.
Clearly, this equation is simply the very common linear model and thus also can be written as:
$$
\boldsymbol{w}^{\intercal}\boldsymbol{x} + b=y,
$$
where $\boldsymbol{x}=[x_{1}\quad x_{2}\quad \dots\quad x_{p}]^{\intercal}$ is the input, $\boldsymbol{w}=[w_{1}\quad w_{2}\quad \dots\quad w_{p}]^{\intercal}$ the weights and $y$ the output. 
We may compress the above equation to $\boldsymbol{w}^{\intercal}\boldsymbol{x}=y$, where $\boldsymbol{x}$ includes the bias term and the weight vector $\boldsymbol{w}$ a unit element, *i.e.* $\boldsymbol{x}=[1\quad x_{1}\quad\dots\quad\ x_{p}]^{\intercal}$ is the input, $\boldsymbol{w}=[b\quad w_{1}\quad \dots\quad w_{p}]^{\intercal}$.

The weights convey the importance of each input features in predicting the target.
The larger $|w_{k}|$ is, the greater is the contribution of $x_{k}$ towards the output.
If $w_{k}=0$, $x_{k}$ has no influence on the target.
However the weights are unknown and therefore we need to estimate them.

In linear regression this is done by means of the method of ordinary least squares.
Since a neural network consists of many inter-conncected neurons, an alternative estimation procedure is required. 
This is the topic of the next section.

Often a linear model will be too rigid to model a certain response of interest. 
In order to fit a more flexible model, we may add more neurons.
Consider the use of two neurons, where the second neuron accepts the same input as the first neuron, but uses a different set of weights.
Thus we have two different outputs produced by the two neurons, *i.e.*. $z_{1}=\boldsymbol{w}_{1}^{\intercal}\boldsymbol{x}$ and $z_{2}= \boldsymbol{w}_{2}^{\intercal}\boldsymbol{x}$.
In order to produce a final estimate from the initial two estimates, *viz.* $z_{1}$ and $z_{2}$, they are passed to a third neuron.
That is, $y=\boldsymbol{w}_{3}^{\intercal}\boldsymbol{z}$, where $\boldsymbol{z}=[z_{1}\quad z_{2}]^{\intercal}$.
\autoref{fig:simple_nn} illustrates this pipepline in network form.

```{r, fig.cap="A simple neural network accepting $p$-sized inputs, with one hidden layer which has two neurons.\\label{fig:simple_nn}", out.width="200px", align='center'}
knitr::include_graphics('figures/simple_nn.png')
```

The first two neurons each received all $p$ inputs and each produced a single output. 
These two outputs were received by the third neuron, and combined in order to produce the final output, *viz.* $y$. 
The operations performed by the first two neurons may be expressed as $\boldsymbol{z}=W\boldsymbol{x}^{\intercal}$, where
$$
 W=\begin{bmatrix}
 \boldsymbol{w}_{1}^{\intercal} \\
 \boldsymbol{w}_{2}^{\intercal}
 \end{bmatrix}=
 \begin{bmatrix}
 w_{10} & w_{11} & w_{12} & \dots & w_{1p}\\
 w_{20} & w_{21} & w_{22} & \dots & w_{2p}\\
 \end{bmatrix} \quad \text{and}\quad \boldsymbol{z}=[z_{1}\quad z_{2}]^{\intercal}.
$$

The collection of these two neurons is what is called a layer.
Since our third neuron (which is also a layer but with a single neuron) receives the output of this layer as input, it is possible to express the complete input-output relationship in one equation, *i.e.*
$$
y=\boldsymbol{w}_{3}^{\intercal}\boldsymbol{z}=\boldsymbol{w}_{3}^{\intercal}W\boldsymbol{x}.
$$
Note here that the weights from the first layer, $W$, and the third neuron, $\boldsymbol{w}_{3}$, can be collapsed into a single vector $\boldsymbol{w}$, effectively reducing all of the neuron operations back into a single neuron representation and thus the fitted model is still linear.
In order to fit a non-linear model, a non-linear transformation function is applied to the output of each layer. 
This function is called an *activation function*. 

Subsequently the neural network equation can be written as
$$
y=a_{2}\left(\boldsymbol{w}_{3}^{\intercal}a_{1}(W\boldsymbol{x})\right),
$$
where $a_{1}$ denotes the activation function applied after the first (linear) layer, and where $a_{2}$ is the the activation function applied after the final layer.
                           
The introduction of non-linear activation functions serves to enlarge the class of functions that can be approximated by the network *i.e.* making it possible for the network to learn complex, non-linear relationships between the inputs and the outputs.
Next, we briefly discuss the various activation functions.

### Activation Functions

There are plenty of activation functions to choose from, since any simple non-linear and differentiable function can be used. 
Originally, the *sigmoid* activation function was a common choice [@Rumelhart1988].
It can be expressed as $\text{sigmoid}(x)=\frac{1}{1+e^{-x}}$.
The shape of the sigmoid activation can be seen in \autoref{fig:activation_functions} (a).
It has an S-shape and its values range between 0 and 1.
The reason it fell out of favour is because of issues related to the gradient based optimisation procedure of neural networks (which we discuss in more detail in \Cref{sec:training}).
One of the issues is that the values of sigmoid activations are not centered around zero.
This may cause the gradient weight updates to veer too far in different directions.


```{r, fig.cap="Plots of various activation functions (a) and their local derivatives (b).\\label{fig:activation_functions}", fig.subcap=c('',''), out.width='50%', fig.show='hold'}
knitr::include_graphics(c('figures/activation_functions.pdf', 'figures/activation_functions_derivative.pdf'))
```


The hyperbolic tangent or the *tanh* activation function, on the other hand, returns outputs centered around zero.
It is expressed as $\text{tanh}(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$ and its shape can be seen in \autoref{fig:activation_functions} (a).
But the problem with both the sigmoid and tanh activation functions is that they can cause the gradients to saturate during training.
By observing the tails of the sigmoid and tanh functions, it is clear that the gradient tends to zero as $|x|\to\infty$.
This can cause the weight updates to be very close to zero and thus resulting in the network getting stuck at a certain point in the parameter space.
Furthermore, the maximum gradient of the sigmoid activation function is 0.25 (at $x=0.5$), and because of the nature of the chain rule this causes the lower layers to train much slower than the higher layers.
The tanh activation function typically have larger derivatives than the sigmoid and thus it is not as susceptible to this vanishing gradient problem, however, it is still not immune to it.
The local derivatives of the activation functions discussed here are plotted in \autoref{fig:activation_functions} (b).
This will become more obvious after \Cref{sec:training}.

To date, the most popular choice in activation function is the *Rectified Linear Units* (ReLU) non-linearity.
It is defined as:
$$
\text{relu}(x)=\begin{cases}x\quad \text{if }x>0\\ 0\quad\text{otherwise}\end{cases}.
$$ 
Again, its shape and derivative is plotted in \autoref{fig:activation_functions} (a) and (b), respectively.
The ReLU limits the gradient vanishing problem as its derivative is always 1 when $x$ is positive.
This results in significantly shorter steps to convergence, as found by the authors in [@Krizhevsky2012].
However, ReLUs may suffer from the "dead ReLU" problem, which is, if a ReLU neuron gets clamped to zero, then its weights will get zero gradient and may remain permanently "dead" during training.
This sparsity of the activations is what some believe is the reason for the effectivenss of ReLUs [@Sun2014].
If dead activations wants to be avoided, alternative activations functions are PReLUs [@He2015] and Leaky ReLUs [@Maas2013], but they fall out of the scope of this work.

Typically, choosing the right activation function for a specific task comes down to trial and error, however, in most cases it would be sufficient to use ReLUs after each hidden layer.
When doing classification, it is often useful to produce outputs between 0 and 1 for each category as estimates of the conditional class probabilities.
Therefore we may use a sigmoid activation function on the output layer.
When doing (single label) multiclass classifcation it is also desirable for these ouputs to sum to 1, similar to prababilities.
In that case we may use an activation function called the *softmax*, which is exactly the logit transformation introduced in \Sref{sec:SLT}: $\text{softmax}(\boldsymbol{x})_{k}=\frac{e^{x_{k}}}{\sum_{l=1}^{K}x_{l}}$.
For regression tasks, we mostly omit an activation function on the output layer.

In our experiments we will show how the different activation functions perform on tabular data and compare it to the *Scaled Exponential Linear Units* (SELUs) [@Klambauer2017] activation, which is supposed to allow us to more effectively train deeper neural networks.
The neural network depth refers to its number of hidden layers, whereas the width of a layer refers to the number of neurons it consists of.
The choice in network depth and layer width is the topic of the next section.

### Size of the Network \label{sec:netsize}

The network depth and the width of its hidden layers (*i.e.* the size of the network) are hyperparameters of the model.
They control the capacity or the flexibility of a neural network; referring to its ability to model complex functions.
The bigger the network, the more flexible the model is.
Like with most such parameters, increasing the model size is usually benefical up to a certain point, after which its test performance starts to degrade (overfitting).
In addition, more layers and more neurons adds to the time taken and hardware required to train the neural network.

Thus the challenge is to find a network size that is large enough to capture all the complexities in the data, but small enough to avoid overfitting.
Currently the best way of finding the optimal size of a network for a given problem is by experimentation.
Note, that this is true for many of the compoments of neural networks and deep learning.
Later in this chapter, \Sref{sec:basicreg}, we will see why tunining the network size is not necessarily the best way of controlling overfitting.

Theoretically, according to the universal approximation theorem [@Cybenko1989], a neural network with a single hidden layer and a finite number of neurons can approximate any continuous function.
Which begs the question as to why we need more hidden layers.
Even though a neural network can represent any function, it does not mean that the available learning algorithms can find these optimal weights [@Ba2013].
It may also be that the number of neurons needed for a single hidden layer network to represent a specific function is infeasibly large.
By choosing deeper networks we are assuming that the function we are trying to learn is composed of several simpler functions.
Building in this prior proves to useful empirically [@Goodfellow2016, pp.197-198], especially for tasks of a hierarchical nature like in computer vision.

In our experiments we will see what effect the layer width and network depth have on the generalisation performance on different tabular datasets.
The networks used for tabular data learning problems are typically much shallower than those used for say computer vision and NLP.
We investigate why this is the case.
Later in this chapter we view this problem from a representation learning perspective which will also give us some insight on the matter.
But first, in the next section, we discuss the process of training a neural network.

## Training a Neural Network \label{sec:training}

### Optimisation \label{sec:optimisation}

Statistical learning and optimisation are closely related. 
Optimisation refers to the task of either minimising or maximising some function $J(x)$ by altering $x$. 
The function we want to optimise is called the *objective function*. 
When we are minimising the objective function, we may also refer to the objective function as the *cost* or *loss function*. 
These terms will be used interchangeably throughout the thesis.

As mentioned in the previous chapter, parameter estimation (or optimisation) of a linear (or logistic regression) model is usually done using OLS or maximum likelihood estimation (MLE). 
In this section, however, we discuss an alternative parameter estimation method which is also relevant for the optimisation of neural networks.

Consider the MSE loss function:
$$
\begin{aligned}
L&=\sum_{i=1}^{N}L_{i}\\
&=\sum_{i=1}^{N}\sum_{k=1}^{K}(y_{ik}-f_{k}(\boldsymbol{x}_{i}))^{2}\\
&=\sum_{i=1}^{N}\sum_{k=1}^{K}(y_{ik}-\boldsymbol{w}_{k}^{T}\boldsymbol{x}_{i})^{2},
\end{aligned}
$$
where $f_{k}(\cdot)$ in this case is the linear model used to predict the $k$-th class posterior probability. 
Although the MSE loss is mostly used in a regression setup and not really well suited for classification, we make use of it here for illustration purposes.

To find the weights, $\boldsymbol{w}$, that minimise $L$, we can follow a process of iterative refinement. 
That is, starting with a random initialisation of $\boldsymbol{w}$, one iteratively updates the values such that $L$ decreases. 
The updating steps are repeated until the loss converges.
In order to minimise $L$ with respect to $\boldsymbol{w}$, we calculate the gradient of the loss function at the point $L(\boldsymbol{x};\boldsymbol{w})$.
The gradient (or slope) of the loss function indicates the direction in which the function has the steepest rate of increase. 
Therefore, once we have determined this direction, we can update the weights by a step in the opposite direction - thereby reaching a smaller value of $L$.

The gradient of $L_{i}$ is computed by obtaining the partial derivative of $L_{i}$ with respect to $\boldsymbol{w}_{k}$, *i.e.*:
$$
\frac{\partial L_{i}}{\partial\boldsymbol{w_{k}}}=-2(y_{ik}-\boldsymbol{w}_{k}^{T}\boldsymbol{x}_{i})\boldsymbol{x}_{i}.
$$
After obtaining the above ${N}$ partial derivatives, an update at the $(r+1)$-th iteration may be obtained as follows:
$$
\boldsymbol{w}_{k}^{(r+1)}=\boldsymbol{w}_{k}^{(r)}-\gamma\sum_{i=1}^{n}\frac{\partial L_{i}}{\partial\boldsymbol{w_{k}^{(r)}}},
$$
where $\gamma$ is called the *learning rate* and determines the size of the step taken towards the optimal direction. 
One typically would like to set the learning rate small enough so that one does not overshoot the minimum, but large enough to limit the number of iterations before convergence. 
The learning rate is a crucial parameter when training neural networks; we will discuss its significance in \Sref{sec:lr}.

The procedure of repeatedly evaluating the gradient of the objective function and then performing a parameter update, is called *gradient descent* (Cauchy, 1847). 
Gradient descent forms the basis of the optimisation procedure for neural networks.

Note that a weight update is made by evaluating the gradient over a set of observations, $\{\boldsymbol{x}_{i},i=1,\dots,n\}$. 
One of the advantages of gradient descent is that at an iteration, the gradient need not be computed over the complete training dataset, *i.e.* $n\le N$.
When updates are iteratively determined by using subsets of the data, the process is called *mini-batch gradient descent*.
This is extremely helpful in large-scale applications, since it obviates computation of the full loss function over the entire dataset. 
This leads to faster convergence, because of more frequent parameter updates, and allows processing of data sets that are too large to fit into a computer's memory. 
The choice regarding batch size depends on the available computation power. 
Typically a batch consists of 64, 128 or 256 data points, since in practice many vectorised operation implementations work faster when their inputs are sized in powers of 2. 
The gradient obtained using mini-batches is only an approximation of the gradient of the full loss but it seems to be sufficient in practice [@Li2014]. 
Note at this point that the collection of iterations needed to make one sweep through the training data set is called an *epoch*.

The extreme case of mini-batch gradient descent is when the batch size is selected to be 1.
This is called *Stochastic Gradient Descent* (SGD). 
Recently SGD has been used much less, since it is more efficient to calculate the gradient in larger batches compared to only using one example. 
However, note that it remains common to use the term SGD when actually referring to mini-batch gradient descent.
Gradient descent in general has often been regarded as slow or unreliable but it works well for optimising neural networks.
SGD will most probably not find even a local minimum of the objective function. 
It typically however finds a very low value of the cost function quickly enough to be useful.

### Optimisation Example

To illustrate the SGD algorithm, consider the linear model in a classification context. 
Suppose we are given a training dataset with two-dimensional inputs and only two possible classes.
Let the data be generated in the same way as described in [@Hastie2009, pp. 16-17]. 

We want to fit a linear regression model to the data such that we can classify an observation to the class with the highest predicted score. 
In the binary case it is only necessary to model one class probability and then assign an observation to that class if the score exceeds some threshold (usually 0.5), otherwise it is assigned to the other class. 
Therefore the decision boundary is given by $\{\boldsymbol{x}:\boldsymbol{x}^{T}\hat{\boldsymbol{w}}=0.5\}$.

The example is illustrated in \autoref{fig:sgd}. 
The colour shaded regions represent the parts of the input space classified to the respective classes, as determined by the decision boundary based upon OLS parameter estimates.
Gradient descent was applied to the determine the optimal weights using a learning rate of 0.001. 
Since the total number of training observations are small, it is not necessary to use SGD. 
In \autoref{fig:sgd}, the dashed lines represent the decision boundary defined by the gradient descent parameter estimates at different iterations.
We observe that initially the estimated decision boundary is far from the OLS solution, but as the update iterations proceed, the decision boundary is rotated and translated until finally matching the OLS line. 
It took 29 iterations for the procedure to reach convergence.

```{r, cache=TRUE,eval=TRUE, fig.cap = "Plots of the gradient descent example. (a) The data points in input space. The shades in the background represent the class division in input space, with the decision boundary determined by linear least squares estimation. The dashed lines represent the decision boundaries learned at different iterations. (b) The loss calculated at each iteration. \\label{fig:sgd}"}
library(MASS)
library(cowplot)
# generate the data
set.seed(1)

K <- 2
m <- lapply(list(c(1, 0), c(0, 1), c(3, 3))[1:K], 
            function(a) mvrnorm(n = 10, mu = a, Sigma = diag(2)))

X <- lapply(m, function(b) {
  t(sapply(1:100, function(a) {
    mvrnorm(n = 1, mu = b[sample(10, 1), ], Sigma = diag(2)/5)
  }))
})

D <- data.frame(X = do.call("rbind", X), Y = rep(0:(K-1), each = 100))
D <- cbind(X.0 = 1, D)
#D[, -3] <- scale(D[, -3])

# SGD
lin_model <- function(x, b) sum(x * b)
L <- function(y, yhat) sum((y - yhat)^2)

set.seed(125)
B <- mvrnorm(1, mu = c(0, 0, 0), Sigma = diag(3))
B_mat <- B
yhat <- apply(D[, -3], 1, function(a) lin_model(a, B))
loss <- L(D$Y, yhat)
lr <- 0.001
for(i in 1:200) {
  gradient <- c(-2*t(as.matrix(D[, -4])) %*% (D$Y - yhat))
  B_new <- B - lr * gradient
  
  yhat <- apply(D[, -4], 1, function(a) lin_model(a, B_new))
  loss <- c(loss, L(D$Y, yhat))
  
  if(abs(loss[i+1] - loss[i]) < 0.00001) {
    return()
  } else {
    B <- B_new
    B_mat <- rbind(B_mat, B_new)
  }
  #B <- B_new
}

x <- as.matrix(D[, -4])
Bhat <- solve(t(x)%*%x)%*%t(x)%*%D$Y

d_bounds <- t(apply(B_mat, 1, function(a) c(slope = -a[2]/a[3], intercept = (0.5-a[1])/a[3])))
rownames(d_bounds) <- NULL
d_bounds <- data.frame(iteration = 0:(nrow(d_bounds)-1), d_bounds)
d_bounds <- d_bounds[c(1, 5, 10, 30), ]

xlims <- range(D$X.1) * 1.1
ylims <- range(D$X.2) * 1.1

shade_x <- seq(xlims[1], xlims[2], len = 100)
shade_y <- -shade_x*Bhat[2]/Bhat[3] + (0.5 - Bhat[1])/Bhat[3]
shade_y[shade_y>ylims[2]] <- ylims[2]

db_coords <- t(sapply(1:nrow(d_bounds), function(a) {
  ycut <- xlims[2] * d_bounds[a, "slope"] + d_bounds[a, "intercept"]
  if(ycut < ylims[2]) {
    c(xlims[2], ycut)
  } else {
    c((ylims[2] - d_bounds[a, "intercept"])/ d_bounds[a, "slope"], ylims[2])
  }
}))
colnames(db_coords) <- c("dx", "dy")
d_bounds <- cbind(d_bounds, db_coords)

library(ggplot2)
library(dplyr)
library(latex2exp)
# Plot
p <- D %>% 
  ggplot() + 
  geom_point(aes(X.1, X.2, color = factor(Y)), show.legend = FALSE)+
  #coord_fixed() +
  theme(panel.background = element_rect(fill = "white")) +
  geom_abline(data = d_bounds, aes(slope = slope, intercept = intercept), linetype = "dashed") +
  geom_ribbon(data = data.frame(shade_x, shade_y),
            aes(x = shade_x, ymin = ylims[1], ymax = shade_y), alpha = 0.15, fill = "red") +
  geom_ribbon(data = data.frame(shade_x, shade_y),
            aes(x = shade_x, ymin = shade_y, ymax = ylims[2]), alpha = 0.15, fill = "blue") +
  scale_y_continuous(expand = c(0, 0), name = TeX("$X_2$"), breaks = NULL) + 
  scale_x_continuous(expand = c(0, 0), name = TeX("$X_1$"), breaks = NULL) +
  geom_text(data = d_bounds, aes(dx, dy, label = paste0("i=", iteration)), hjust = c(1, rep(0, 3)), 
            vjust = c(0, rep(1, 3)), nudge_y = c(0, rep(-0.02, 3)), nudge_x = c(-0.02, rep(0, 3))) +
  panel_border(colour = "black")

loss_data <- data.frame(loss = loss, iteration = 0:(length(loss) - 1)) 
library(ggthemes)
p_loss <- loss_data %>% 
  ggplot(aes(iteration, loss)) + 
  geom_line(color = "blue") +
  theme(axis.ticks.y = element_blank()) +
  # theme(panel.background = element_rect(fill = "white"),
  #       axis.ticks.x = element_blank()) +
  geom_segment(data = loss_data[c(1,5,10,30), ], aes(x = iteration, xend = iteration, yend = 0), linetype = "dashed") +
  scale_y_continuous(expand = c(0, 0), labels = NULL) + 
  scale_x_continuous(expand = c(0.05, 0), breaks = c(0, 4, 9, 29))

plot_grid(p, p_loss, labels = c("(a)", "(b)"), nrow = 1, align = "vh", rel_widths = c(3,2))
```


### Backpropogation

In \Cref{sec:optimisation} we discussed how to fit a linear model using the Stochastic Gradient Descent optimisation procedure.
Currenlty, SGD is the most effective way of training deep networks. 
To recap, SGD optimises the parameters $\theta$ of a network to minimise the loss,
$$
\theta = \arg\min_{\theta}\frac{1}{N}\sum_{i=1}^{N}l(\boldsymbol{x}_{i}, \theta).
$$
With SGD the training proceeds in steps and at each step we consider a mini-batch of size $n\le N$ training samples. 
The mini-batch is used to approximate the gradient of the loss function with respect to the paramaters by computing, 
$$
\frac{1}{n}\frac{\partial l(\boldsymbol{x}_{i},\theta)}{\partial \theta}.
$$
Using a mini-batch of samples instead of one at a time produces a better estimate of the gradient over the full training set and it is computationally much more efficient.

This section discusses the same procedure, but applied to a simple single hidden layer neural network for multiclass classification, decomposed as:
$$
\begin{aligned}
f_{k}(\boldsymbol{x})&=g_{k}(\boldsymbol{\beta}^{\intercal}_{k}\boldsymbol{z}), \quad k=1,\dots,K\\
z_{m}&=\sigma(\boldsymbol{\alpha}_{m}^{\intercal}\boldsymbol{x}),\quad m=1,\dots, M
\end{aligned}
$$
where $\sigma(\cdot)$ is the sigmoid activation and $g(\cdot)$ the softmax activation.
The neural network has a set of unknown adjustable weights that defines the input-output function of the network. 
They are the parameters of the linear function of the inputs, $\boldsymbol{\alpha}_{m}=(\alpha_{0m}, \alpha_{1m},\dots,\alpha_{pm})$, and the paramaters of the linear transformation of the derived features, $\boldsymbol{\beta}_{k}=(\beta_{0k}, \beta_{1k},\dots,\beta_{mk})$.
Denote the complete set of parameters by $\theta$.
Then the objective function for regression can be chosen as the sum-of-squared-errors:
$$
L(\theta) = \sum_{k=1}^{K}\sum_{i=1}^{N}\left(y_{ik}-f_{k}(\boldsymbol{x}_{i})\right)^{2}
$$
and for classification, the cross-entropy:

$$
L(\theta) = -\sum_{i=1}^{N}\sum_{k=1}^{K}y_{ik}\log f_{k}(\boldsymbol{x}_{i}),
$$
with corresponding classifier $G(\boldsymbol{x})=\arg\max_{k}f_{k}(\boldsymbol{x})$.
Since the neural network for classification is a linear logistic regression model in the hidden units, the paramaters can be estimated by maximum likelihood. 
According to @Hastie2009 [p. 395], the global minimiser of $L(\theta)$ is most likely an overfit solution and we instead require regularisation techniques when minimising $L(\theta)$.
Furthermore, as the network becomes larger, MLE becomes intractable.

Therefore, one rather uses gradient descent and the *backpropogation* algortihm [@Rumelhart1988] to minimise $L(\theta)$.
This is possible because of the modular nature of a neural network, allowing the gradients to be derived by iterative application of the chain rule for differentiation. 
This is done by a forward and backward sweep over the network, keeping track only of quantities local to each unit.

In detail, the backpropogation algorithm for the sum-of-squared error objective function,
$$
\begin{aligned}
L(\theta)&=\sum_{i=1}^{N}L_{i}\\
&=\sum_{i=1}^{N}\sum_{k=1}^{K}(y_{ik}-f_{k}(\boldsymbol{x}_{i}))^{2},
\end{aligned}
$$
is as follows. 
Following the chain-rul, the relevant derivatives for gradient descent are:

$$
\begin{aligned}
\frac{\partial L_{i}}{\partial \beta_{km}}&=-2(y_{ik}-f_{k}(\boldsymbol{x}_{i}))g_{k}'(\boldsymbol{\beta}_{k}^{T}\boldsymbol{z}_{i})z_{mi},\\
\frac{\partial L_{i}}{\partial \alpha_{ml}}&=-\sum_{k=1}^{K}2(y_{ik}-f_{k}(\boldsymbol{x}_{i}))g_{k}'(\boldsymbol{\beta}_{k}^{T}\boldsymbol{z}_{i})\beta_{km}\sigma'(\boldsymbol{\alpha}_{m}^{T}\boldsymbol{x}_{i})x_{il}.
\end{aligned}
$$

Given these derivatives, a gradient descent update at the $(r+1)$-th iteration has the form,
$$
\begin{aligned}
\beta_{km}^{(r+1)}&=\beta_{km}^{(r)}-\gamma_{r}\sum_{i=1}^{N}\frac{\partial L_{i}}{\partial \beta_{km}^{(r)}},\\
\alpha_{ml}^{(r+1)}&=\alpha_{ml}^{(r)}-\gamma_{r}\sum_{i=1}^{N}\frac{\partial L_{i}}{\partial \alpha_{ml}^{(r)}}.
\end{aligned}
$$

Now write the gradients as
$$
\begin{aligned}
\frac{\partial L_{i}}{\partial \beta_{km}}&=\delta_{ki}z_{mi},\\
\frac{\partial L_{i}}{\partial \alpha_{ml}}&=s_{mi}x_{il}.
\end{aligned}
$$
The quantities, $\delta_{ki}$ and $s_{mi}$ are errors from the current model at the output and hidden layer units respectively. 
From their definitions, they satify the following,
$$
s_{mi}=\sigma'(\boldsymbol{\alpha}_{m}^{T}\boldsymbol{x}_{i})\sum_{k=1}^{K}\beta_{km}\delta_{ki},
$$
which is known as the backpropogation equations.
Using this, the weight updates can be made with an algortihm consisting of a forward and a backward pass over the network. 
In the forward pass, the current weights are fixed and the predicted values $\hat{f}_{k}(\boldsymbol{x}_{i})$ are computed. 
In the backward pass, the errors $\delta_{ki}$ are computed, and then backpropogated via the backpropogation equations to obtain $s_{mi}$. 
These are then used to update the weights.
This approach extends naturally to any size network and differentiable layers (functions).

Backpropogation is simple and its local nature (each hidden unit passes only information to and from its connected units) allows it to be implented efficiently in parallel. 
The other advantage is that the computation of the gradient can be done on a batch (subset of the training set) of observations. 
This allows the network to be trained on very large datasets. 

In summary, training a neural network consists of these four steps:

1. Initialise the network weights: give random set of numbers to the network parameters.
2. Forward propogation: pass the input through the network layers to produce an output.
3. Calculate the error: compare the predicted output with the true output and measure the difference using an objective function.
4. Backward propogation: calculate the gradients of the objective function with respect to the weights and update the weights accordingly.

These four steps are typically repeated until convergence of the loss function.
It can take many training epochs for the objective function to converge.

### Weight initialisation

Before one begins training a neural network, one needs to initialise its weights.
We expect a well trained neural network to have an equal number of weights smaller than zero and greater than zero.
If we initialise all weights to be zero, every neuron will compute the same output and therefore prodcue the same gradient and undergo the same weight update.
We still want to initialise the weights as small as possible but each one unique.
Sampling from zero mean and unit variance Gaussian distribution is a natural fit, however, the variance of the outputs from a randomly initialised neuron grows with increasing number of inputs.
Therefore we may scale the weight vector by the square root of its number of inputs to normalise the variance to 1.
This ensures that all neurons in the network initially have approximately the same output distribution and empirically improves the rate of convergence.
[@He2015a] derived a weight initialisation specifically for ReLU neurons.

### Basic Regularisation \label{sec:basicreg}

From \Sref{sec:netsize} it seems that smaller neural networks can be preferred if the data is not complex enough to prevent overfitting.
However, this is untrue and there are much better ways of regularising neural networks, which we discuss shortly and in the next chapter.
Smaller networks are harder to train with local methods such as gradient descent because their loss functions have poor local minima that are easy to converge to.
Whereas the local minima of larger networks are much better in terms of their actual loss [@Choromanska2014].

One of the preferred ways of fighting overfitting in neural networks is by using L1 or L2 regularisation, *i.e.* adding a penalty term proportional to the magnitude of the weights to the objective function.
This is exactly what is done in Ridge Regression and the Lasso [@Hastie2009, Ch. 4].
In L2 regularisation we add the term $\frac{1}{2}\lambda w^{2}$ to the objective and in L1 regularisation the penalty term $\lambda |w|$.
These penalties forces the weights to be small and the $\lambda$ parameter controls the strength of the regularisation.
The "$\frac{1}{2}$" in front of the L2 penalty is added for convenience to make the derivative equal to $\lambda w$.
This makes it then equivalent to decaying each of the weights linearly towards zero: $w'=w-\lambda w$, which is also known as *weight decay*.
In practice, L2 regularisation outperforms L1 regularisaion.

Another basic way of preventing the neural network to overfit is to stop the training process "early" and to not train until the training loss converges.
A converged training loss is not equivalent to an optimal test loss and therefore the test or validation loss should be observerd during training.
One approach is to stop training when the validation loss stops decreasing.
As mentioned before, the learning rate also plays a big part in finding the optimal weights.
Next we discuss how we can tune the learning rate to train faster and to find better local minima.
More advanced regularisation techniques are discussed in \Cref{chp:dl}.

### Adaptive Learning Rates and Annealing \label{sec:lr}

Although the right learning rate can reduce training time and improve performance, there is no silver bullet when it comes to setting the parameter.
A small learning rate slows downs the training time, but is safer against overfitting and overshooting the optimal solution.
With a large learning rate, convergence may be reached quicker, but the optimal solution may not have been found. 
One could do a line search of a range of possible values, but this usually takes too long for bigger networks. 

However, one does not need to keep the learning rate fixed throughout the training process.
A popular approach is to decrease the learning rate by a fraction after a certain number of epochs or as the validation loss starts to converge, as done in [@He2015] for example.
The intuition is that larger steps can be taken when we are still far away from an optimal position on the loss surface, and then gradually take smaller steps once we get closer to not overshoot it.
Note, that this means that we would also need to tune the rate of decrease and the time steps of each decrease during training.
Fortunately, it is believed that the learning algorithms are not that sensitive to this choice.

In addition, there are ways of manipulating the learning rates at a local level, as opposed the aforementioned global methods.
Adagrad [@Duchi2011] is an adaptive learning rate method which increases the learning rate at neurons with small gradients and *vice versa*.
Adam [@Kingma2014] is the most commonly used weight update approach.
It also uses the magnitude of the gradient to control each weight update, in addition to the previous iteration's gradients and it combines them in a smooth fashion.
This resembles the physical property of momentum [@Bengio2012].
For more detail, the reader can refer to the cited publications as it falls out of the scope of this work.

## Representation Learning \label{sec:rep}

We are now familiar with the mathematical operations of basic layers, how they are connected and how their weights are tweaked to minimise a loss function.
In this section we will discuss why this works and what the neural network is actually doing to model the data.
The central idea is that of a data *representation* [@Bengio2013] and that at each layer of the network the data is transformed into a higher-level abstraction of itself.
Understanding and interpreting neural networks remains a challenge [@Frosst2017], but the notion of learning an optimal data representation allows us to gain a deeper intuition of the inner mechanics of neural networks.

Machine learning models are very sensitive to the form and the properties of the input given to it.
Thus a large part of building machine learning models is to find the best way of representing the raw data to make it easier for the models to extract useful information.
This is typically a laborious manual task of creating, analyzing, evaluating and selecting appropriate features[^feature_engineering] requires practitioner expertise and domain knowledge.
This *feature engineering* process is more trial-and-error than a systematic recipe.
Therefore if one can effectively automate this process, it will save a lot of time and raise the performance ceiling of models.
Automatically learning representations of the data that make it easier to extract useful information for classifiers or other predictors is called representation learning [@Bengio2013].

[^feature_engineering]: http://blog.kaggle.com/2014/08/01/learning-from-the-best/

A neural network can be viewed from the perspective of representation learning.
Consider a classification task.
Since the final layer of a neural network is a linear model, in order for the network to produce accuracte predictions, the previous layers should be able to project the data into a space where the classes are linearly separable.
Thus the network learns a representation of the data that is optimal for classification.

Each of the simple but non-linear modules of a neural network transforms the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level.
With the composition of enough such transformations, very complex functions can be learned [@Lecun2015].
These transformations can emphasise (and create) features that are important for discrimination and drop those which are redundant.

Let us work through a simple example to illustrate the representations learned by a neural network.
Consider a dataset with two classes; the two curves on a plane shown in \autoref{fig:simple_dataset}.
Clearly, the observations from the two classes are not linearly separable in their raw form.
Thus if we fit a single layer neural network (*i.e.* only an output layer) to this data, we will get an unsatisfactory decision boundary, since the decision boundary can only be linear, as shown in \autoref{fig:simple_dataset_simpleNN}.
However, if we fit a two-layer neural network, where the hidden layer has two neurons and a sigmoid acitvation to the same dataset, the decision boundary perfectly separates the two classes.
This is shown in \autoref{fig:simple_dataset_complexNN}.

![Simple dataset with two linearly inseparable classes.\label{fig:simple_dataset}](figures/simple_dataset.pdf)

![Decision boundary of 1-layer neural network.\label{fig:simple_dataset_simpleNN}](figures/simple_dataset_simpleNN.pdf)

![Decision boundary of 2-layer neural network.\label{fig:simple_dataset_complexNN}](figures/simple_dataset_complexNN.pdf)

Since the hidden layer consists of only two neurons, we are able to plot the output from the hidden layer after the raw data has passed through it.
This is depicted in \autoref{fig:simple_dataset_complexNN_rep}.
This shows how the hidden layer projected the input data into a space where the observations from the two classes are linearly separable.
Which leaves it to the final layer to find the best hyperplane between them.

![Hidden representation of 2-layer neural network.\label{fig:simple_dataset_complexNN_rep}](figures/simple_dataset_complexNN_rep.pdf)

Of course this is a very simple example, but the same concepts apply to more complicated datasets and models.
However, even though it is technically possible to separate any arrangement of points with a sufficiently large network[^colah], in reality it can become quite challenging to find such representations.
This is where the need for more data, regularisation, smarter optimsation procedures and architecture design arises.
Without the aforementioned, it is likely that the network will get stuck in a sub-optimal local minima, not being able to find the optimal representation of the data.
In the following chapters we explore the approaches available to find optimal represenations of tabular data for classification and regression tasks.

[^colah]: http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/






