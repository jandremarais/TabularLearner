# Experiments \label{chp:exp}

\chapterprecishere{"For us, the most important part of rigor is better empiricism, not more mathematical theories."\par\raggedleft--- \textup{Ali Rahimi and Ben Recht}, NIPS 2017}

## Introduction

Since theory and practice does not always go hand-in-hand, it is usually advantageous to compliment a theoretical study or literature review with empirical results. 
Another motivation for empirical study is that we regard the ability to implement an approach equally as important as understanding the theory behind it. 
We characterise a good empirical experiment as one that is *rigorous* and *reproducible*. 
Recently the field of DL has been criticised for the growing gap between the understanding of its techniques and its practical successes[^talk2] where most of the recent focus was on the latter. 
The speakers urged the deep learning community to be more rigorous in their experiments where, for them, the most important part of rigor is better empiricism, not more mathematical theories. 

[^talk2]: How do I cite the talk given at NIPS2017 - https://www.youtube.com/watch?v=Qi1Yry33TQE

In this chapter we aim for good empiricism by exploring many hyperparamters in various data scenarios and doing cross-validation for unbiased performance measures along with standard errors.
Our work is not necessarily about beating the benchmark and instead consists of simple experiments that aid in the understanding of how the techniques work and what effect they have.

Furthermore, we want all our experiments to be as reproducible as possible.
Therefore we provide all the code, data and necessary documentation to reproduce the experiments that were done in this thesis (see \autoref{app:code}). 
This is often an overlooked feature of experiments, but is however crucial for transparent and accountable reporting and making your work useful for others to build upon.

The main aim of this chapter is to better understand the behaviours of certain models and parameters and to cross-check the literature with empirical observations.
We focus on the same main modelling challenges for deep learning on tabular datasets that we discussed in \autoref{chp:td}, which are:

- how to represent the inputs,
- how to learn from feature interactions, and
- how to be more sample efficient.

The model interpretation challenge was covered in \autoref{sec:interp}.

The more general hyperparameters, like learning rate, batch size, layer size and layer depth is not the focus of these experiments and some we have already experimented with in previous chapters.
However, since these parameters are tightly linked with each other and other model parameters, we still do a hyperparamter search where we deem appropriate and report the findings in \Cref{app:B}.
The rest of the chapter continues as follows: 
In \Sref{sec:datasets} we discuss the dataset used for these experiments and why it was chosen.
Thereafter, in \Sref{sec:eval}, we discuss the general methodology of the experiments and how they are evaluated.
Then the main experiments follow.
\Sref{sec:rep_exp} compares the performance of various entity embedding sizes.
\Sref{sec:exp_int} looks at the proposed approaches to model high-order feature interactions.
\Sref{sec:exp_samp} evaluates the different approaches to avoid overfitting in constrained data environments.

## Datasets \label{sec:datasets}

We chose to do our experiments on the Adult dataset for the following reasons:

+ **Simplicity**: We wanted a dataset that is representative of a real-world case but one that does not have specific modelling challenges like plenty of missing values or highly imbalanced classes. Our goal is to evaluate the models on a generic tabular dataset and not one that requires special attention or the skills of a domain expert. Thus when we fall short we know it is because of the model and not something that is in the data.
+ **Minimal preprocessing**: We want to focus our time on training the algorithms and not preprocessing the data. The Adult dataset is relatively clean.
+ **Open access**: Since we want our work to be reproducible, we want the dataset we use to be accessible by all.
+ **Good size**: Neural networks are data hungry and therefore for optimal performance we prefer a medium size dataset. If we want to test the performance of the models on smaller datasets, we can just run the experiment on a subset of the data.
+ **Strong baselines**: In order to know how well we are doing we need to be able to compare our performance with those of others.

For more specifics on the dataset, one can refer to \autoref{app:data}

## General Methodology \label{sec:eval}

### Loss Function and Evaluation Metric

Since this is a binary classification task we train the neural network to optimise the binary cross-entropy loss as defined in \Sref{sec:backprop}, with $K=2$.
This is the standard loss function to optimise for binary classification [@Song2018, @Wang2017b, @Zhang2016, @Qu2016].
We also monitor the accuracy of the model during training since this is often more interpretable, but note that the accuracy is not directly being optimised and that the binary cross-entropy only acts as an differentialable proxy of the accuracy.

### Cross-validation

For most of the experiments we will do a 5-fold cross validation [@Hastie2009, p. 241] to estimate the performance of a model. 
That is, randomly dividing the dataset in five equal parts and then in turn, hold out one of those parts for validation purposes and train the model using the remaining four parts.
\autoref{fig:cv} visually explains how the dataset is sub-divided.
The performance of the model can then be evaluated on the held-out part.
This process is repeated for every one of the five segmentations of the dataset and thus five measurements of the performance of model is obtained.
We can then compute the average over these five measurements to obtain a less biased estimate of the model performance.
Another advantage of this approach is that we can obtain standard error for the model performance.
These standard errors will be displayed on the figures illustrating the results of the experiments as confidence intervals, *i.e.* $\mu\pm\sigma$.
This helps us to evaluate the significance of the results.

![5-Fold Cross-validation dataset split schematic.\label{fig:cv}](figures/cv.pdf)

It is common in deep learning research to do a once-off split of the data dividing it into training, validation and (sometimes) testing datasets, for example in [@Klambauer2017, @Song2018, @Zhang2016].
Cross-validation is rarely done in Deep Learning, since the models typically take very long to train and any repitition is thus more costly.
The problem with doing a once-off split is that it does not account for the variance of the model and the performance of the model can in fact be very sensitive to the subset of data.
By doing cross-validation, we can have more robust performance metrics, including the benefit of reporting on standard errors.
However, Deep Learning is also mostly applied to large datasets and if a large test set is available, the gains from cross-validation diminishes.
Fortunately, the neural networks applied to tabular data are much smaller than ones used for unstructured data and therefore cross-validation makes sense.

### Preprocessing

For our experiments we aimed to do as little feature engineering and preprocessing as possible, in the spirit of automated analysis.
We do not create any new feature combinations.
The only necessary preprocessing steps are to convert the categorical features into integers (necessary for entity embeddings), standard normal scaling for the continuous features (necessary for optimisation algorithms), mean impute the missing values in the continuous features and assign a "null" category to missing values in the categorical features.
We do not do feature selection since we would want the model to learn by itself which features are relevant.

### General Hyperparameters

Based on our findings in \Sref{sec:hs_tb} we decide to train all models using the 1Cycle policy and the Adam optimiser.
Unfortunately, we cannot follow the hyperparameter selection process suggested by [@Smith2018] for all the experiments.
The process is too manual.
Therefore we follow the approach once to find a good selection of the learning rate, number of epochs and weight decay and then use these paramters for the rest of the experiments on this dataset.
Some of this is reported in \autoref{app:B}.
If the model significantly changes over experiments or we experience instable training, we rerun the parameter selection process.
Thus we will definitely not find the optimal model for each experiment but it should be sufficient to use for comparison purposes.
According to [@Smith2018] these parameters are also quite robust and the model is not too sensitive on these choices.

We follow a bit of a greedy approach when selecting optimal parameters.
All the hyperparameters are very dependent on each other but we cannot run experiments for every possible combination.
Therefore we find optimal parameters for a certain experiment and then assume that these parameters are also good for other experiments.
If not specified otherwise, we used a simple MLP as the network mapping the input representation to the output.
The basice structural hyperparameters we tested can be found in \autoref{app:B}.
There we experimented with things like the dropout ratio, width and depth of the network, amongst other.

## Input Representation \label{sec:rep_exp}

### Embeddings Sizes

The aim of these experiments is to evaluate performance at different embedding sizes.
We first explore fixed embedding sizes for all categorical features.
The sizes we experiment with are [2,4,8].
Again, we expect there to be optimal embedding size for each variable depedning on the cardinality of the variable and how complex its relationship is with the other variables and the target.
We expect the ideal embedding size to be as small as possible but still being able to capture all of the information of the variable.
But the only way to tell is by trial and error.
We would also like to see if the rules-of-thumb mentioned in \Sref{sec:cat_inp} provide reasonable performance.
A benefit that these schemes can add is that they depend on the cardinality of the categorical feature.

The results of the one sized embeddings are displayed in \autoref{fig:exp_emb_f}.
We see that if all embeddings are fixed at size 2, the model showed greater performance than at 4 and 8.
One reason for this may be that the categorical features are not that important for the prediction of the target and that a smaller representation helps to reduce noise.

![Effect of the embedding size if all categorical features are mapped to the same number of dimensions.\label{fig:exp_emb_f}](../data/adult/results/embeddingsize/all_epochs.png)

We now compare the constant size 2 embeddings with the rules from [@Wang2017b] and [@Brebisson2015] to see how the cardinality dependency effects the performance.
The results are shown in \autoref{fig:exp_emb_r}.
There is almost no difference between the approaches.
Brebisson's methos performs sightly better on the validation loss and the fixed size of 2 performs the best on accuracy.
On other datasets this result will most likely be different if the categorical features contain more rich information.

![Effect of variable sizes on the performance of the model.\label{fig:exp_emb_r}](../data/adult/results/embeddingsize_rule/all_epochs.png)

## Feature Interactions \label{sec:exp_int}

### Attention



### SeLU

Deeper networks can help us learn higher-order feature interactions.
The SeLU activation function is supposed to help us train deeper neural networks.
Therefore we compare it to the ReLU activation function over two models, one with 2 layers and the other with 8, to investigate its impact on performance.
The SeLU activation function was implemented with its special weight intitialisation, but no dropout was used.
In \autoref{fig:selu_all} and \autoref{fig:selu_best} we observe that the model with 8 layers and SeLU activations, perform the best overall in terms of the validation loss, but in terms of accuracy the ReLU model with 2 layers showed the best results.
The differences observed are minor which leads us to believe that the choice is not that important and that sticking with shallow ReLU networks are sufficient until more experiments are done.

![The average performance of ReLU and SeLU activation functions for shallow and deep networks over every epoch.\label{fig:selu_all}](../data/adult/results/activation_depth/all_epochs2.png)

![The average performance of ReLU and SeLU activation functions for shallow and deep networks.\label{fig:selu_best}](../data/adult/results/activation_depth/best2.png)

### Skip-Connections

Another mechanism we can use to make deeper networks possible is a skip-connection.
Skip-connections may also help to combine different orders of feature interactions.
The skip connection combines the activations before and afer a linear layer through an elementwise addition.
We tested networks with skip-connections for a shallow and a deep neural network.
The results are summarised in \autoref{fig:resid_epoch} and \autoref{fig:resid_best}.
On average did the deeper neural network perform better and in terms of the validation loss did the skip-connections slightly improve the performance.
Again, there is very little between the two types and thus we cannot say one is better thant the other.
For simplicity sake, we would suggest not using skip-connections, until more experiments have been done.

![Average performance at each epoch for shallow and deep neural networks, with and without skip connections.\label{fig:resid_epoch}](../data/adult/results/residual_depth/all_epochs.png)

![Overall performance of the skip-connections used in a shallow and deep neural network.\label{fig:resid_best}](../data/adult/results/residual_depth/best.png)

## Sample Efficiency \label{sec:exp_samp}

We did an experiment to see how the number of samples available to the network for training influences its learning ability.
We trained the network at training set sizes of 1000, 2000, 4000, 8000, 16000 and 32000.
The results are reported in \autoref{fig:sampsize}.
As expected, the network performs better as the training set increases, but the increase diminish as the sample size gets higher.
We do observe a strange results where the model trained only using 4000 samples exhibit one of the best accuracies.

![Effect of the number of training samples on the performance of neural networks.\label{fig:sampsize}](../data/adult/results/samplesize/all_epochs2.png)

### Data Augmentation

Mixup was the most promising form of data augmentation from our literature review.
In this experiment we want to investigate how mixup with different mixup ratios influence the performance of our neural networks on the adult dataset.
Since data augmentation also acts as a form of regularisation, we want to see the interaction between weight decay and mixup ratios.
We experimented with mixup $\alpha$-ratios of [0,0.4] and with weight decays of [$10^{-5}$, $10^{-3}$] on a neural network with 3 hidden layers of 200 units each.
The results of the experiment are captured in \autoref{fig:exp_mix_best} and \autoref{fig:exp_mix_epoch}.

The results indicate that mixup is not improving the validation loss or the accuracy of the models.
It is also interesting to see the interaction between mixup and weight decay.
Since both are forms of regualrisation we would expect one to reduce the need of the other, but this is not what the results suggest.
Although a promising technique from the literature, we think the value of mixup depends on the dataset and whether or not interpolating between samples makes sense.

![Average performance of models with various mixup and weight decay parameters.\label{fig:exp_mix_best}](../data/adult/results/mixup/best.png)

![Performance per epoch for models with different weight decays and mixup ratios.\label{fig:exp_mix_epoch}](../data/adult/results/mixup/all_epochs.png)

### Unsupervised Pretraining

Here we investigate whether or not DAEs as a method of unsupervised pretraining is beneficial to a supervised learning neural network.
We trained a DAE with swap noise for 15 epochs on the Adult dataset, with the swap noise proportion set at 15%.
The DAE had 3 hidden layers of 500 units each.
Then we transfer the learned weights to a supervised learning network, also with 3 hidden layers of 500 units.
But the supervised network has a different output layer to the DAE.
Therefore we first keep the transferred weights fixed and train only the last layer of the supervised learning network, as a way of initialising its weights.
Otherwise if we would have trained all the weights together from the start, the random weights of the output layer might have interfered with the learned weights in the hidden layers.
Once the output layer is initialised with this process, we can train all of the network simultaneously.
This final stage of training is showed in \autoref{fig:exp_pretr} where we compare it to a model without pretraining.
We see that in terms of both the accuracy and the validation loss that the pretrained model has an advantage over the trained from scratch classifier.
This makes sense since the pretrained model does not have to start from a random initialisation.
But as training continues this gaps becomes smaller.
In terms of the validation loss the two models are equivalent when training completed, but in terms of the accuracy the trained from scratch classifier outperformed the pretrained model.
We do not believe these results are conclusive since there are still so many avenues to explore for unsupervised pretraining with DAEs.
We do not know yet how to design the DAE in terms of its network architecture, amount of noise injected, lenght of training cycle, etc, and then what the best way is of transferring this knowledge to a classifier.
As a test to validate if the DAE learned something useful one can feed the activations from one of the DAE hidden layers to another machine learning algorithm and see whether these representations perform better than with the raw data.
From the results of this basic implementation, we do still believe that there is value in this approach and that it warrants further exploration.

![The effect of pretraining on the classifier's performance.\label{fig:exp_pretr}](../data/adult/results/pretraining/all_epochs3.png)

## General Thoughts

We ran thorough experiments for the most promising techniques in deep learning for tabular data.

- Not a big difference between various approaches
- can be because adult dataset is too simple
- generic hyperparameters are not good choices for these experiments, especially learning rate and weight decay.


