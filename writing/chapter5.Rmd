# Experiments \label{chp:exp}

\chapterprecishere{"For us, the most important part of rigor is better empiricism, not more mathematical theories."\par\raggedleft--- \textup{Ali Rahimi and Ben Recht}, NIPS 2017}

## Introduction

Since theory and practice does not always go hand-in-hand, it is usually advantageous to compliment a theoretical study or literature review with empirical results. 
Another motivation for empirical study is that we regard the ability to implement an approach equally as important as understanding the theory behind it. 
We characterise a good empirical experiment as one that is *rigorous* and *reproducible*. 
Recently the field of DL has been criticised for the growing gap between the understanding of its techniques and its practical successes[^talk2] where most of the recent focus was on the latter. 
The speakers urged the deep learning community to be more rigorous in their experiments where, for them, the most important part of rigor is better empiricism, not more mathematical theories. 

[^talk2]: How do I cite the talk given at NIPS2017 - https://www.youtube.com/watch?v=Qi1Yry33TQE

In this chapter we aim for good empiricism by exploring many hyperparamters in various data scenarios and doing cross-validation for unbiased performance measures along with standard errors.
Our work is not necessarily about beating the benchmark and instead consists of simple experiments that aid in the understanding of how the techniques work and what effect they have.

Furthermore, we want all our experiments to be as reproducible as possible.
Therefore we provide all the code, data and necessary documentation to reproduce the experiments that were done in this thesis (see \autoref{app:code}). 
This is often an overlooked feature of experiments, but is however crucial for transparent and accountable reporting and making your work useful for others to build upon.

The main aim of this chapter is to better understand the behaviours of certain models and parameters and to cross-check the literature with empirical observations.
We focus on the same main modelling challenges for deep learning on tabular datasets that we discussed in \autoref{chp:td}, which are:

- how to represent the inputs,
- how to learn from feature interactions, and
- how to be more sample efficient.

The model interpretation challenge was covered in \autoref{sec:interp}.

The more general hyperparameters, like learning rate, batch size, layer size and layer depth is not the focus of these experiments and some we have already experimented with in previous chapters.
However, since these parameters are tightly linked with each other and other model parameters, we still do a hyperparamter search where we deem appropriate and report the findings in \Cref{app:B}.
The rest of the chapter continues as follows: 
In \Sref{sec:datasets} we discuss the dataset used for these experiments and why it was chosen.
Thereafter, in \Sref{sec:eval}, we discuss the general methodology of the experiments and how they are evaluated.
Then the main experiments follow.
\Sref{sec:rep_exp} compares the performance of various continuous feature transforms and entity embedding sizes.
\Sref{sec:exp_int} looks at the proposed approaches to model high-order feature interactions.
\Sref{sec:exp_samp} evaluates the different approaches to avoid overfitting in constrained data environments.

## Datasets \label{sec:datasets}

We chose to do our experiments on the Adult dataset for the following reasons:

+ **Simplicity**: We wanted a dataset that is representative of a real-world case but one that does not have specific modelling challenges like plenty of missing values or highly imbalanced classes. Our goal is to evaluate the models on a generic tabular dataset and not one that requires special attention or the skills of a domain expert. Thus when we fall short we know it is because of the model and not something that is in the data.
+ **Minimal preprocessing**: We want to focus our time on training the algorithms and not preprocessing the data. The Adult dataset is relatively clean.
+ **Open access**: Since we want our work to be reproducible, we want the dataset we use to be accessible by all.
+ **Good size**: Neural networks are data hungry and therefore for optimal performance we prefer a medium size dataset. If we want to test the performance of the models on smaller datasets, we can just run the experiment on a subset of the data.
+ **Strong baselines**: In order to know how well we are doing we need to be able to compare our performance with those of others.

For more specifics on the dataset, one can refer to \autoref{app:data}

## General Methodology \label{sec:eval}

### Loss Function and Evaluation Metric

Since this is a binary classification task we train the neural network to optimise the binary cross-entropy loss as defined in \Sref{sec:backprop}, with $K=2$.
This is the standard loss function to optimise for binary classification [@Song2018, @Wang2017b, @Zhang2016, @Qu2016].
We also monitor the accuracy of the model during training since this is often more interpretable, but note that the accuracy is not directly being optimised and that the binary cross-entropy only acts as an differentialable proxy of the accuracy.

### Cross-validation

For most of the experiments we will do a 5-fold cross validation [@Hastie2009, p. 241] to estimate the performance of a model. 
That is, randomly dividing the dataset in five equal parts and then in turn, hold out one of those parts for validation purposes and train the model using the remaining four parts.
\autoref{fig:cv} visually explains how the dataset is sub-divided.
The performance of the model can then be evaluated on the held-out part.
This process is repeated for every one of the five segmentations of the dataset and thus five measurements of the performance of model is obtained.
We can then compute the average over these five measurements to obtain a less biased estimate of the model performance.
Another advantage of this approach is that we can obtain standard error for the model performance.

![5-Fold Cross-validation dataset split schematic.\label{fig:cv}](figures/cv.pdf)

It is common in deep learning research to do a once-off split of the data dividing it into training, validation and (sometimes) testing datasets, for example in [@Klambauer2017, @Song2018, @Zhang2016].
Cross-validation is rarely done in Deep Learning, since the models typically take very long to train and any repitition is thus more costly.
The problem with doing a once-off split is that it does not account for the variance of the model and the performance of the model can in fact be very sensitive to the subset of data.
By doing cross-validation, we can have more robust performance metrics, including the benefit of reporting on standard errors.
However, Deep Learning is also mostly applied to large datasets and if a large test set is available, the gains from cross-validation diminishes.
Fortunately, the neural networks applied to tabular data are much smaller than ones used for unstructured data and therefore cross-validation makes sense.

### Preprocessing

For our experiments we aimed to do as little feature engineering and preprocessing as possible, in the spirit of automated analysis.
We do not create any new feature combinations.
The only necessary preprocessing steps are to convert the categorical features into integers (necessary for entity embeddings), mean impute the missing values in the continuous features and assign a "null" category to missing values in the categorical features.
We do not do feature selection since we would want the model to learn by itself which features are relevant.

### General Hyperparameters

Based on our findings in \Sref{sec:hs_tb} we decide to train all models using the 1Cycle policy and the Adam optimiser.
Unfortunately, we cannot follow the hyperparameter selection process suggested by [@Smith2018] for all the experiments.
The process is too manual.
Therefore we follow the approach once to find a good selection of the learning rate, number of epochs and weight decay and then use these paramters for the rest of the experiments on this dataset.
Some of this is reported in \autoref{app:B}.
If the model significantly changes over experiments or we experience instable training, we rerun the parameter selection process.
Thus we will definitely not find the optimal model for each experiment but it should be sufficient to use for comparison purposes.
According to [@Smith2018] these parameters are also quite robust and the model is not too sensitive on these choices.

We follow a bit of a greedy approach when selecting optimal parameters.
All the hyperparameters are very dependent on each other but we cannot run experiments for every possible combination.
Therefore we find optimal parameters for a certain experiment and then assume that these parameters are also good for other experiments.

## Input Representation \label{sec:rep_exp}

### Numeric Feature Processing

- zero mean unit variance
- cumalitve transform

### Embeddings Sizes

The aim of these experiments are to evaluate performance at different embedding sizes.
We explore embedding sizes at different ratios of the cardinality of categorical variables.
The ratios we look at are: 10%, 20% 30%, 40%, 50%, 60%, 70%, 80% and 90% of the cardinality of each categorical feature.
Possibly explore certain max embedding sizes.

As we increase the size we will also look at the effect it has by visually inspecting the embedding layers in a 2-dimensional space.

Again, we expect there to be optimal embedding size for each variable depedning on the cardinality of the variable and how complex its relationship is with the other variables and the target.
We expect the ideal embedding size to be as small as possible but still being able to capture all of the information of the variable.

Look at wide and deep models from [@Cheng2016].
They restrics all embeddings to be of size 32.

## Feature Interactions \label{sec:exp_int}

### Attention

### SeLU

### Skip-Connections

## Sample Efficiency \label{sec:exp_samp}

### Data Augmentation

- mixup vs swap noise

### Unsupervised Pretraining
- does it help the validation loss
- Are these features useful for tree based methods.

