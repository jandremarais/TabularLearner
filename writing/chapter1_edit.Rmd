# Introduction

## Deep Learning

This thesis is concerned with the study of *deep learning* approaches to solve *machine learning* (ML) tasks.
More specifically, our interest lies in machine learning tasks that may be solved using tabular data inputs.
The deep learning field is an extention of the class of machine learning algorithms called *Artifical Neural Networks* (NNs).
Whereas until relatively recently, the neural network field was not a very active research field, rapid development in computing power and the growing abundance of data lead to advances in neural network optimisation and architecture.
These advances constitutes the deep learning field as we know it today [@Lecun2015].

Currently, deep learning is receiving a remarkable amount of attention, both in research and in practice (see \autoref{fig:dlpapers}).
Much of the deep learning hype stems from the tremendous value neural networks have shown in application areas such as *computer vision* [@Hu2017], audio processing [@Battenberg2017], and *natural language processing* (NLP) [@Devlin2018]. 
In these application areas, deep learning methods have reached a maturity level sufficient to be able to run these systems in a production or commercial environment.
Examples of the application of deep learning in commercial applications include voice assistants like Amazon Alexa [@Sarikaya2017], face recognition with Apple iPhones [^faceid], and language translation with Google [@Wu2016].

[^faceid]: https://www.apple.com/business/site/docs/FaceID_Security_Guide.pdf

![The exponential growth of published papers and Google search terms containing the term *Deep Learning*\label{fig:dlpapers}. 
Sources: Google Trends[^googletrend], Semantic Scholar[^semschol]](figures/trends.pdf)

[^jeffdeantalk]: https://www.slideshare.net/AIFrontiers/jeff-dean-trends-and-developments-in-deep-learning-research
[^googletrend]: https://trends.google.com/trends/explore?date=all&q=deep%20learning
[^semschol]: https://www.semanticscholar.org/search?year%5B0%5D=2000&year%5B1%5D=2019&q=%22deep%20learning%22&sort=relevance

One of the most attractive attributes of deep learning is its ability to model almost any input-output relationship. 
This has lead to the use of deep learning in a very wide array of applications.  
For example, deep learning has been used to generate art [@Gatys2015] and music [@Mogren2016], to control various modules in autonomous cars [@Fridman2017], to play video games [@Mnih2013], to recommend movies [@Covington2016], to improve the quality of images [@Shi2016], and to beat the world's best Go player [@Silver2017].

A common characteristic of all of the above deep learning applications, is that the data used to construct them, contain the same type of values or measurements.
That is, in computer vision the data are pixel values, in NLP the data are words and in audio processing the data represent sound waves.
This is not a criterion for deep learning algorithms to be successful, but may be viewed as a driver for their sucess in these application domains.
It is simpler to model data consisting of the same type of measurements, since each input feature can be treated the same.
Furthermore in the above deep learning applications, it is found that in each of these domains, universal patterns exist.
This allows for knowledge to be transferred between tasks belonging to the same domain.
The knowledge to be transferred is both knowledge aquired by humans, and that learned by a deep learning model.
For example, in computer vision, advancements in classifying pictures of pets will most likely also be facilitate improved identification of tumors in X-rays.
That is, patterns learned by a deep learning model when attempting one task, may also be useful in a different, but related task (see *Transfer Learning*).

A data domain in which deep learning does not flourish is that of tabular data.
A *tabular dataset* can be represented by a two-dimensional table, where each of the rows of the table corresponds to one observation and where each column denotes an individual meaningful feature.
We further explain the use of tabular data in \Cref{sec:tabdat} below.

Some research have recently been done on the use of deep learning models for tabular data [@Shavitt2018, @Song2018].
However, state-of-the-Art (SotA) results are reported only rarely [@Brebisson2015] (and in this competition[^port]).
Therefore it can be said that the area is nowhere near as mature or receiving as much attention as is the case with deep learning for computer vision or for NLP.
In a comprehensive study in the paper by [@Delgado2014], it was found that ML tasks that make use of tabular data are typically more effectively solved using tree-based methods.
This is also evident when one considers the winning solutions of relevant Kaggle competitions[^kaggle].
A possible explanation for the superior performance of tree-based metods, is the heterogeneity of tabular data [@Shavitt2018], which forms part of the discussion in the next section.

[^kaggle]: https://www.kaggle.com
[^port]: https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629

## Tabular Data \label{sec:tabdat}

In this section, we make use of the so-called Adult[^adult1] dataset in order to discuss the use of tabular data.
The reader may refer to \autoref{tab:adult} for an extract of this dataset.
The data were collected during an American census with the aim of predicting whether or not an indivdual earns more than $50,000 a year.

[^adult1]: http://archive.ics.uci.edu/ml/datasets/Adult

```{r, results='asis'}
df = read_csv('../data/adult/adult_sample.csv')
xtable(df, caption = 'Preview of the Adult dataset.\\label{tab:adult}')
```

\autoref{tab:adult} represents a typical tabular dataset, where the columns contain measurements on different features.
Therefore each column may contain different data types. 
Some columns may consist of continuous measurements, whereas other columns contain discrete or categorical measurements. 
Hence the columns are heterogeneous with respect to data types.
Furthermore, in tabular data, the rows and columns occur in no particular order.
This of course stands in contrast to image or text data.

Many important ML applications make use of tabular data. Some of these applications are listed below:

- Various tasks that make use of Electronic Health Records. These include the prediction of in-hopsital mortality rates, and prolonged length of stay [@Rajkomar2018].
- Recommender systems for items like videos [@Covington2016] or property listings [@haldar2018].
- Click-through rate (CTR) prediction in web applications, *i.e.* predicting which item a user will click on next [@Song2018]. 
- Predicting which clients are at risk of defaulting on their accounts[^loandef].
- Predicting store sales [@Guo2016].
- Drug discovery [@Klambauer2017].

[^loandef]: https://www.kaggle.com/c/loan-default-prediction

Tabular datasets come in all shapes and sizes.
The number of rows can range from hundreds to millions, and the number of columns also has no limits.
It is not unusual for tabular datasets to be noisy.
A proportion of the observations may have missing features and/or incorrect values.
Continuous measurements may be based upon vastly different scales, some containing outliers, whereas categorical features may have high cardinality which leads to sparse data.

During the construction of models for tabular datasets, the most important step in terms of leading to improvements in the model performance, is pre-processing and manipulation of the input features [@Rajkomar2018].
This includes data merging, customising, filtering and cleaning.
In a process called feature engineering one strives to create new features from the original features based on some domain knowledge.
The idea is that such engineered features enables a model to learn interactions between features, thereby facilitating more accurate prediction.
Feature engineering is an extremely laborious process with no clear recipe to follow and therefore typically cannot succesfully be implemented without some domain expertise.

Ensemble methods based upon trees are currently viewed as the most effective machine learning models in the face of tabular datasets.
As mentioned above, a possible reason for this may be their robustness to different feature scales and data types,  linked with their ability to effectively model interactions among features having different data types.

Indeed, in the context of tabular data, classical neural network approaches are no match for tree ensembles.
Although the deep learning field has advanced and matured a lot in recent years, it is not yet clear how to leverage these modern techniques to effectively build and train deep neural networks on tabular datasets.
In this thesis we explore ways of doing so.
By reviewing the most recent literature on the topic, and through empirical work, we aim to summarise best practices when using deep learning for tabular data.

## Challenges of Deep Learning for Tabular Data

Some of the challenges of deep learning for tabular data have been alluded to in earlier sections of this chapter.
These will form the framework for the literature review which follow later on, therefore they are summarised below, in terms of relevant questions to ask when applyting deep learning in the context of tabular data:

- **How should input features be represented numerically?**: We have mentioned that tabular data consist of mixed feature types, *i.e.* a combination of categorical and continuous features. The question here relates to how these features should be processed and presented to the model during training.
- **How can we exploit feature interactions?**: Once we have found the optimal feature representation for all feature types, we will need a way to effectively learn the interactions between them and also a way to learn how they relate to the target. This a crucial step towards the effective application of deep learning models to tabular data.
- **How can we be more sample efficient?**:  Tabular datasets are typically smaller than the datasets use in computer vision and in NLP. Moreover, no general large dataset with universal properties exists to be used by a model to learn from (as is the case in for example image classification). Thus a key challenge is to facilitate learning from less data.
- **How do we interpret model decisions?**: The use of deep learning is often restricted by its perceived lack of interpretability. Therefore we need ways of explaining the model output in order for it to be useful in many applications.

Clearly there are plenty of considerations when it comes to using deep learning for tabular data. 
The main objective of this thesis is to find the best ways of overcoming the above challenges. 
Towards this objective, the study should lead to a thorough understanding of the *status quo* of the field, and of the necessary factors in order to ensure deep learning to be as effective in other data domains as it currently is in computer vision and NLP.

The study is divided in two parts. 
We start by first providing an overview of the relevant literature.
Subsequently, we make use of experimental work in order to compare various deep learning algorithms (and possible improvements) on relevant datasets. 
Here an important aim will be to ensure our experiments to be *rigorous*.
The importance of rigorous research has relatively recently again been emphasised during an NIPS talk[^talk], during which researchers in the deep learning field have been criticised for the growing gap between the understanding of its techniques and practical successes.
Currently much more emphasis is placed on the latter.
The speakers urged the deep learning community to be more rigorous in their experiments where, for them, the most important part of rigor is better empiricism, not more mathematical theories. 
Better empiricism in classification may include, for example, practices such as using cross-validation to estimate the generalisation ability of a model, and reporting standard errors. 
Empirical studies should involve more than simply attempting to beat the benchmark, for example where possible, they should also involve simple experiments that facilitate understanding why some algorithms are successful, while other are not. 

[^talk]: Talk given at NIPS2017 - https://www.youtube.com/watch?v=Qi1Yry33TQE

In addition, we want the empirical work in this study to be as reproducible as possible. 
This aspect is often overlooked. 
However it is a crucial aspect, ensuring transparent and accountable reporting of results.
Reproducibility add to the value of research, since without it, researchers are not able to build on each other's work.
There all code, data and necessary documentation in order to reproduce the experiments that were done in this thesis will be available [^git]. 

[^git]: Shared publicly at https://github.com/jandremarais/tabularLearner

Having stated the objectives of this study, we now turn to a discussion of the fundamental concepts of Statistical Learning Theory.
This is followed by a more detailed summary of the thesis.

## Overview of Statistical Learning Theory \label{sec:SLT}

Machine- or statistical learning algorithms (used interchangeably) are used to perform certain tasks that are too difficult to solve with fixed rule-based programs. 
Hence statistical learning algorithms are able to use data in order to learn how to perform difficult tasks. 
For an algorithm to learn from data means that it can improve its ability to perform an assigned *task* with respect to some *performance measure*, by processing *data*.
In this section we discuss some of the important types of tasks, data and performance measures in the statistical learning field.

A learning task describes the way in which an algorithm should process an observation. 
An observation is a collection of features that have been measured, corresponding to some object or event that we want the system to process, for example an image.
We will represent an observation by a vector $\boldsymbol{x}\in\mathbb{R}^{p}$, where each element $x_{j}$ of the vector is an observed value of the $j$-th feature, $j=1,\dots,p$. 
For example, the features of an image are usually the color intensity values of the pixels in the image.

Many kinds of tasks can be solved using statistical learning. 
One of the most common learning tasks is that of *classification*, where it is expected of an algorithm to determine which of $K$ categories an input belongs to. 
In order to complete the classification task, the learning algorithm is usually asked to produce a function $f:\mathbb{R}^{p}\to \{1,\dots,K\}$.
When $y=f(\boldsymbol{x})$, the model assigns an input described by the vector $\boldsymbol{x}$ to a category identified by the numeric code $y$, called the *output* or *response*. 
In other variants of the classification task, $f$ may output a probability distribution over the possible classes.

*Regression* is another main learning task and requires the algorithm to predict a continuous value given some input. 
This task requires a function $f:\mathbb{R}^{p}\to\mathbb{R}$, where the only difference between regression and classification is the format of the output.

Learning algorithms learns such tasks by observing a relevant set of data points, *i.e.* a dataset.
A dataset containing $N$ observations of $p$ features is commonly denoted by a data matrix $X:N\times p$, 
where each row represents a different observation and where each column corresponds to a different feature of the observations, *i.e.*

$$
X = 
\begin{bmatrix}
x_{11} & x_{12} & \dots & x_{1p}\\
x_{21} & x_{22} & \dots & x_{2p}\\
\vdots & \vdots & \ddots & \vdots\\
x_{N1} & x_{N2} & \dots & x_{Np}
\end{bmatrix}.
$$
Often the dataset includes annotations for each observation in the form of a label (*i.e.* in classification) or in the form of a target value (*i.e.* in regression). 
These $N$ annotations are represented by the vector $\boldsymbol{y}$, where the element $y_{i}$ is associated with the $i$-th row of $X$.
Therefore the response vector may be denoted by

$$
\boldsymbol{y}=
\begin{bmatrix}
y_{1}\\
y_{2}\\
\vdots\\
y_{N}
\end{bmatrix}.
$$
Note that in the case of multiple labels or targets, a matrix representation $Y:N\times K$ is required.

Statistical learning algorithms can be divided into two main categories, *viz.* *supervised* and *unsupervised* algorithms.
This categorisation is determined by the presence (or absence) of annotations in the dataset to be analysed. 
Unsupervised learning algorithms learn from data consisting only of features, $X$, and are used to find useful properties and structure in the dataset [see @Hastie2009, Ch. 14]. 
On the other hand, superivised learning algorithms learn from datasets which consist of both features and annotations, $(X,Y)$, with the aim to model the relationship between them.
Therefore, both classification and regression are considered to be supervised learning tasks.

In order to evaluate the ability of a learning algorithm to perform its assigned task, we have to design a quantitative performance measure. 
For example, in a classification task we are usually interested in the accuracy of the algorithm, *i.e.* the percentage of times that the algorithm assigns the correct classification. 
We are mostly interested in how well the learning algorithm performs on data that it has not seen before, since this demonstrates how well it will perform in real-world situations. 
Thus we typically evaluate the algorithm on a *test set* of data points. 
This dataset is independent of the *training set* of data points that used during the learning process.

For a more concrete example of supervised learning, and keeping in mind that the linear model is one of the main building blocks of neural networks, consider the learning task underlying *linear regression*. 
The objective here is to construct a system which takes a vector $\boldsymbol{x}\in \mathbb{R}^{p}$ as input and which predicts the value of a scalar $y\in \mathbb{R}$ as response. 
In the case of linear regression, we assume the output be a linear function of the input. 
Let $\hat{y}$ be the predicted response. We define the output to be 

$$
\hat{y}=\hat{\boldsymbol{w}}^{T}\boldsymbol{x},
$$
where $\hat{\boldsymbol{w}}=[w_{0},w_{1},\dots,w_{p}]$ denotes a vector of parameters and where $\boldsymbol{x}=[1,x_{1},x_{2},\dots,x_{p}]$. 
Note that an intercept is included in the model (also known as a *bias* in machine learning). 
The parameters are values that control the behaviour of the system. We can think of them as a set of *weights* that determine how each feature affects the prediction. 
Hence the learning task can be defined as predicting $y$ from $\boldsymbol{x}$ through $\hat{y}=\hat{\boldsymbol{w}}^{T}\boldsymbol{x}$.

We of course need to define a performance measure to evaluate the linear predictions. 
For a set of observations, an evaluation metric tells us how (dis)similar the predicted output is to the actual response values. 
A very common measure of performance in regression is the *mean squared error* (MSE), given by

$$
MSE = \frac{1}{N}\sum_{i=1}^{N}(y_{i}-\hat{y}_{i})^{2}.
$$
The process of learning from the data (or fitting a model to the data) can be reduced to the following optimisation problem: find the set of weights, $\hat{\boldsymbol{w}}$, which produces a $\hat{\boldsymbol{y}}$ that minimises the MSE. 
Of course this problem has a closed form solution and can quite trivially be found by means of *ordinary least squares* (OLS) [see @Hastie2009, p. 12]. 
However, we have mentioned that we are more interested in the algorithm's performance evaluated on a test set. 
Unfortunately the least squares solution does not guarantee the solution to be optimal in terms of the MSE on a test set, rendering statistical learning to be much more than a pure optimisation problem.

The ability of a model to perform well on previously unobserved inputs is referred to as its *generalisation* ability. 
To be able to fit a model that generalises well to new unseen data cases is the key challenge of statistical learning. 
One way of improving the generalisation ability of a linear regression model is to modify the optimisation criterion $J$, to include a *weight decay* (or *regularisation*) term. 
That is, we want to minimise
$$
J(\boldsymbol{w})=MSE_{\text{train}} +\lambda\boldsymbol{w}^{T}\boldsymbol{w},
$$
where $J(\boldsymbol{w})$ now expresses preference for smaller weights. 
The parameter $\lambda$ is non-negative and needs to be specified ahead of time. 
It controls the strength of the preference by determining how much influence the penalty term, $\boldsymbol{w}^{T}\boldsymbol{w}$, has on the optimisation criterion. 
If $\lambda=0$, no preference is imposed, and the solution is equivalent to the OLS solution. Larger values of $\lambda$ force the weights to decrease, and thus referred to as a so-called *shrinkage* method ([*cf*. for example @Hastie2009, pp. 61-79] and [@Goodfellow2016]. 

We can further generalise linear regression to the classification scenario. 
First, note the different types of classification schemes. 
Consider $\mathcal{G}$, the discrete set of values which may be assumed by $G$, where $G$ is used to denote a categorical output variable (instead of $Y$). 
Let $|\mathcal{G}|=K$ denote the number of discrete categories in the set $\mathcal{G}$. 
The simplest form of classification is known as binary classification and refers to scenarios where the input is associated with only one of two possible classes, *i.e.* $K=2$. 
When $K>2$, the task is known as multiclass classification. 
In multi-label classification an input may be associated with multiple classes (out of $K$ available classes), where the number of classes that each observation belongs to, is unknown.
Here we start by introducing the two single label classification setups, *viz*. binary and multiclass classification.

In multiclass classification, given the input values $\boldsymbol{X}$, we would like to accurately predict the output, $G$, which we denote by $\hat{G}$.
One approach would be to represent $G$ by an indicator vector $\boldsymbol{Y}_{G}:K\times1$, with elements all zero except in the $G$-th position, where it is assigned a 1, *i.e.* $Y_{k}=1$ for $k=G$ and $Y_{k}=0$ for $k\neq G$, $k=1,2,...,K$. 
We may then treat each of the elements in $\boldsymbol{Y}_{G}$ as quantitative outputs, and predict values for them, denoted by $\hat{\boldsymbol{Y}}=[\hat{Y}_{1},\dots,\hat{Y}_{K}]$. 
The class with the highest predicted value will then be the final categorical prediction of the classifer, *i.e.* $\hat{G}=\arg\max_{k\in\{1,\dots,K\}}\hat{Y}_{k}$.

Within the above framework we therefore seek a function of the inputs which is able to produce accurate predictions of the class scores, *i.e.*
$$
\hat{Y}_{k}=\hat{f}_{k}(\boldsymbol{X}),
$$
for $k=1,\dots, K$. Here $\hat{f}_{k}$ is an estimate of the true function, $f_{k}$, which is meant to capture the relationship between the inputs and output of class $k$. 
As with the linear regression case described above, we can use a linear model $\hat{f}_{k}(\boldsymbol{X})=\hat{\boldsymbol{w}}_{k}^{T}\boldsymbol{X}$ to approximate the true function. 
The linear model for classification divides the input space into a collection of regions labelled according to the classification, where the division is done by linear *decision boundaries* (see \autoref{fig:lin_bound} for an illustration). 
The decision boundary between classes $k$ and $l$ is the set of points for which $\hat{f}_{k}(\boldsymbol{x})=\hat{f}_{l}(\boldsymbol{x})$. 
These set of points form an affine set or hyperplane in the input space.

![Linear model on simple binary classification dataset.\label{fig:lin_bound}](figures/linear_boundary.pdf)

After the weights are estimated from the data, an observation represented by $\boldsymbol{x}$ (including the unit element) can be classified as follows:

- Compute $\hat{f}_{k}(\boldsymbol{x})=\hat{\boldsymbol{w}}_{k}^{T}\boldsymbol{x}$ for all $k=1,\dots,K$.
- Identify the largest component and classify to the corresponding class, *i.e.* $\hat{G}=\arg\max_{k\in\{1,\dots,K\}}\hat{f}_{k}(\boldsymbol{x})$.

One may view the predicted class scores as estimates of the conditional class probabilities (or posterior probabilities), *i.e.* $P(G=k|\boldsymbol{X}=\boldsymbol{x})\approx \hat{f}_{k}(\boldsymbol{x})$. 
However, these values are not the best estimates of posterior probabilities. 
Although the values sum to 1, they do not lie in the interval [0,1]. 
A way to overcome this problem is to estimate posterior probabilities using the *logit transform* of $\hat{f}_{k}(\boldsymbol{x})$. 
That is,
$$
P(G=k|\boldsymbol{X}=\boldsymbol{x})\approx\frac{e^{\hat{f}_{k}(\boldsymbol{x})}}{\sum_{l=1}e^{\hat{f}_{l}(\boldsymbol{x})}}.
$$
Through this transformation, the estimates of the posterior probabilities both sum to 1 and are contained in [0,1]. 
The above model is the well-known *logistic regression* model [@Hastie2009, p. 119]. 
With this formulation there is no closed form solution for the weights. 
Instead, the weight estimates may be searched for by maximising the log-likelihood function. 
One way of doing this is by minimising the negative log-likelihood using gradient descent, which will be discussed in the next chapter.

Finally in this section, note that any supervised learning problem can also be viewed as a function approximation problem. 
Suppose we are trying to predict a variable $Y$ given an input vector $\boldsymbol{X}$, where we assume the true relationship between them to be given by
$$
Y=f(\boldsymbol{X})+\epsilon,
$$
where $\epsilon$ represents the part of $Y$ that is not predictable from $\boldsymbol{X}$, because of, for example, incomplete features or noise present in the labels. 
Then in function approximation we are estimating $f$ with an estimate $\hat{f}$. 
In parametric function approximation, for example in linear regression, estimation of $f(\boldsymbol{X},\theta)$ is equivalent to estimating the optimal set of weights, $\hat{\theta}$. 
In the remainder of the thesis, we refer to $\hat{f}$ as the *model*, *classifier* or *learner*.

## Outline

This chapter provided the context, motivation, objectives and theoretical background of this study.
An outline of the remainder of the thesis follows below:

In \Cref{chp:nn}, the theory underlying neural networks is described.
The building blocks of neural networks are discussed, thereby introducing neurons, basic layers and the way in which neural networks are trained.
The important concept of regularisation is also discussed.
Using the perspective of representation- and manifold learning, we then attempt to gain insight into what happens inside an neural network.

\Cref{chp:dl} continues the discussion by focusing on the key advances in neural networks in recent times.
The idea is that all concepts introduced in this chapter should potentially be able to facilitate the construction of improved deep neural networks on tabular data.
Improved ways of fighting overfitting, such as data augmentation, the use of dropout and transfer learning, as well as the SotA training policy called *1Cycle* are analysed here.
New developments in architectural design are also highlighted.
The chapter concludes with approaches towards interpretting neural networks and their predictions.

\Cref{chp:td} may be viewed as a core chapter of this thesis.
It mainly serves as a literature review of all research with regard to deep learning for tabular data.
The chapter is organised according to the modelling challenges faced when using deep learning for tabular data, 
investigating and comparing what other researchers have done in order to overcome these challenges.
It will be seen that the key concept involves finding the right representation for tabular data.
This may be done through embeddings, and by means of designing architectures that can efficiently learn feature interactions.
This is for example done with attention models, possibly with the help of unsupervised pretraining.

In \Cref{chp:exp} we empirically evaluate several claims made in the literature.
The aim of the chapter is to evaluate and compare different approaches to tackling the various challenges.
Hence the main experiments involve evaluating neural networks at various samples sizes, evaluating potential gains from doing unsupervised pretraining and using data augmentation, 
and comparing attention modules with classic fully-connected layers.
We also illustrate a way in which resulting neural networks may be interpreted by means of permutation importance and knowledge distillation.

The thesis concludes in \Cref{chp:conc}, where we summarise the work, some highlights and the main take-home points.
The limitations of this study are discussed, and promising future research directions are identified.


