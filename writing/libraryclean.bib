% This file was created with JabRef 2.10.
% Encoding: UTF-8

@inproceedings{Kohavi1996,
  title={Scaling up the accuracy of Naive-Bayes classifiers: a decision-tree hybrid.},
  author={Kohavi, Ron},
  organization={Citeseer},
  year={1996}
}

@misc{Smith2015b,
  author = {Smith, Leslie},
  title = {Cyclical Learning Rates for Training Neural Networks},
  year = 2015
}

@incollection{Sriva2015,
title = {Training Very Deep Networks},
author = {Srivastava, Rupesh K and Greff, Klaus and Schmidhuber, J\"{u}rgen},
booktitle = {Advances in Neural Information Processing Systems 28},
pages = {2377--2385},
year = {2015}
}


@inproceedings{He2016,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
year = {2016},
month = {10},
pages = {630-645},
title = {Identity Mappings in Deep Residual Networks},
volume = {9908}
}

@inproceedings{Orhan2018,
title={Skip Connections Eliminate Singularities},
author={Emin, Orhan and Xaq, Pitkow},
booktitle={International Conference on Learning Representations},
year={2018},
}

@article{Sriva2014,
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal = {Journal of machine learning research},
  number = 1,
  pages = {1929--1958},
  title = {Dropout: a simple way to prevent neural networks from overfitting},
  volume = 15,
  year = 2014
}

@article{Utgoff2002,
  author    = {Paul Utgoff and
               David Stracuzzi},
  title     = {Many-Layered Learning},
  journal   = {Neural Computation},
  volume    = {14},
  number    = {10},
  pages     = {2497--2529},
  year      = {2002}
}

@article{Hinton1989,
    author = {Geoffrey Hinton},
    title = {Connectionist Learning Procedures},
    year = {1989}
}
@book{Rumelhart1986,
 editor = {McClelland, James and Rumelhart, David and the PDP Research Group},
 title = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations},
 year = {1986}
 publisher = {MIT Press}
} 

@article{Alexei1965,
author = {Ivakhnenko, Alexei and Lapa, Valentin},
year = {1965},
month = {04},
pages = {250},
title = {Cybernetic Predicting Devices}
}

@article{Karras2017,
  author    = {Tero Karras and
               Timo Aila and
               Samuli Laine and
               Jaakko Lehtinen},
  title     = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},
  journal   = {CoRR},
  volume    = {abs/1710.10196},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.10196},
  archivePrefix = {arXiv},
  eprint    = {1710.10196},
  timestamp = {Mon, 13 Aug 2018 16:46:42 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1710-10196},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Donahue2018,
  title={Synthesizing Audio with Generative Adversarial Networks},
  author={Donahue, Chris and McAuley, Julian and Puckette, Miller},
  journal={arXiv preprint arXiv:1802.04208},
  year={2018}
}


@inproceedings{Goodfellow2014,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}

@article{Lundberg2017,
  author    = {Scott Lundberg and
               Su{-}In Lee},
  title     = {A unified approach to interpreting model predictions},
  journal   = {CoRR},
  volume    = {abs/1705.07874},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.07874},
  archivePrefix = {arXiv},
  eprint    = {1705.07874},
  timestamp = {Mon, 13 Aug 2018 16:48:04 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LundbergL17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Friedman2001,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  pages={1189--1232},
  year={2001},
  publisher={JSTOR}
}


@article{Breiman2001,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  number={1},
  pages={5--32},
  year={2001},
  publisher={Springer}
}


@article{Hinton2015,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@article{Erhan2009,
  title={Visualizing higher-layer features of a deep network},
  author={Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  year={2009}
}


@inproceedings{Selvaraju2017,
  title={Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization.},
  author={Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv and others},
  booktitle={ICCV},
  pages={618--626},
  year={2017}
}


@inproceedings{Zhou2016,
  title={Learning deep features for discriminative localization},
  author={Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2921--2929},
  year={2016}
}


@article{Smith2015a,
  author    = {Leslie N. Smith},
  title     = {No More Pesky Learning Rate Guessing Games},
  journal   = {CoRR},
  volume    = {abs/1506.01186},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.01186},
  archivePrefix = {arXiv},
  eprint    = {1506.01186},
  timestamp = {Mon, 13 Aug 2018 16:47:53 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/Smith15a},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Smith2017,
  author    = {Leslie N. Smith and
               Nicholay Topin},
  title     = {Super-Convergence: Very Fast Training of Residual Networks Using Large
               Learning Rates},
  journal   = {CoRR},
  volume    = {abs/1708.07120},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.07120},
  archivePrefix = {arXiv},
  eprint    = {1708.07120},
  timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-07120},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Cheng2016a,
  author    = {Jianpeng Cheng and
               Li Dong and
               Mirella Lapata},
  title     = {Long Short-Term Memory-Networks for Machine Reading},
  journal   = {CoRR},
  volume    = {abs/1601.06733},
  year      = {2016},
  url       = {http://arxiv.org/abs/1601.06733},
  archivePrefix = {arXiv},
  eprint    = {1601.06733},
  timestamp = {Mon, 13 Aug 2018 16:48:39 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ChengDL16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Duong2016,
  title={An attentional model for speech translation without transcription},
  author={Duong, Long and Anastasopoulos, Antonios and Chiang, David and Bird, Steven and Cohn, Trevor},
  booktitle={Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={949--959},
  year={2016}
}


@article{Xu2015,
  author    = {Kelvin Xu and
               Jimmy Ba and
               Ryan Kiros and
               Kyunghyun Cho and
               Aaron C. Courville and
               Ruslan Salakhutdinov and
               Richard S. Zemel and
               Yoshua Bengio},
  title     = {Show, Attend and Tell: Neural Image Caption Generation with Visual
               Attention},
  journal   = {CoRR},
  volume    = {abs/1502.03044},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.03044},
  archivePrefix = {arXiv},
  eprint    = {1502.03044},
  timestamp = {Mon, 13 Aug 2018 16:47:52 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/XuBKCCSZB15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Mikolov2013,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}


@article{Huang2017,
  title={Learning deep resnet blocks sequentially using boosting theory},
  author={Huang, Furong and Ash, Jordan and Langford, John and Schapire, Robert},
  journal={arXiv preprint arXiv:1706.04964},
  year={2017}
}

@article{Huang2016,
  author    = {Gao Huang and
               Zhuang Liu and
               Kilian Q. Weinberger},
  title     = {Densely Connected Convolutional Networks},
  journal   = {CoRR},
  volume    = {abs/1608.06993},
  year      = {2016},
  url       = {http://arxiv.org/abs/1608.06993},
  archivePrefix = {arXiv},
  eprint    = {1608.06993},
  timestamp = {Mon, 10 Sep 2018 15:49:32 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HuangLW16a},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Zeiler2014,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer}
}


@article{Yosinski2015,
  title={Understanding neural networks through deep visualization},
  author={Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
  journal={arXiv preprint arXiv:1506.06579},
  year={2015}
}


@article{Howard2018,
  title={Fine-tuned Language Models for Text Classification},
  author={Howard, Jeremy and Ruder, Sebastian},
  journal={arXiv preprint arXiv:1801.06146},
  year={2018}
}


@article{Devlin2018,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}


@article{Kingma2013,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}


@article{Alain2014,
  title={What regularized auto-encoders learn from the data-generating distribution},
  author={Alain, Guillaume and Bengio, Yoshua},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={3563--3593},
  year={2014},
  publisher={JMLR. org}
}


@article{Larochelle2009,
  title={Exploring strategies for training deep neural networks},
  author={Larochelle, Hugo and Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Lamblin, Pascal},
  journal={Journal of machine learning research},
  volume={10},
  number={Jan},
  pages={1--40},
  year={2009}
}


@article{Hinton2006a,
  title={Reducing the dimensionality of data with neural networks},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  journal={science},
  volume={313},
  number={5786},
  pages={504--507},
  year={2006},
  publisher={American Association for the Advancement of Science}
}


@article{Bengio2012,
  author    = {Yoshua Bengio and
               Nicolas Boulanger{-}Lewandowski and
               Razvan Pascanu},
  title     = {Advances in Optimizing Recurrent Networks},
  journal   = {CoRR},
  volume    = {abs/1212.0901},
  year      = {2012},
  url       = {http://arxiv.org/abs/1212.0901},
  archivePrefix = {arXiv},
  eprint    = {1212.0901},
  timestamp = {Mon, 13 Aug 2018 16:48:06 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1212-0901},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Duchi2011,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Jul},
  pages={2121--2159},
  year={2011}
}


@article{Kingma2014,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  journal   = {CoRR},
  volume    = {abs/1412.6980},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.6980},
  archivePrefix = {arXiv},
  eprint    = {1412.6980},
  timestamp = {Mon, 13 Aug 2018 16:47:35 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KingmaB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Choromanska2014,
  author    = {Anna Choromanska and
               Mikael Henaff and
               Micha{\"{e}}l Mathieu and
               G{\'{e}}rard Ben Arous and
               Yann LeCun},
  title     = {The Loss Surface of Multilayer Networks},
  journal   = {CoRR},
  volume    = {abs/1412.0233},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.0233},
  archivePrefix = {arXiv},
  eprint    = {1412.0233},
  timestamp = {Mon, 13 Aug 2018 16:48:26 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ChoromanskaHMAL14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Ba2013,
  author    = {Lei Jimmy Ba and
               Rich Caurana},
  title     = {Do Deep Nets Really Need to be Deep?},
  journal   = {CoRR},
  volume    = {abs/1312.6184},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.6184},
  archivePrefix = {arXiv},
  eprint    = {1312.6184},
  timestamp = {Mon, 13 Aug 2018 16:46:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BaC13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Cybenko1989,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}


@inproceedings{Maas2013,
  title={Rectifier nonlinearities improve neural network acoustic models},
  author={Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y}
}


@article{Sun2014,
  author    = {Yi Sun and
               Xiaogang Wang and
               Xiaoou Tang},
  title     = {Deeply learned face representations are sparse, selective, and robust},
  journal   = {CoRR},
  volume    = {abs/1412.1265},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.1265},
  archivePrefix = {arXiv},
  eprint    = {1412.1265},
  timestamp = {Mon, 13 Aug 2018 16:46:48 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SunWT14a},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Frosst2017,
  author    = {Nicholas Frosst and
               Geoffrey E. Hinton},
  title     = {Distilling a Neural Network Into a Soft Decision Tree},
  journal   = {CoRR},
  volume    = {abs/1711.09784},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.09784},
  archivePrefix = {arXiv},
  eprint    = {1711.09784},
  timestamp = {Mon, 13 Aug 2018 16:48:56 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1711-09784},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{Chapelle2001,
title = {Vicinal Risk Minimization},
author = {Olivier Chapelle and Weston, Jason and Bottou, L\'{e}on and Vladimir Vapnik},
booktitle = {Advances in Neural Information Processing Systems 13},
editor = {T. K. Leen and T. G. Dietterich and V. Tresp},
pages = {416--422},
year = {2001},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/1876-vicinal-risk-minimization.pdf}
}

@article{Geras2014,
  author    = {Krzysztof J. Geras and
               Charles A. Sutton},
  title     = {Scheduled denoising autoencoders},
  journal   = {CoRR},
  volume    = {abs/1406.3269},
  year      = {2014},
  url       = {http://arxiv.org/abs/1406.3269},
  archivePrefix = {arXiv},
  eprint    = {1406.3269},
  timestamp = {Mon, 13 Aug 2018 16:46:18 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/GerasS14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Delgado2014,
  author  = {Manuel Fern\'{a}ndez-Delgado and Eva Cernadas and Sen\'{e}n Barro and Dinani Amorim},
  title   = {Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  pages   = {3133-3181},
  url     = {http://jmlr.org/papers/v15/delgado14a.html}
}

@inproceedings{Shi2016,
  title={Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network},
  author={Shi, Wenzhe and Caballero, Jose and Husz{\'a}r, Ferenc and Totz, Johannes and Aitken, Andrew P and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1874--1883},
  year={2016}
}

@article{Silver2017,
  title={Mastering the game of Go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{Mnih2013,
  author    = {Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver and
               Alex Graves and
               Ioannis Antonoglou and
               Daan Wierstra and
               Martin A. Riedmiller},
  title     = {Playing Atari with Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1312.5602},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.5602},
  archivePrefix = {arXiv},
  eprint    = {1312.5602},
  timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MnihKSGAWR13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Fridman2017,
  author    = {Lex Fridman and
               Daniel E. Brown and
               Michael Glazer and
               William Angell and
               Spencer Dodd and
               Benedikt Jenik and
               Jack Terwilliger and
               Julia Kindelsberger and
               Li Ding and
               Sean Seaman and
               Hillary Abraham and
               Alea Mehler and
               Andrew Sipperley and
               Anthony Pettinato and
               Bobbie Seppelt and
               Linda Angell and
               Bruce Mehler and
               Bryan Reimer},
  title     = {{MIT} Autonomous Vehicle Technology Study: Large-Scale Deep Learning
               Based Analysis of Driver Behavior and Interaction with Automation},
  journal   = {CoRR},
  volume    = {abs/1711.06976},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.06976},
  archivePrefix = {arXiv},
  eprint    = {1711.06976},
  timestamp = {Mon, 13 Aug 2018 16:46:48 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1711-06976},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Mogren2016,
  author    = {Olof Mogren},
  title     = {{C-RNN-GAN:} Continuous recurrent neural networks with adversarial
               training},
  journal   = {CoRR},
  volume    = {abs/1611.09904},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.09904},
  archivePrefix = {arXiv},
  eprint    = {1611.09904},
  timestamp = {Mon, 13 Aug 2018 16:46:34 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/Mogren16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Gatys2015,
  author    = {Leon A. Gatys and
               Alexander S. Ecker and
               Matthias Bethge},
  title     = {A Neural Algorithm of Artistic Style},
  journal   = {CoRR},
  volume    = {abs/1508.06576},
  year      = {2015},
  url       = {http://arxiv.org/abs/1508.06576},
  archivePrefix = {arXiv},
  eprint    = {1508.06576},
  timestamp = {Mon, 13 Aug 2018 16:48:03 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/GatysEB15a},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Wu2016,
  author    = {Yonghui Wu and
               Mike Schuster and
               Zhifeng Chen and
               Quoc V. Le and
               Mohammad Norouzi and
               Wolfgang Macherey and
               Maxim Krikun and
               Yuan Cao and
               Qin Gao and
               Klaus Macherey and
               Jeff Klingner and
               Apurva Shah and
               Melvin Johnson and
               Xiaobing Liu and
               Lukasz Kaiser and
               Stephan Gouws and
               Yoshikiyo Kato and
               Taku Kudo and
               Hideto Kazawa and
               Keith Stevens and
               George Kurian and
               Nishant Patil and
               Wei Wang and
               Cliff Young and
               Jason Smith and
               Jason Riesa and
               Alex Rudnick and
               Oriol Vinyals and
               Greg Corrado and
               Macduff Hughes and
               Jeffrey Dean},
  title     = {Google's Neural Machine Translation System: Bridging the Gap between
               Human and Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1609.08144},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.08144},
  archivePrefix = {arXiv},
  eprint    = {1609.08144},
  timestamp = {Mon, 13 Aug 2018 16:46:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/WuSCLNMKCGMKSJL16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{Sarikaya2017,
author={R. Sarikaya},
journal={IEEE Signal Processing Magazine},
title={The Technology Behind Personal Digital Assistants: An overview of the system architecture and key components},
year={2017},
volume={34},
number={1},
pages={67-81},
keywords={human computer interaction;learning (artificial intelligence);mobile computing;notebook computers;speech recognition;personal digital assistant;language understanding;LU;natural user interface;computer interaction;speech recognition;machine learning;user productivity;system-initiated task completion;user-initiated task completion;Web service;personal life management;work productivity;PDA;human-computer interaction;Handheld computers;Mobile handsets;Magnetic sensors;Natural language processing;Web services},
doi={10.1109/MSP.2016.2617341},
ISSN={1053-5888},
month={Jan},}

@article{Battenberg2017,
  author    = {Eric Battenberg and
               Jitong Chen and
               Rewon Child and
               Adam Coates and
               Yashesh Gaur and
               Yi Li and
               Hairong Liu and
               Sanjeev Satheesh and
               David Seetapun and
               Anuroop Sriram and
               Zhenyao Zhu},
  title     = {Exploring Neural Transducers for End-to-End Speech Recognition},
  journal   = {CoRR},
  volume    = {abs/1707.07413},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.07413},
  archivePrefix = {arXiv},
  eprint    = {1707.07413},
  timestamp = {Mon, 13 Aug 2018 16:46:32 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BattenbergCCCGL17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Hu2017,
  author    = {Jie Hu and
               Li Shen and
               Gang Sun},
  title     = {Squeeze-and-Excitation Networks},
  journal   = {CoRR},
  volume    = {abs/1709.01507},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.01507},
  archivePrefix = {arXiv},
  eprint    = {1709.01507},
  timestamp = {Mon, 13 Aug 2018 16:47:05 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1709-01507},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Covington2016,
title	= {Deep Neural Networks for YouTube Recommendations},
author	= {Paul Covington and Jay Adams and Emre Sargin},
year	= {2016},
booktitle	= {Proceedings of the 10th ACM Conference on Recommender Systems},
address	= {New York, NY, USA}
}

@ARTICLE{Zhou2017,
   author = {{Zhou}, G. and {Song}, C. and {Zhu}, X. and {Fan}, Y. and {Zhu}, H. and 
	{Ma}, X. and {Yan}, Y. and {Jin}, J. and {Li}, H. and {Gai}, K.
	},
    title = "{Deep Interest Network for Click-Through Rate Prediction}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1706.06978},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, I.2.6, H.3.2},
     year = 2017,
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170606978Z},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Vaswani2017,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {CoRR},
  volume    = {abs/1706.03762},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
  archivePrefix = {arXiv},
  eprint    = {1706.03762},
  timestamp = {Mon, 13 Aug 2018 16:48:37 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/VaswaniSPUJGKP17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Klambauer2017,
  author    = {G{\"{u}}nter Klambauer and
               Thomas Unterthiner and
               Andreas Mayr and
               Sepp Hochreiter},
  title     = {Self-Normalizing Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1706.02515},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.02515},
  archivePrefix = {arXiv},
  eprint    = {1706.02515},
  timestamp = {Mon, 13 Aug 2018 16:46:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KlambauerUMH17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Wang2017b,
  author    = {Ruoxi Wang and
               Bin Fu and
               Gang Fu and
               Mingliang Wang},
  title     = {Deep {\&} Cross Network for Ad Click Predictions},
  journal   = {CoRR},
  volume    = {abs/1708.05123},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.05123},
  archivePrefix = {arXiv},
  eprint    = {1708.05123},
  timestamp = {Mon, 13 Aug 2018 16:46:07 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-05123},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Bahdanau2014,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  journal   = {CoRR},
  volume    = {abs/1409.0473},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.0473},
  archivePrefix = {arXiv},
  eprint    = {1409.0473},
  timestamp = {Mon, 13 Aug 2018 16:46:05 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BahdanauCB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{He2015,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1512.03385},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.03385},
  archivePrefix = {arXiv},
  eprint    = {1512.03385},
  timestamp = {Mon, 13 Aug 2018 16:46:56 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HeZRS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{He2015a,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on
               ImageNet Classification},
  journal   = {CoRR},
  volume    = {abs/1502.01852},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.01852},
  archivePrefix = {arXiv},
  eprint    = {1502.01852},
  timestamp = {Mon, 13 Aug 2018 16:47:36 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HeZR015},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Song2018,
  author    = {Weiping Song and
               Chence Shi and
               Zhiping Xiao and
               Zhijian Duan and
               Yewen Xu and
               Ming Zhang and
               Jian Tang},
  title     = {AutoInt: Automatic Feature Interaction Learning via Self-Attentive
               Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1810.11921},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.11921},
  archivePrefix = {arXiv},
  eprint    = {1810.11921},
  timestamp = {Tue, 06 Nov 2018 14:23:59 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-11921},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{    
anonymous2019,    
title={TabNN: A Universal Neural Network Solution for Tabular Data},    
author={Anonymous},    
booktitle={Submitted to International Conference on Learning Representations},    
year={2019},    
url={https://openreview.net/forum?id=r1eJssCqY7},    
note={under review}    
}

@article{Cheng2016,
  author    = {Heng{-}Tze Cheng and
               Levent Koc and
               Jeremiah Harmsen and
               Tal Shaked and
               Tushar Chandra and
               Hrishi Aradhye and
               Glen Anderson and
               Greg Corrado and
               Wei Chai and
               Mustafa Ispir and
               Rohan Anil and
               Zakaria Haque and
               Lichan Hong and
               Vihan Jain and
               Xiaobing Liu and
               Hemal Shah},
  title     = {Wide {\&} Deep Learning for Recommender Systems},
  journal   = {CoRR},
  volume    = {abs/1606.07792},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.07792},
  archivePrefix = {arXiv},
  eprint    = {1606.07792},
  timestamp = {Mon, 13 Aug 2018 16:47:53 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ChengKHSCAACCIA16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Zhang2016,
  author    = {Weinan Zhang and
               Tianming Du and
               Jun Wang},
  title     = {Deep Learning over Multi-field Categorical Data: {A} Case Study on
               User Response Prediction},
  journal   = {CoRR},
  volume    = {abs/1601.02376},
  year      = {2016},
  url       = {http://arxiv.org/abs/1601.02376},
  archivePrefix = {arXiv},
  eprint    = {1601.02376},
  timestamp = {Mon, 13 Aug 2018 16:47:51 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ZhangDW16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Qu2016,
  author    = {Yanru Qu and
               Han Cai and
               Kan Ren and
               Weinan Zhang and
               Yong Yu and
               Ying Wen and
               Jun Wang},
  title     = {Product-based Neural Networks for User Response Prediction},
  journal   = {CoRR},
  volume    = {abs/1611.00144},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.00144},
  archivePrefix = {arXiv},
  eprint    = {1611.00144},
  timestamp = {Mon, 13 Aug 2018 16:47:51 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/QuCRZYWW16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Guo2017,
  author    = {Huifeng Guo and
               Ruiming Tang and
               Yunming Ye and
               Zhenguo Li and
               Xiuqiang He},
  title     = {DeepFM: {A} Factorization-Machine based Neural Network for {CTR} Prediction},
  journal   = {CoRR},
  volume    = {abs/1703.04247},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.04247},
  archivePrefix = {arXiv},
  eprint    = {1703.04247},
  timestamp = {Mon, 13 Aug 2018 16:49:16 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/GuoTYLH17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{Dua2017,
author = "Dheeru, Dua and Karra Taniskidou, Efi",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@ARTICLE{haldar2018,
   author = {{Haldar}, M. and {Abdool}, M. and {Ramanathan}, P. and {Xu}, T. and 
	{Yang}, S. and {Duan}, H. and {Zhang}, Q. and {Barrow-Williams}, N. and 
	{Turnbull}, B.~C. and {Collins}, B.~M. and {Legrand}, T.},
    title = "{Applying Deep Learning To Airbnb Search}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1810.09591},
 keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, Statistics - Machine Learning},
     year = 2018,
    month = oct,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181009591H},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{kasar2018,
   author = {{Kosar}, R. and {Scott}, D.~W.},
    title = "{The Hybrid Bootstrap: A Drop-in Replacement for Dropout}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1801.07316},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
     year = 2018,
    month = jan,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180107316K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Che2016,
  title={Interpretable Deep Models for ICU Outcome Prediction},
  author={Zhengping Che and Sanjay Purushotham and Robinder G. Khemani and Yan Liu},
  journal={AMIA ... Annual Symposium proceedings. AMIA Symposium},
  year={2016},
  volume={2016},
  pages={
          371-380
        }
}

@article{Shickel2017,
  author    = {Benjamin Shickel and
               Patrick Tighe and
               Azra Bihorac and
               Parisa Rashidi},
  title     = {Deep {EHR:} {A} Survey of Recent Advances on Deep Learning Techniques
               for Electronic Health Record {(EHR)} Analysis},
  journal   = {CoRR},
  volume    = {abs/1706.03446},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03446},
  archivePrefix = {arXiv},
  eprint    = {1706.03446},
  timestamp = {Mon, 13 Aug 2018 16:46:19 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ShickelTBR17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Vincent2008,
 author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
 title = {Extracting and Composing Robust Features with Denoising Autoencoders},
 booktitle = {Proceedings of the 25th International Conference on Machine Learning},
 series = {ICML '08},
 year = {2008},
 isbn = {978-1-60558-205-4},
 location = {Helsinki, Finland},
 pages = {1096--1103},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1390156.1390294},
 doi = {10.1145/1390156.1390294},
 acmid = {1390294},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Perez2017,
  author    = {Luis Perez and
               Jason Wang},
  title     = {The Effectiveness of Data Augmentation in Image Classification using
               Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1712.04621},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.04621},
  archivePrefix = {arXiv},
  eprint    = {1712.04621},
  timestamp = {Mon, 13 Aug 2018 16:48:03 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1712-04621},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Zhang2017,
  author    = {Hongyi Zhang and
               Moustapha Ciss{\'{e}} and
               Yann N. Dauphin and
               David Lopez{-}Paz},
  title     = {mixup: Beyond Empirical Risk Minimization},
  journal   = {CoRR},
  volume    = {abs/1710.09412},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.09412},
  archivePrefix = {arXiv},
  eprint    = {1710.09412},
  timestamp = {Mon, 13 Aug 2018 16:47:14 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1710-09412},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{VanDerMaaten2013,
 author = {Van Der Maaten, Laurens and Chen, Minmin and Tyree, Stephen and Weinberger, Kilian Q.},
 title = {Learning with Marginalized Corrupted Features},
 booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
 series = {ICML'13},
 year = {2013},
 location = {Atlanta, GA, USA},
 pages = {I-410--I-418},
 url = {http://dl.acm.org/citation.cfm?id=3042817.3042865},
 acmid = {3042865},
 publisher = {JMLR.org},
} 

@article{Rajkomar2018,
  author    = {Alvin Rajkomar and
               Eyal Oren and
               Kai Chen and
               Andrew M. Dai and
               Nissan Hajaj and
               Peter J. Liu and
               Xiaobing Liu and
               Mimi Sun and
               Patrik Sundberg and
               Hector Yee and
               Kun Zhang and
               Gavin E. Duggan and
               Gerardo Flores and
               Michaela Hardt and
               Jamie Irvine and
               Quoc V. Le and
               Kurt Litsch and
               Jake Marcus and
               Alexander Mossin and
               Justin Tansuwan and
               De Wang and
               James Wexler and
               Jimbo Wilson and
               Dana Ludwig and
               Samuel L. Volchenboum and
               Katherine Chou and
               Michael Pearson and
               Srinivasan Madabushi and
               Nigam H. Shah and
               Atul J. Butte and
               Michael Howell and
               Claire Cui and
               Greg Corrado and
               Jeff Dean},
  title     = {Scalable and accurate deep learning for electronic health records},
  journal   = {CoRR},
  volume    = {abs/1801.07860},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.07860},
  archivePrefix = {arXiv},
  eprint    = {1801.07860},
  timestamp = {Mon, 13 Aug 2018 16:47:20 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-07860},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@ARTICLE{Shavitt2018,
   author = {{Shavitt}, I. and {Segal}, E.},
    title = "{Regularization Learning Networks: Deep Learning for Tabular Datasets}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1805.06440},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
     year = 2018,
    month = may,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180506440S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Bengio2013,
author={Y. Bengio and A. Courville and P. Vincent},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Representation Learning: A Review and New Perspectives},
year={2013},
volume={35},
number={8},
pages={1798-1828},
keywords={artificial intelligence;data structures;probability;unsupervised learning;representation learning;machine learning algorithms;data representation;AI;unsupervised feature learning;probabilistic models;autoencoders;manifold learning;geometrical connections;density estimation;Learning systems;Machine learning;Abstracts;Feature extraction;Manifolds;Neural networks;Speech recognition;Deep learning;representation learning;feature learning;unsupervised learning;Boltzmann machine;autoencoder;neural nets;Algorithms;Artificial Intelligence;Humans;Neural Networks (Computer)},
doi={10.1109/TPAMI.2013.50},
ISSN={0162-8828},
month={Aug},}


@inproceedings{Miotto2016,
  title={Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records},
  author={Riccardo Miotto and Li Li and Brian A. Kidd and Joel T. Dudley},
  booktitle={Scientific reports},
  year={2016}
}

@article{Guo2016,
  author    = {Cheng Guo and
               Felix Berkhahn},
  title     = {Entity Embeddings of Categorical Variables},
  journal   = {CoRR},
  volume    = {abs/1604.06737},
  year      = {2016},
  url       = {http://arxiv.org/abs/1604.06737},
  archivePrefix = {arXiv},
  eprint    = {1604.06737},
  timestamp = {Mon, 13 Aug 2018 16:49:04 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/GuoB16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Brebisson2015,
  author    = {Alexandre de Br{\'{e}}bisson and
               {\'{E}}tienne Simon and
               Alex Auvolat and
               Pascal Vincent and
               Yoshua Bengio},
  title     = {Artificial Neural Networks Applied to Taxi Destination Prediction},
  journal   = {CoRR},
  volume    = {abs/1508.00021},
  year      = {2015},
  url       = {http://arxiv.org/abs/1508.00021},
  archivePrefix = {arXiv},
  eprint    = {1508.00021},
  timestamp = {Mon, 13 Aug 2018 16:46:45 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/BrebissonSAVB15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Smith2018,
  author    = {Leslie N. Smith},
  title     = {A disciplined approach to neural network hyper-parameters: Part 1
               - learning rate, batch size, momentum, and weight decay},
  journal   = {CoRR},
  volume    = {abs/1803.09820},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.09820},
  archivePrefix = {arXiv},
  eprint    = {1803.09820},
  timestamp = {Mon, 13 Aug 2018 16:46:45 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1803-09820},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{friedman1981,
  title={Projection pursuit regression},
  author={Friedman, Jerome H and Stuetzle, Werner},
  journal={Journal of the American statistical Association},
  volume={76},
  number={376},
  pages={817--823},
  year={1981},
  publisher={Taylor \& Francis}
}

@book{rosenblatt1962,
  title={Principles of neurodynamics: perceptrons and the theory of brain mechanisms},
  author={Rosenblatt, F.},
  lccn={62012882},
  series={Report (Cornell Aeronautical Laboratory)},
  url={https://books.google.ca/books?id=7FhRAAAAMAAJ},
  year={1962},
  publisher={Spartan Books}
}

@inproceedings{Ioannou2010,
 author = {Ioannou, Marios and Sakkas, George and Tsoumakas, Grigorios and Vlahavas, Ioannis},
 title = {Obtaining Bipartitions from Score Vectors for Multi-Label Classification},
 booktitle = {Proceedings of the 2010 22Nd IEEE International Conference on Tools with Artificial Intelligence - Volume 01},
 series = {ICTAI '10},
 year = {2010},
 isbn = {978-0-7695-4263-8},
 pages = {409--416},
 numpages = {8},
 url = {http://dx.doi.org/10.1109/ICTAI.2010.65},
 doi = {10.1109/ICTAI.2010.65},
 acmid = {1917188},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {multi-label data, multi-label classification, thresholding, score vector},
} 

@inproceedings{Yang2001,
 author = {Yang, Yiming},
 title = {A Study of Thresholding Strategies for Text Categorization},
 booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
 series = {SIGIR '01},
 year = {2001},
 isbn = {1-58113-331-6},
 location = {New Orleans, Louisiana, USA},
 pages = {137--145},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/383952.383975},
 doi = {10.1145/383952.383975},
 acmid = {383975},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@ARTICLE{Szymanski2017,
   author = {{Szyma{\'n}ski}, P. and {Kajdanowicz}, T.},
    title = "{A Network Perspective on Stratification of Multi-Label Data}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1704.08756},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning, Statistics - Methodology},
     year = 2017,
    month = apr,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170408756S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Quevedo2012,
 author = {Ram\'{o}N Quevedo, Jos{\'e} and Luaces, Oscar and Bahamonde, Antonio},
 title = {Multilabel Classifiers with a Probabilistic Thresholding Strategy},
 journal = {Pattern Recogn.},
 issue_date = {February, 2012},
 volume = {45},
 number = {2},
 month = feb,
 year = {2012},
 issn = {0031-3203},
 pages = {876--883},
 numpages = {8},
 url = {http://dx.doi.org/10.1016/j.patcog.2011.08.007},
 doi = {10.1016/j.patcog.2011.08.007},
 acmid = {2304986},
 publisher = {Elsevier Science Inc.},
 address = {New York, NY, USA},
 keywords = {Expected loss, Multilabel classification, Posterior probability, Thresholding strategies},
} 



@misc{oakdenrayner2017, 
 title={Quick thoughts on ChestXray14, performance claims, and clinical tasks.}, 
 url={https://lukeoakdenrayner.wordpress.com/2017/11/18/quick-thoughts-on-chestxray14-performance-claims-and-clinical-tasks/}, 
 journal={Luke Oakden-Rayner}, 
 author={Oakden-Rayner, Luke}, 
 year={2017}, 
 month={Nov}
 }


@inproceedings{Prabhu2014,
 author = {Prabhu, Yashoteja and Varma, Manik},
 title = {FastXML: A Fast, Accurate and Stable Tree-classifier for Extreme Multi-label Learning},
 booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '14},
 year = {2014},
 isbn = {978-1-4503-2956-9},
 location = {New York, New York, USA},
 pages = {263--272},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2623330.2623651},
 doi = {10.1145/2623330.2623651},
 acmid = {2623651},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {extreme classification, multi-label learning, ranking},
} 

@article{Karpathy2015,
    author = {Karpathy, Andrej},
    citeulike-article-id = {13776474},
    citeulike-linkout-0 = {http://cs231n.stanford.edu/syllabus.html},
    keywords = {deep\_learning\_architectures, multilayer\_networks, networks, neural, tutorials},
    posted-at = {2015-09-27 23:58:47},
    priority = {2},
    title = {{Stanford University CS231n: Convolutional Neural Networks for Visual Recognition}},
    url = {http://cs231n.stanford.edu/syllabus.html}
}

@article{Russakovsky2015,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
doi = {10.1007/s11263-015-0816-y},
volume={115},
number={3},
pages={211-252}
}

@Inbook{LeCun1999,
author="LeCun, Yann
and Haffner, Patrick
and Bottou, L{\'e}on
and Bengio, Yoshua",
title="Object Recognition with Gradient-Based Learning",
bookTitle="Shape, Contour and Grouping in Computer Vision",
year="1999",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="319--345",
isbn="978-3-540-46805-9",
doi="10.1007/3-540-46805-6_19",
url="https://doi.org/10.1007/3-540-46805-6_19"
}

@Article{Fukushima1980,
author="Fukushima, Kunihiko",
title="Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
journal="Biological Cybernetics",
year="1980",
month="Apr",
day="01",
volume="36",
number="4",
pages="193--202",
issn="1432-0770",
doi="10.1007/BF00344251",
url="https://doi.org/10.1007/BF00344251"
}

@article {Hubel1962,
author = {Hubel, D. H. and Wiesel, T. N.},
title = {Receptive fields, binocular interaction and functional architecture in the cat's visual cortex},
journal = {The Journal of Physiology},
volume = {160},
number = {1},
issn = {1469-7793},
url = {http://dx.doi.org/10.1113/jphysiol.1962.sp006837},
doi = {10.1113/jphysiol.1962.sp006837},
pages = {106--154},
year = {1962},
}

@article{WHO2001,
  title={Standardization of interpretation of chest radiographs for the diagnosis of pneumonia in children},
  author={World Health Organization and others},
  year={2001},
  publisher={Geneva: World Health Organization}
}

@inproceedings{Dembszynski2010,
  title={On label dependence in multilabel classification},
  author={Dembszynski, Krzysztof and Waegeman, Willem and Cheng, Weiwei and H{\"u}llermeier, Eyke},
  booktitle={LastCFP: ICML Workshop on Learning from Multi-label data},
  year={2010},
  organization={Ghent University, KERMIT, Department of Applied Mathematics, Biometrics and Process Control}
}


@ARTICLE{Chzhen2017,
   author = {{Chzhen}, E. and {Denis}, C. and {Hebiri}, M. and {Salmon}, J.
	},
    title = "{On the benefits of output sparsity for multi-label classification}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1703.04697},
 primaryClass = "math.ST",
 keywords = {Mathematics - Statistics Theory, Computer Science - Learning, Statistics - Machine Learning},
     year = 2017,
    month = mar,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170304697C},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Liu2017,
 author = {Liu, Jingzhou and Chang, Wei-Cheng and Wu, Yuexin and Yang, Yiming},
 title = {Deep Learning for Extreme Multi-label Text Classification},
 booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
 series = {SIGIR '17},
 year = {2017},
 isbn = {978-1-4503-5022-8},
 location = {Shinjuku, Tokyo, Japan},
 pages = {115--124},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/3077136.3080834},
 doi = {10.1145/3077136.3080834},
 acmid = {3080834},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {convolutional neural network, deep learning, extreme text classification, multi-label},
} 

@article{Yao2017,
  author    = {Li Yao and
               Eric Poblenz and
               Dmitry Dagunts and
               Ben Covington and
               Devon Bernard and
               Kevin Lyman},
  title     = {Learning to diagnose from scratch by exploiting dependencies among
               labels},
  journal   = {CoRR},
  volume    = {abs/1710.10501},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.10501},
  archivePrefix = {arXiv},
  eprint    = {1710.10501},
  timestamp = {Thu, 02 Nov 2017 14:25:36 +0100},
  biburl    = {http://dblp.org/rec/bib/journals/corr/abs-1710-10501},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Li2017,
  author    = {Yuncheng Li and
               Yale Song and
               Jiebo Luo},
  title     = {Improving Pairwise Ranking for Multi-label Image Classification},
  journal   = {CoRR},
  volume    = {abs/1704.03135},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.03135},
  archivePrefix = {arXiv},
  eprint    = {1704.03135},
  timestamp = {Wed, 07 Jun 2017 14:41:16 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/LiSL17},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}



@article{Lin2017,
  author    = {Tsung{-}Yi Lin and
               Priya Goyal and
               Ross B. Girshick and
               Kaiming He and
               Piotr Doll{\'{a}}r},
  title     = {Focal Loss for Dense Object Detection},
  journal   = {CoRR},
  volume    = {abs/1708.02002},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.02002},
  archivePrefix = {arXiv},
  eprint    = {1708.02002},
  timestamp = {Tue, 05 Sep 2017 10:03:46 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/abs-1708-02002},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Rajpurkar2017,
  author    = {Pranav Rajpurkar and
               Jeremy Irvin and
               Kaylie Zhu and
               Brandon Yang and
               Hershel Mehta and
               Tony Duan and
               Daisy Ding and
               Aarti Bagul and
               Curtis Langlotz and
               Katie Shpanskaya and
               Matthew P. Lungren and
               Andrew Y. Ng},
  title     = {CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with
               Deep Learning},
  journal   = {CoRR},
  volume    = {abs/1711.05225},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.05225},
  archivePrefix = {arXiv},
  eprint    = {1711.05225},
  timestamp = {Fri, 01 Dec 2017 14:22:24 +0100},
  biburl    = {http://dblp.org/rec/bib/journals/corr/abs-1711-05225},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}


@article{Ramachandran2017,
  author    = {Prajit Ramachandran and
               Barret Zoph and
               Quoc V. Le},
  title     = {Searching for Activation Functions},
  journal   = {CoRR},
  volume    = {abs/1710.05941},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.05941},
  archivePrefix = {arXiv},
  eprint    = {1710.05941},
  timestamp = {Wed, 01 Nov 2017 19:05:42 +0100},
  biburl    = {http://dblp.org/rec/bib/journals/corr/abs-1710-05941},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{Lazebnik2006,
 author = {Lazebnik, Svetlana and Schmid, Cordelia and Ponce, Jean},
 title = {Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories},
 booktitle = {Proceedings of the 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Volume 2},
 series = {CVPR '06},
 year = {2006},
 isbn = {0-7695-2597-0},
 pages = {2169--2178},
 numpages = {10},
 url = {http://dx.doi.org/10.1109/CVPR.2006.68},
 doi = {10.1109/CVPR.2006.68},
 acmid = {1153549},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
} 

@INPROCEEDINGS{Csurka2004,
    author = {Gabriella Csurka and Christopher R. Dance and Lixin Fan and Jutta Willamowski and Cédric Bray},
    title = {Visual categorization with bags of keypoints},
    booktitle = {In Workshop on Statistical Learning in Computer Vision, ECCV},
    year = {2004},
    pages = {1--22}
}

@article{Sanchez2013,
 author = {S\'{a}nchez, Jorge and Perronnin, Florent and Mensink, Thomas and Verbeek, Jakob},
 title = {Image Classification with the Fisher Vector: Theory and Practice},
 journal = {Int. J. Comput. Vision},
 issue_date = {December  2013},
 volume = {105},
 number = {3},
 month = dec,
 year = {2013},
 issn = {0920-5691},
 pages = {222--245},
 numpages = {24},
 url = {http://dx.doi.org/10.1007/s11263-013-0636-x},
 doi = {10.1007/s11263-013-0636-x},
 acmid = {2590173},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {Bag-of-Visual words, Fisher kernel, Fisher vector, Image classification, Large-scale classification, Product quantization},
}

@book{Goodfellow2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{Shimodaira2000,
title = "Improving predictive inference under covariate shift by weighting the log-likelihood function",
journal = "Journal of Statistical Planning and Inference",
volume = "90",
number = "2",
pages = "227 - 244",
year = "2000",
note = "",
issn = "0378-3758",
doi = "http://dx.doi.org/10.1016/S0378-3758(00)00115-4",
url = "http://www.sciencedirect.com/science/article/pii/S0378375800001154",
author = "Hidetoshi Shimodaira",
}

@inproceedings{Sutskever2013,
 author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
 title = {On the Importance of Initialization and Momentum in Deep Learning},
 booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
 series = {ICML'13},
 year = {2013},
 location = {Atlanta, GA, USA},
 pages = {III-1139--III-1147},
 url = {http://dl.acm.org/citation.cfm?id=3042817.3043064},
 acmid = {3043064},
 publisher = {JMLR.org},
} 

@article{Ioffe2015,
  author    = {Sergey Ioffe and
               Christian Szegedy},
  title     = {Batch Normalization: Accelerating Deep Network Training by Reducing
               Internal Covariate Shift},
  journal   = {CoRR},
  volume    = {abs/1502.03167},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.03167},
  timestamp = {Wed, 07 Jun 2017 14:40:49 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/IoffeS15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Hinton2012,
  author    = {Geoffrey E. Hinton and
               Nitish Srivastava and
               Alex Krizhevsky and
               Ilya Sutskever and
               Ruslan Salakhutdinov},
  title     = {Improving neural networks by preventing co-adaptation of feature detectors},
  journal   = {CoRR},
  volume    = {abs/1207.0580},
  year      = {2012},
  url       = {http://arxiv.org/abs/1207.0580},
  timestamp = {Wed, 07 Jun 2017 14:40:12 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1207-0580},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@ARTICLE{Clevert2015,
   author = {{Clevert}, D.-A. and {Unterthiner}, T. and {Hochreiter}, S.},
    title = "{Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1511.07289},
 primaryClass = "cs.LG",
 keywords = {Computer Science - Learning},
     year = 2015,
    month = nov,
   adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151107289C},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Bengio2006,
 author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
 title = {Greedy Layer-wise Training of Deep Networks},
 booktitle = {Proceedings of the 19th International Conference on Neural Information Processing Systems},
 series = {NIPS'06},
 year = {2006},
 location = {Canada},
 pages = {153--160},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=2976456.2976476},
 acmid = {2976476},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article{Hinton2006,
 author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
 title = {A Fast Learning Algorithm for Deep Belief Nets},
 journal = {Neural Comput.},
 issue_date = {July 2006},
 volume = {18},
 number = {7},
 month = jul,
 year = {2006},
 issn = {0899-7667},
 pages = {1527--1554},
 numpages = {28},
 url = {http://dx.doi.org/10.1162/neco.2006.18.7.1527},
 doi = {10.1162/neco.2006.18.7.1527},
 acmid = {1161605},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}

@article{Dauphin2014,
  author    = {Yann Dauphin and
               Razvan Pascanu and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               Kyunghyun Cho and
               Surya Ganguli and
               Yoshua Bengio},
  title     = {Identifying and attacking the saddle point problem in high-dimensional
               non-convex optimization},
  journal   = {CoRR},
  volume    = {abs/1406.2572},
  year      = {2014},
  url       = {http://arxiv.org/abs/1406.2572},
  timestamp = {Wed, 07 Jun 2017 14:40:20 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/DauphinPGCGB14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@book{Rosenblatt1957,
  url = { /reference-material/rosenblatt1957perceptron.pdf },
  publisher = { Cornell Aeronautical Laboratory },
  year = { 1957 },
  author = { Rosenblatt },
  title = { The perceptron, a perceiving and recognizing automaton, Project Para },
}

@article{Selfridge1959,
    author = {Selfridge, O. G.},
    booktitle = {H.M.S.O., London.},
    citeulike-article-id = {3703134},
    posted-at = {2013-05-05 16:40:23},
    priority = {2},
    title = {{Pandemonium: a paradigm for learning. In  The mechanisation of thought processes.}},
    year = {1959}
}

@article{Hornik1991,
title = "Approximation capabilities of multilayer feedforward networks",
journal = "Neural Networks",
volume = "4",
number = "2",
pages = "251 - 257",
year = "1991",
note = "",
issn = "0893-6080",
doi = "http://dx.doi.org/10.1016/0893-6080(91)90009-T",
url = "http://www.sciencedirect.com/science/article/pii/089360809190009T",
author = "Kurt Hornik",
keywords = "Multilayer feedforward networks",
keywords = "Activation function",
keywords = "Universal approximation capabilities",
keywords = "Input environment measure",
keywords = "() approximation",
keywords = "Uniform approximation",
keywords = "Sobolev spaces",
keywords = "Smooth approximation"
}

@article{Harzallah2009,
  title={Combining efficient object localization and image classification},
  author={Hedi Harzallah and Fr{\'e}d{\'e}ric Jurie and Cordelia Schmid},
  journal={2009 IEEE 12th International Conference on Computer Vision},
  year={2009},
  pages={237-244}
}

@inproceedings{Sanchez2011,
 author = {Sanchez, J. and Perronnin, F.},
 title = {High-dimensional Signature Compression for Large-scale Image Classification},
 booktitle = {Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition},
 series = {CVPR '11},
 year = {2011},
 isbn = {978-1-4577-0394-2},
 pages = {1665--1672},
 numpages = {8},
 url = {http://dx.doi.org/10.1109/CVPR.2011.5995504},
 doi = {10.1109/CVPR.2011.5995504},
 acmid = {2192136},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
 keywords = {scalable training algorithm, image classification, signature compression, data compression, lossy compression strategy, dimensionality reduction technique, hash kernel technique, encoding technique, product quantizers, lM Flickr images, classifier learning},
} 

@article{Ojala1996,
title = "A comparative study of texture measures with classification based on featured distributions",
journal = "Pattern Recognition",
volume = "29",
number = "1",
pages = "51 - 59",
year = "1996",
note = "",
issn = "0031-3203",
doi = "http://dx.doi.org/10.1016/0031-3203(95)00067-4",
url = "http://www.sciencedirect.com/science/article/pii/0031320395000674",
author = "Timo Ojala and Matti Pietikäinen and David Harwood",
keywords = "Texture analysis",
keywords = "Classification",
keywords = "Feature distribution",
keywords = "Brodatz textures",
keywords = "Kullback discriminant",
keywords = "Performance evaluation"
}

@inproceedings{Dalal2005,
 author = {Dalal, Navneet and Triggs, Bill},
 title = {Histograms of Oriented Gradients for Human Detection},
 booktitle = {Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Volume 1 - Volume 01},
 series = {CVPR '05},
 year = {2005},
 isbn = {0-7695-2372-2},
 pages = {886--893},
 numpages = {8},
 url = {http://dx.doi.org/10.1109/CVPR.2005.177},
 doi = {10.1109/CVPR.2005.177},
 acmid = {1069007},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}

@article{Lowe2004,
 author = {Lowe, David G.},
 title = {Distinctive Image Features from Scale-Invariant Keypoints},
 journal = {Int. J. Comput. Vision},
 issue_date = {November 2004},
 volume = {60},
 number = {2},
 month = nov,
 year = {2004},
 issn = {0920-5691},
 pages = {91--110},
 numpages = {20},
 url = {http://dx.doi.org/10.1023/B:VISI.0000029664.99615.94},
 doi = {10.1023/B:VISI.0000029664.99615.94},
 acmid = {996342},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {image matching, invariant features, object recognition, scale invariance},
} 

@incollection{Cun1990,
 author = {Cun, Y. Le and Boser, B. and Denker, J. S. and Howard, R. E. and Habbard, W. and Jackel, L. D. and Henderson, D.},
 chapter = {Handwritten Digit Recognition with a Back-propagation Network},
 title = {Advances in Neural Information Processing Systems 2},
 editor = {Touretzky, David S.},
 year = {1990},
 isbn = {1-55860-100-7},
 pages = {396--404},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=109230.109279},
 acmid = {109279},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
}

@INPROCEEDINGS{Deng2009,
    author = {Jia Deng and Wei Dong and Richard Socher and Li-jia Li and Kai Li and Li Fei-fei},
    title = {Imagenet: A large-scale hierarchical image database},
    booktitle = {In CVPR},
    year = {2009}
}

@incollection{Rumelhart1988,
 author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
 chapter = {Learning Representations by Back-propagating Errors},
 title = {Neurocomputing: Foundations of Research},
 editor = {Anderson, James A. and Rosenfeld, Edward},
 year = {1988},
 isbn = {0-262-01097-6},
 pages = {696--699},
 numpages = {4},
 url = {http://dl.acm.org/citation.cfm?id=65669.104451},
 acmid = {104451},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}

@inproceedings{Li2014,
 author = {Li, Mu and Zhang, Tong and Chen, Yuqiang and Smola, Alexander J.},
 title = {Efficient Mini-batch Training for Stochastic Optimization},
 booktitle = {Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '14},
 year = {2014},
 isbn = {978-1-4503-2956-9},
 location = {New York, New York, USA},
 pages = {661--670},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2623330.2623612},
 doi = {10.1145/2623330.2623612},
 acmid = {2623612},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {big data, distributed computing, machine learning, minibatch, stochastic gradient descent},
}

@article{Tang2013,
  author    = {Yichuan Tang},
  title     = {Deep Learning using Support Vector Machines},
  journal   = {CoRR},
  volume    = {abs/1306.0239},
  year      = {2013},
  url       = {http://arxiv.org/abs/1306.0239},
  timestamp = {Wed, 07 Jun 2017 14:41:28 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/Tang13},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Crammer2002,
 author = {Crammer, Koby and Singer, Yoram},
 title = {On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2002},
 volume = {2},
 month = mar,
 year = {2002},
 issn = {1532-4435},
 pages = {265--292},
 numpages = {28},
 url = {http://dl.acm.org/citation.cfm?id=944790.944813},
 acmid = {944813},
 publisher = {JMLR.org},
} 

@article{Lecun2015,
title = "Deep learning",
abstract = "Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.",
author = "Yann Lecun and Yoshua Bengio and Geoffrey Hinton",
year = "2015",
month = "5",
doi = "10.1038/nature14539",
volume = "521",
pages = "436--444",
journal = "Nature",
issn = "0028-0836",
publisher = "Nature Publishing Group",
number = "7553",

}


@article{Hong2014,
  author    = {Charmgil Hong and
               Iyad Batal and
               Milos Hauskrecht},
  title     = {A Mixtures-of-Experts Framework for Multi-Label Classification},
  journal   = {CoRR},
  volume    = {abs/1409.4698},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.4698},
  timestamp = {Wed, 07 Jun 2017 14:41:23 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/HongBH14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Jordan1994,
 author = {Jordan, Michael I. and Jacobs, Robert A.},
 title = {Hierarchical Mixtures of Experts and the EM Algorithm},
 journal = {Neural Comput.},
 issue_date = {March 1994},
 volume = {6},
 number = {2},
 month = mar,
 year = {1994},
 issn = {0899-7667},
 pages = {181--214},
 numpages = {34},
 url = {http://dx.doi.org/10.1162/neco.1994.6.2.181},
 doi = {10.1162/neco.1994.6.2.181},
 acmid = {188106},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}

@ARTICLE{Wang2017a,
   author = {{Wang}, H.-D. and {Zhang}, T. and {Wu}, J.},
    title = {The Monkeytyping Solution to the YouTube-8M Video Understanding Challenge},
  journal = {ArXiv e-prints},
archivePrefix = {arXiv},
   eprint = {1706.05150},
 primaryClass = {cs.CV},
 keywords = {Computer Science - Computer Vision and Pattern Recognition},
     year = {2017},
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170605150W},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@Article{Miech2017,
   author = {{Miech}, A. and {Laptev}, I. and {Sivic}, J.},
    title = {Learnable pooling with Context Gating for video classification},
  journal = {ArXiv e-prints},
archivePrefix = {arXiv},
   eprint = {1706.06905},
 primaryClass = {cs.CV},
 keywords = {Computer Science - Computer Vision and Pattern Recognition},
     year = {2017},
    month = jun,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170606905M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@Article{Szalkai2017,
   author = {{Szalkai}, B. and {Grolmusz}, V.},
    title = {Near Perfect Protein Multi-Label Classification with Deep Neural Networks},
  journal = {ArXiv e-prints},
archivePrefix = {arXiv},
   eprint = {1703.10663},
 primaryClass = {q-bio.BM},
 keywords = {Quantitative Biology - Biomolecules, Computer Science - Learning, Statistics - Machine Learning},
     year = {2017},
    month = mar,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170310663S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Oquab2015,
  TITLE = {{Is object localization for free? -- Weakly-supervised learning with convolutional neural networks}},
  AUTHOR = {Oquab, Maxime and Bottou, L{\'e}on and Laptev, Ivan and Sivic, Josef},
  URL = {https://hal.inria.fr/hal-01015140},
  BOOKTITLE = {{IEEE Conference on Computer Vision and Pattern Recognition}},
  ADDRESS = {Boston, United States},
  YEAR = {2015},
  MONTH = Jun,
  PDF = {https://hal.inria.fr/hal-01015140/file/Oquab15.pdf},
  HAL_ID = {hal-01015140},
  HAL_VERSION = {v2},
}

@InProceedings{Wang2017,
author    = {Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi 
and Summers, Ronald},
title     = {ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases},
booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages     = {3462--3471},
year      = {2017}
}

@article{Martins2016,
  author    = {Andr{\'{e}} F. T. Martins and
               Ram{\'{o}}n Fern{\'{a}}ndez Astudillo},
  title     = {From Softmax to Sparsemax: {A} Sparse Model of Attention and Multi-Label
               Classification},
  journal   = {CoRR},
  volume    = {abs/1602.02068},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.02068},
  timestamp = {Wed, 07 Jun 2017 14:40:30 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/MartinsA16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Grzeszick2016,
  author    = {Rene Grzeszick and
               Sebastian Sudholt and
               Gernot A. Fink},
  title     = {Optimistic and Pessimistic Neural Networks for Scene and Object Recognition},
  journal   = {CoRR},
  volume    = {abs/1609.07982},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.07982},
  timestamp = {Wed, 07 Jun 2017 14:42:59 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/GrzeszickSF16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Zhu2017a,
 author = {Zhu, Jianqing and Liao, Shengcai and Lei, Zhen and Li, Stan Z.},
 title = {Multi-label Convolutional Neural Network Based Pedestrian Attribute Classification},
 journal = {Image Vision Comput.},
 issue_date = {February 2017},
 volume = {58},
 number = {C},
 month = feb,
 year = {2017},
 issn = {0262-8856},
 pages = {224--229},
 numpages = {6},
 url = {https://doi.org/10.1016/j.imavis.2016.07.004},
 doi = {10.1016/j.imavis.2016.07.004},
 acmid = {3063831},
 publisher = {Butterworth-Heinemann},
 address = {Newton, MA, USA},
 keywords = {Convolutional neural network, Multi-label classification, Pedestrian attribute classification},
}

@article{Hu2015,
  author    = {Hexiang Hu and
               Guang{-}Tong Zhou and
               Zhiwei Deng and
               Zicheng Liao and
               Greg Mori},
  title     = {Learning Structured Inference Neural Networks with Label Relations},
  journal   = {CoRR},
  volume    = {abs/1511.05616},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.05616},
  timestamp = {Wed, 07 Jun 2017 14:41:16 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/HuZDLM15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Nam2013,
  author    = {Jinseok Nam and
               Jungi Kim and
               Iryna Gurevych and
               Johannes F{\"{u}}rnkranz},
  title     = {Large-scale Multi-label Text Classification - Revisiting Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1312.5419},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.5419},
  timestamp = {Wed, 07 Jun 2017 14:41:51 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/NamKGF13},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Jin2016,
  author    = {Jiren Jin and
               Hideki Nakayama},
  title     = {Annotation Order Matters: Recurrent Image Annotator for Arbitrary
               Length Image Tagging},
  journal   = {CoRR},
  volume    = {abs/1604.05225},
  year      = {2016},
  url       = {http://arxiv.org/abs/1604.05225},
  timestamp = {Wed, 07 Jun 2017 14:42:03 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/JinN16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Liu2016,
  author    = {Feng Liu and
               Tao Xiang and
               Timothy M. Hospedales and
               Wankou Yang and
               Changyin Sun},
  title     = {Semantic Regularisation for Recurrent Image Annotation},
  journal   = {CoRR},
  volume    = {abs/1611.05490},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.05490},
  timestamp = {Wed, 07 Jun 2017 14:42:14 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/LiuXHYS16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Chen2017,
   author = {{Chen}, S.-F. and {Chen}, Y.-C. and {Yeh}, C.-K. and {Wang}, Y.-C.~F.
	},
    title = {Order-Free RNN with Visual Attention for Multi-Label Classification},
  journal = {ArXiv e-prints},
archivePrefix = {arXiv},
   eprint = {1707.05495},
 primaryClass = {cs.CV},
 keywords = {Computer Science - Computer Vision and Pattern Recognition},
     year = {2017},
    month = jul,
   adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170705495C},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Weston2011,
 author = {Weston, Jason and Bengio, Samy and Usunier, Nicolas},
 title = {WSABIE: Scaling Up to Large Vocabulary Image Annotation},
 booktitle = {Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence - Volume Volume Three},
 series = {IJCAI'11},
 year = {2011},
 isbn = {978-1-57735-515-1},
 location = {Barcelona, Catalonia, Spain},
 pages = {2764--2770},
 numpages = {7},
 url = {http://dx.doi.org/10.5591/978-1-57735-516-8/IJCAI11-460},
 doi = {10.5591/978-1-57735-516-8/IJCAI11-460},
 acmid = {2283856},
 publisher = {AAAI Press},
}

@article{Gong2012,
  author    = {Yunchao Gong and
               Qifa Ke and
               Michael Isard and
               Svetlana Lazebnik},
  title     = {A Multi-View Embedding Space for Modeling Internet Images, Tags, and
               their Semantics},
  journal   = {CoRR},
  volume    = {abs/1212.4522},
  year      = {2012},
  url       = {http://arxiv.org/abs/1212.4522},
  timestamp = {Wed, 07 Jun 2017 14:41:15 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1212-4522},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{Xue2011,
 author = {Xiangyang Xue and Wei Zhang and Jie Zhang and Bin Wu and Jianping Fan and Yao Lu},
 title = {Correlative Multi-label Multi-instance Image Annotation},
 booktitle = {Proceedings of the 2011 International Conference on Computer Vision},
 series = {ICCV '11},
 year = {2011},
 isbn = {978-1-4577-1101-5},
 pages = {651--658},
 numpages = {8},
 url = {http://dx.doi.org/10.1109/ICCV.2011.6126300},
 doi = {10.1109/ICCV.2011.6126300},
 acmid = {2356568},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}

@article{Girshick2013,
  author    = {Ross B. Girshick and
               Jeff Donahue and
               Trevor Darrell and
               Jitendra Malik},
  title     = {Rich feature hierarchies for accurate object detection and semantic
               segmentation},
  journal   = {CoRR},
  volume    = {abs/1311.2524},
  year      = {2013},
  url       = {http://arxiv.org/abs/1311.2524},
  timestamp = {Wed, 07 Jun 2017 14:43:00 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/GirshickDDM13},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{Oquab2014,
 author = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
 title = {Learning and Transferring Mid-level Image Representations Using Convolutional Neural Networks},
 booktitle = {Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition},
 series = {CVPR '14},
 year = {2014},
 isbn = {978-1-4799-5118-5},
 pages = {1717--1724},
 numpages = {8},
 url = {http://dx.doi.org/10.1109/CVPR.2014.222},
 doi = {10.1109/CVPR.2014.222},
 acmid = {2680210},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}

@article{Sermanet2013,
  author    = {Pierre Sermanet and
               David Eigen and
               Xiang Zhang and
               Micha{\"{e}}l Mathieu and
               Rob Fergus and
               Yann LeCun},
  title     = {OverFeat: Integrated Recognition, Localization and Detection using
               Convolutional Networks},
  journal   = {CoRR},
  volume    = {abs/1312.6229},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.6229},
  timestamp = {Wed, 07 Jun 2017 14:42:22 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SermanetEZMFL13},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Razavian2014,
  author    = {Ali Sharif Razavian and
               Hossein Azizpour and
               Josephine Sullivan and
               Stefan Carlsson},
  title     = {{CNN} Features off-the-shelf: an Astounding Baseline for Recognition},
  journal   = {CoRR},
  volume    = {abs/1403.6382},
  year      = {2014},
  url       = {http://arxiv.org/abs/1403.6382},
  timestamp = {Wed, 07 Jun 2017 14:40:09 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/RazavianASC14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Gong2013,
  author    = {Yunchao Gong and
               Yangqing Jia and
               Thomas Leung and
               Alexander Toshev and
               Sergey Ioffe},
  title     = {Deep Convolutional Ranking for Multilabel Image Annotation},
  journal   = {CoRR},
  volume    = {abs/1312.4894},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.4894},
  timestamp = {Wed, 07 Jun 2017 14:40:25 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/GongJLTI13},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{Li2016,
  author = {Li, Yining and Huang, Chen and Loy, Chen Change and Tang, Xiaoou},
  title = {Human Attribute Recognition by Deep Hierarchical Contexts},
  booktitle = {European Conference on Computer Vision},
  year = {2016}
}

@article{Lin2014,
  author    = {Tsung{-}Yi Lin and
               Michael Maire and
               Serge J. Belongie and
               Lubomir D. Bourdev and
               Ross B. Girshick and
               James Hays and
               Pietro Perona and
               Deva Ramanan and
               Piotr Doll{\'{a}}r and
               C. Lawrence Zitnick},
  title     = {Microsoft {COCO:} Common Objects in Context},
  journal   = {CoRR},
  volume    = {abs/1405.0312},
  year      = {2014},
  url       = {http://arxiv.org/abs/1405.0312},
  timestamp = {Wed, 07 Jun 2017 14:41:35 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/LinMBHPRDZ14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@misc{Everingham2012,
	author = {Everingham, Mark and Gool, Luc and Williams, Christopher K. and Winn, John and Zisserman, Andrew},
	title = {The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2012 {(VOC2012)} {R}esults},
	year = {2012},
	howpublished = "http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html"}

@article{Everingham2010,
 author = {Everingham, Mark and Gool, Luc and Williams, Christopher K. and Winn, John and Zisserman, Andrew},
 title = {The Pascal Visual Object Classes (VOC) Challenge},
 journal = {IJCV},
 issue_date = {June 2010},
 volume = {88},
 number = {2},
 month = jun,
 year = {2010},
 issn = {0920-5691},
 pages = {303--338},
 numpages = {36},
 url = {http://dx.doi.org/10.1007/s11263-009-0275-4},
 doi = {10.1007/s11263-009-0275-4},
 acmid = {1747104},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {Benchmark, Database, Object detection, Object recognition},
} 

@inproceedings{Chua2009,
 author = {Chua, Tat-Seng and Tang, Jinhui and Hong, Richang and Li, Haojie and Luo, Zhiping and Zheng, Yantao},
 title = {NUS-WIDE: A Real-world Web Image Database from National University of Singapore},
 booktitle = {Proceedings of the ACM International Conference on Image and Video Retrieval},
 series = {CIVR '09},
 year = {2009},
 isbn = {978-1-60558-480-5},
 location = {Santorini, Fira, Greece},
 pages = {48:1--48:9},
 articleno = {48},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1646396.1646452},
 doi = {10.1145/1646396.1646452},
 acmid = {1646452},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Flickr, annotation, retrieval, tag refinement, training set construction, web image},
}

@article{Wei2014,
  author    = {Yunchao Wei and
               Wei Xia and
               Junshi Huang and
               Bingbing Ni and
               Jian Dong and
               Yao Zhao and
               Shuicheng Yan},
  title     = {{CNN:} Single-label to Multi-label},
  journal   = {CoRR},
  volume    = {abs/1406.5726},
  year      = {2014},
  url       = {http://arxiv.org/abs/1406.5726},
  timestamp = {Wed, 07 Jun 2017 14:43:14 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/WeiXHNDZY14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{Krizhevsky2012,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems},
 series = {NIPS'12},
 year = {2012},
 location = {Lake Tahoe, Nevada},
 pages = {1097--1105},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2999134.2999257},
 acmid = {2999257},
 publisher = {Curran Associates Inc.},
 address = {USA},
}

@Article{Esteva2017,
author={Esteva, Andre
and Kuprel, Brett
and Novoa, Roberto A.
and Ko, Justin
and Swetter, Susan M.
and Blau, Helen M.
and Thrun, Sebastian},
title={Dermatologist-level classification of skin cancer with deep neural networks},
journal={Nature},
year={2017},
month={Feb},
day={02},
publisher={Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
volume={542},
number={7639},
pages={115-118},
note={Letter},
issn={0028-0836},
url={http://dx.doi.org/10.1038/nature21056}
}

@book{Hastie2009,
  added-at = {2010-06-03T15:15:09.000+0200},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  biburl = {https://www.bibsonomy.org/bibtex/200d858c0bd2826d4eb5f39450192d1f5/ukoethe},
  edition = 2,
  file = {:Books\\HastieTibshiraniFriedman-09-Elements-of-Statistical-Learning-2nd-edition\\hastie_09_elements-of.statistical-learning.pdf:PDF},
  interhash = {52d1772f39be836e3b298d37b8c0cfa1},
  intrahash = {00d858c0bd2826d4eb5f39450192d1f5},
  keywords = {inference mathmatics dataanalysis method clutering statistics},
  publisher = {Springer},
  timestamp = {2010-06-03T15:15:09.000+0200},
  title = {The elements of statistical learning: data mining, inference and prediction},
  url = {http://www-stat.stanford.edu/~tibs/ElemStatLearn/},
  year = 2009
}

@Article{Abu-El-Haija2016,
  Title                    = {{YouTube-8M: A Large-Scale Video Classification Benchmark}},
  Author                   = {Abu-El-Haija, Sami and Kothari, Nisarg and Lee, Joonseok and Natsev, Paul and Toderici, George and Varadarajan, Balakrishnan and Vijayanarasimhan, Sudheendra},
  Journal                  = {arXiv},
  Year                     = {2016},

  Abstract                 = {Many recent advancements in Computer Vision are attributed to large datasets. Open-source software packages for Machine Learning and inexpensive commodity hardware have reduced the barrier of entry for exploring novel approaches at scale. It is possible to train models over millions of examples within a few days. Although large-scale datasets exist for image understanding, such as ImageNet, there are no comparable size video classification datasets. In this paper, we introduce YouTube-8M, the largest multi-label video classification dataset, composed of {\~{}}8 million videos (500K hours of video), annotated with a vocabulary of 4800 visual entities. To get the videos and their labels, we used a YouTube video annotation system, which labels videos with their main topics. While the labels are machine-generated, they have high-precision and are derived from a variety of human-based signals including metadata and query click signals. We filtered the video labels (Knowledge Graph entities) using both automated and manual curation strategies, including asking human raters if the labels are visually recognizable. Then, we decoded each video at one-frame-per-second, and used a Deep CNN pre-trained on ImageNet to extract the hidden representation immediately prior to the classification layer. Finally, we compressed the frame features and make both the features and video-level labels available for download. We trained various (modest) classification models on the dataset, evaluated them using popular evaluation metrics, and report them as baselines. Despite the size of the dataset, some of our models train to convergence in less than a day on a single machine using TensorFlow. We plan to release code for training a TensorFlow model and for computing metrics.},
  Annote                   = {NULL},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1609.08675},
  Eprint                   = {1609.08675},
  File                     = {:home/jan/Documents/Mendeley Desktop/Abu-El-Haija et al/arXiv/Abu-El-Haija et al. - Unknown - YouTube-8M A Large-Scale Video Classification Benchmark.pdf:pdf},
  Url                      = {https://arxiv.org/pdf/1609.08675.pdf http://arxiv.org/abs/1609.08675}
}

@Article{Alazaidah2016,
  Title                    = {{Trending Challenges in Multi Label Classification}},
  Author                   = {Alazaidah, Raed and Ahmad, Farzana Kabir},
  Journal                  = {IJACSA) International Journal of Advanced Computer Science and Applications},
  Year                     = {2016},
  Number                   = {10},
  Volume                   = {7},

  Abstract                 = {—Multi label classification has become a very important paradigm in the last few years because of the increasing domains that it can be applied to. Many researchers have developed many algorithms to solve the problem of multi label classification. Nerveless, there are still some stuck problems that need to be investigated in depth. The aim of this paper is to provide researchers with a brief introduction to the problem of multi label classification, and introduce some of the most trending challenges.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Alazaidah, Ahmad/IJACSA) International Journal of Advanced Computer Science and Applications/Alazaidah, Ahmad - 2016 - Trending Challenges in Multi Label Classification.pdf:pdf},
  Keywords                 = {Correlations among labels,Multi Label Classification,—Challenges},
  Url                      = {www.ijacsa.thesai.org}
}

@Article{Alazaidah2015,
  Title                    = {{A Multi-Label Classification Approach Based on Correlations Among Labels}},
  Author                   = {Alazaidah, Raed and Thabtah, Fadi and Al-Radaideh, Qasem},
  Journal                  = {IJACSA) International Journal of Advanced Computer Science and Applications},
  Year                     = {2015},
  Number                   = {2},
  Volume                   = {6},

  Abstract                 = {—Multi label classification is concerned with learning from a set of instances that are associated with a set of labels, that is, an instance could be associated with multiple labels at the same time. This task occurs frequently in application areas like text categorization, multimedia classification, bioinformatics, protein function classification and semantic scene classification. Current multi-label classification methods could be divided into two categories. The first is called problem transformation methods, which transform multi-label classification problem into single label classification problem, and then apply any single label classifier to solve the problem. The second category is called algorithm adaptation methods, which adapt an existing single label classification algorithm to handle multi-label data. In this paper, we propose a multi-label classification approach based on correlations among labels that use both problem transformation methods and algorithm adaptation methods. The approach begins with transforming multi-label dataset into a single label dataset using least frequent label criteria, and then applies the PART algorithm on the transformed dataset. The output of the approach is multi-labels rules. The approach also tries to get benefit from positive correlations among labels using predictive Apriori algorithm. The proposed approach has been evaluated using two multi-label datasets named (Emotions and Yeast) and three evaluation measures (Accuracy, Hamming Loss, and Harmonic Mean). The experiments showed that the proposed approach has a fair accuracy in comparison to other related methods.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Alazaidah, Thabtah, Al-Radaideh/IJACSA) International Journal of Advanced Computer Science and Applications/Alazaidah, Thabtah, Al-Radaideh - 2015 - A Multi-Label Classification Approach Based on Correlations Among Labels.pdf:pdf},
  Keywords                 = {Data mining,Multi-label Classification,—Classification},
  Url                      = {www.ijacsa.thesai.org}
}

@Article{Al-Salemi2015,
  Title                    = {{Boosting algorithms with topic modeling for multi-label text categorization: A comparative empirical study}},
  Author                   = {Al-Salemi, B and Aziz, MJ Ab and Noah, SA},
  Journal                  = {Journal of Information Science},
  Year                     = {2015},

  Doi                      = {10.1177/0165551515590079},
  File                     = {:home/jan/Documents/Mendeley Desktop/Al-Salemi, Aziz, Noah/Journal of Information Science/Al-Salemi, Aziz, Noah - 2015 - Boosting algorithms with topic modeling for multi-label text categorization A comparative empirical study.pdf:pdf},
  ISSN                     = {17416485},
  Keywords                 = {Multi-Label,Text Categorisation,adaboost,boosting,mh,multi-label classification,text categorization,text representation,topic modeling},
  Mendeley-tags            = {Multi-Label,Text Categorisation},
  Url                      = {http://jis.sagepub.com/content/early/2015/06/30/0165551515590079.abstract}
}

@Article{Berger2014,
  Title                    = {{Large Scale Multi-label Text Classification with Semantic Word Vectors}},
  Author                   = {Berger, Mark J},
  Journal                  = {Technical Report},
  Year                     = {2014},
  Pages                    = {1--8},

  Abstract                 = {Multi-label text classification has been applied to a multitude of tasks, including document indexing, tag suggestion, and sentiment classification. However, many of these methods disregard word order, opting to use bag-of-words models or TF- IDF weighting to create document vectors. With the advent of powerful semantic embeddings, such as word2vec and GloVe, we explore how word embeddings and word order can be used to improve multi-label learning. Specifically, we explore how both a convolutional neural network (CNN) and a recurrent network with a gated recurrent unit (GRU) can independently be used with pre-trained word2vec embeddings to solve a large scale multi-label text classification problem. On a data set of over two million documents and 1000 potential labels, we demonstrate that both a CNN and a GRU provide substantial improvement over a Binary Relevance model with a bag-of-words representation.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Berger/Technical Report/Berger - 2014 - Large Scale Multi-label Text Classification with Semantic Word Vectors.pdf:pdf},
  Keywords                 = {Multi-Label,Text Classification},
  Mendeley-tags            = {Multi-Label,Text Classification}
}

@Article{Bhowmick2009,
  Title                    = {{Reader Perspective Emotion Analysis in Text through Ensemble based Multi-Label Classification Framework}},
  Author                   = {Bhowmick, Plaban Kumar and Basu, Anupam and Mitra, Pabitra},
  Journal                  = {Computer and Information Science},
  Year                     = {2009},
  Number                   = {4},
  Pages                    = {64--74},
  Volume                   = {2},

  Abstract                 = {Multiple emotions are often triggered in readers in response to text stimuli like news article. In this paper, we present a novel method for classifying news sentences into multiple emotion categories using an ensemble based multi-label classification technique called RAKEL. The emotion data consists of 1305 news sentences and the emotion classes considered are disgust, fear, happiness and sadness. Words are the most obvious choice as feature for emotion recognition. In addition to that we have introduced two novel feature sets: polarity of subject, verb and object of the sentences and semantic frames. Experiments concerning the comparison of features revealed that semantic frame feature combined with polarity based feature performs best in emotion classification. Experiments on feature selection over word and semantic frame features have been performed in order to handle feature sparseness problem. In both word and semantic frame feature, improvements in the overall performance have been observed after optimal feature selection.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Bhowmick, Basu, Mitra/Computer and Information Science/Bhowmick, Basu, Mitra - 2009 - Reader Perspective Emotion Analysis in Text through Ensemble based Multi-Label Classification Framework.pdf:pdf},
  ISSN                     = {19138997},
  Keywords                 = {Emotion Analysis,Multi-Label,emotion classification,ensemble classifier,feature selection,multi-label classification},
  Mendeley-tags            = {Emotion Analysis,Multi-Label},
  Url                      = {http://ccsenet.org/journal/index.php/cis/article/view/3872/0}
}

@Article{Boutell2004,
  Title                    = {{Learning multi-label scene classification}},
  Author                   = {Boutell, Matthew R. and Luo, Jiebo and Shen, Xipeng and Brown, Christopher M.},
  Journal                  = {Pattern Recognition},
  Year                     = {2004},
  Number                   = {9},
  Pages                    = {1757--1771},
  Volume                   = {37},

  Abstract                 = {In classic pattern recognition problems, classes are mutually exclusive by definition. Classification errors occur when the classes overlap in the feature space. We examine a different situation, occurring when the classes are, by definition, not mutually exclusive. Such problems arise in semantic scene and document classification and in medical diagnosis. We present a framework to handle such problems and apply it to the problem of semantic scene classification, where a natural scene may contain multiple objects such that the scene can be described by multiple class labels (e.g., a field scene with a mountain in the background). Such a problem poses challenges to the classic pattern recognition paradigm and demands a different treatment. We discuss approaches for training and testing in this scenario and introduce new metrics for evaluating individual examples, class recall and precision, and overall accuracy. Experiments show that our methods are suitable for scene classification; furthermore, our work appears to generalize to other classification problems of the same nature. {\textcopyright} 2004 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.},
  Doi                      = {10.1016/j.patcog.2004.03.009},
  File                     = {:home/jan/Documents/Mendeley Desktop/Boutell et al/Pattern Recognition/Boutell et al. - 2004 - Learning multi-label scene classi{\"{y}}cation.pdf:pdf},
  ISBN                     = {0031-3203},
  ISSN                     = {00313203},
  Keywords                 = {Cross-training,Image organization,Image understanding,Jaccard similarity,Multi-label classification,Multi-label evaluation,Multi-label training,Semantic scene classification}
}

@Article{Boutell2004a,
  Title                    = {{Learning multi-label scene classi{\"{y}}cation}},
  Author                   = {Boutell, Matthew R and Luo, Jiebo and Shen, Xipeng and Brown, Christopher M},
  Journal                  = {Pattern Recognition},
  Year                     = {2004},
  Pages                    = {1757--1771},
  Volume                   = {37},

  Abstract                 = {In classic pattern recognition problems, classes are mutually exclusive by de{\"{y}}nition. Classi{\"{y}}cation errors occur when the classes overlap in the feature space. We examine a diierent situation, occurring when the classes are, by de{\"{y}}nition, not mutually exclusive. Such problems arise in semantic scene and document classi{\"{y}}cation and in medical diagnosis. We present a framework to handle such problems and apply it to the problem of semantic scene classi{\"{y}}cation, where a natural scene may contain multiple objects such that the scene can be described by multiple class labels (e.g., a {\"{y}}eld scene with a mountain in the background). Such a problem poses challenges to the classic pattern recognition paradigm and demands a diierent treatment. We discuss approaches for training and testing in this scenario and introduce new metrics for evaluating individual examples, class recall and precision, and overall accuracy. Experiments show that our methods are suitable for scene classi{\"{y}}cation; furthermore, our work appears to generalize to other classi{\"{y}}cation problems of the same nature.},
  Doi                      = {10.1016/j.patcog.2004.03.009},
  File                     = {:home/jan/Documents/Mendeley Desktop/Boutell et al/Pattern Recognition/Boutell et al. - 2004 - Learning multi-label scene classi{\"{y}}cation.pdf:pdf},
  Keywords                 = {Cross-training,Image organization,Image understanding,Jaccard similarity,Multi-label classi{\"{y}}cation,Multi-label evaluation,Multi-label training,Semantic scene classi{\"{y}}cation},
  Url                      = {www.elsevier.com/locate/patcog}
}

@Article{DeCarvalho2009,
  Title                    = {{A tutorial on multi-label classification techniques}},
  Author                   = {de Carvalho, Andr{\'{e}} C. P. L. F. and Freitas, Alex A.},
  Journal                  = {Studies in Computational Intelligence},
  Year                     = {2009},
  Pages                    = {177--195},
  Volume                   = {205},

  Abstract                 = {Industries have to design and produce performing and reliable systems. Nevertheless, designers suffer from the diversity of methods, which are not really adequate to their needs. Authors highlight the need of close interactions between product and project design, often treated either independently or sequentially, necessary to improve system design, and logistics in this context. Strengthening the links between product design and project management processes is an ongoing challenge, and this situation relies on perfect control of methods, tools and know-how, both on the technical side as well as on the organizational side. The aim of our work is to facilitate the project manager's decision making, thus allowing him to define, follow and adapt a working plan, while still considering various organizational options. From these options, the project manager chooses the scheme that best encompasses the project's objectives with respect to costs, delay and risks, without neglecting performance and safety. To encourage the project manager to explore various possibilities, we developed and tested a heuristic based on ant colony optimization and evolutionary algorithm adapted for multi-objective problems. Its hybridization with a tabu search and a greedy algorithm were performed in order to accelerate convergence of the research study and to reduce the cost engendered by the evaluation process. The experiments carried out reveals that it was possible to offer the decision maker a reduced number of solutions that he can evaluate more accurately in order to choose one according to technical, economic and financial criteria. {\textcopyright} 2009 Springer-Verlag Berlin Heidelberg.},
  Doi                      = {10.1007/978-3-642-01536-6_8},
  ISBN                     = {9783642015359},
  ISSN                     = {1860949X},
  Url                      = {http://link.springer.com/10.1007/978-3-642-01536-6{\_}8}
}

@Article{CarvalhoAndreCPLFde2009,
  Title                    = {{A Tutoria on Multi-Label Classification Techniques}},
  Author                   = {{Carvalho, Andr{\'{e}} C P L F de}, Alex A. Freitas},
  Journal                  = {Foundations of Computational Intelligence},
  Year                     = {2009},
  Pages                    = {177--195},
  Volume                   = {5},

  Abstract                 = {Most classification problems associate a single class to each example or instance. However, there are many classification tasks where each instance can be associated with one or more classes. This group of problems represents an area known as multi-label classification. One typical example of multi-label classification problems is the classification of documents, where each document can be assigned to more than one class. This tutorial presents the most frequently used techniques to deal with these problems in a pedagogical manner, with examples illustrating the main techniques and proposing a taxonomy of multi-label techniques that highlights the similarities and differences between these techniques.},
  Doi                      = {10.1007/978-3-642-01536-6_8},
  File                     = {:home/jan/Documents/Mendeley Desktop/Carvalho, Andr{\'{e}} C P L F de/Foundations of Computational Intelligence/C, De Carvalho, Freitas - Unknown - A Tutorial on Multi-Label Classification Techniques.pdf:pdf},
  ISBN                     = {978-3-642-01535-9},
  Url                      = {http://www.icmc.usp.br/{~}andre http://www.cs.kent.ac.uk/{~}aaf http://www.cs.kent.ac.uk/people/staff/aaf/pub{\_}papers.dir/Found-Comp-Intel-bk-ch-2009-Carvalho.pdf}
}

@Article{Charte2016,
  Title                    = {{Working with Multilabel Datasets in {\{}R{\}}: The mldr Package}},
  Author                   = {Charte, Francisco and Charte, David},
  Journal                  = {The R Journal},
  Year                     = {2015},
  Number                   = {2},
  Pages                    = {149--162},
  Volume                   = {7},

  Abstract                 = {Most classification algorithms deal with datasets which have a set of input features, the variables to be used as predictors, and only one output class, the variable to be predicted. However, in late years many scenarios in which the classifier has to work with several outputs have come to life. Automatic labeling of text documents, image annotation or protein classification are among them. Multilabel datasets are the product of these new needs, and they have many specific traits. The mldr package allows the user to load datasets of this kind, obtain their characteristics, produce specialized plots, and manipulate them. The goal is to provide the exploratory tools needed to analyze multilabel datasets, as well as the transformation and manipulation functions that will make possible to apply binary and multiclass classification models to this data or the development of new multilabel classifiers. Thanks to its integrated user interface, the exploratory functions will be available even to non-specialized R users.},
  Doi                      = {10.6084/m9.figshare.1356035},
  File                     = {:home/jan/Documents/Mendeley Desktop/Charte, Charte/The R Journal/Charte, Charte - 2015 - Working with Multilabel Datasets in {\{}R{\}} The mldr Package.pdf:pdf},
  ISSN                     = {20734859},
  Url                      = {https://cran.r-project.org/web/packages/mldr/vignettes/mldr.pdf http://journal.r-project.org/archive/2015-2/charte-charte.pdf}
}

@InProceedings{Charte,
  Title                    = {{Concurrence among imbalanced labels and its influence on multilabel resampling algorithms}},
  Author                   = {Charte, Francisco and Rivera, Antonio and {Del Jesus}, Mar{\'{i}}a Jos{\'{e}} and Herrera, Francisco},
  Booktitle                = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  Year                     = {2014},
  Pages                    = {110--121},
  Volume                   = {8480 LNAI},

  Abstract                 = {See, stats, and : https:// www. researchgate. net / publication / 269140989 Concurrence Influence Conference DOI : 10 . 1007 / 978 - 3 - 319 - 07617 - 1{\_}10 CITATIONS 6 READS 55 4 : Some : Fuzz - IEEE (SS - 11) View FUZZ - IEEE F . Charte Universidad 48 SEE Antonio Universidad 59 SEE Mar{\'{i}}a Universidad 162 , 156 SEE Francisco University 887 , 974 SEE All . Charte . The . All - text and , letting . Abstract . In the context of multilabel classification , the learning from imbalanced data is getting considerable attention recently . Several algo - rithms to face this problem have been proposed in the late five years , as well as various measures to assess the imbalance level . Some of the pro - posed methods are based on resampling techniques , a very well - known approach whose utility in traditional classification has been proven . This paper aims to describe how a specific characteristic of multilabel da - tasets (MLDs) , the level of concurrence among imbalanced labels , could have a great impact in resampling algorithms behavior . Towards this goal , a measure named SCUMBLE , designed to evaluate this concurrence level , is proposed and its usefulness is experimentally tested . As a result , a straightforward guideline on the effectiveness of multilabel resampling algorithms depending on MLDs characteristics can be inferred .},
  Doi                      = {10.1007/978-3-319-07617-1_10},
  File                     = {:home/jan/Documents/Mendeley Desktop/Charte et al/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Charte et al. - 2014 - Concurrence among imbalanced labels and its influence on multilabel resampling algorithms.pdf:pdf},
  ISBN                     = {9783319076164},
  ISSN                     = {16113349},
  Keywords                 = {Imbalanced Learning,Measures,Multilabel Classification,Resampling},
  Url                      = {https://www.researchgate.net/profile/F{\_}Charte/publication/269140989{\_}Concurrence{\_}among{\_}Imbalanced{\_}Labels{\_}and{\_}Its{\_}Influence{\_}on{\_}Multilabel{\_}Resampling{\_}Algorithms/links/566551ad08ae15e74633c4d1.pdf}
}

@InProceedings{Charte2013,
  Title                    = {{A first approach to deal with imbalance in multi-label datasets}},
  Author                   = {Charte, Francisco and Rivera, Antonio and {Del Jesus}, Mar{\'{i}}a Jos{\'{e}} and Herrera, Francisco},
  Booktitle                = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  Year                     = {2013},
  Pages                    = {150--160},
  Volume                   = {8073 LNAI},

  Abstract                 = {See, stats, and : https :// www. researchgate. net / publication / 269100244 A Multi-label Conference DOI : 10 . 1007 / 978 - 3 - 642 - 40846 - 5{\_}16 CITATIONS 13 READS 236 4 : Some : FUZZ - IEEE Imbalanced F . Charte Universidad 48 SEE Antonio Universidad 59 SEE Mar{\'{i}}a Universidad 162 , 156 SEE Francisco University 887 , 974 SEE All . Charte . The . All - text and , letting . Abstract . The process of learning from imbalanced datasets has been deeply studied for binary and multi - class classification . This problem also affects to multi - label datasets . Actually , the imbalance level in multi - label datasets uses to be much larger than in binary or multi - class datasets . Notwithstanding , the proposals on how to measure and deal with imbal - anced datasets in multi - label classification are scarce . In this paper , we introduce two measures aimed to obtain information about the imbalance level in multi - label datasets . Furthermore , two pre - processing methods designed to reduce the imbalance level in multi - label datasets are proposed , and their effectiveness is validated experimentally . Finally , an analysis for determining when these methods have to be ap - plied depending on the dataset characteristics is provided .},
  Doi                      = {10.1007/978-3-642-40846-5_16},
  File                     = {:home/jan/Documents/Mendeley Desktop/Charte et al/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Charte et al. - 2013 - A first approach to deal with imbalance in multi-label datasets.pdf:pdf},
  ISBN                     = {9783642408458},
  ISSN                     = {03029743},
  Keywords                 = {Imbalanced Datasets,Measures,Multi-label Classification,Preprocessing},
  Url                      = {https://www.researchgate.net/profile/F{\_}Charte/publication/269100244{\_}A{\_}First{\_}Approach{\_}to{\_}Deal{\_}with{\_}Imbalance{\_}in{\_}Multi-label{\_}Datasets/links/5493eacb0cf286fe3126a331.pdf}
}

@Article{Charte2015,
  Title                    = {{Addressing imbalance in multilabel classification: Measures and random resampling algorithms}},
  Author                   = {Charte, Francisco and Rivera, Antonio J and del Jesus, Mar{\'{i}}a J. and Herrera, Francisco},
  Journal                  = {Neurocomputing},
  Year                     = {2015},
  Pages                    = {1--14},
  Volume                   = {163},

  Abstract                 = {The purpose of this paper is to analyze the imbalanced learning task in the multilabel scenario, aiming to accomplish two different goals. The first one is to present specialized measures directed to assess the imbalance level in multilabel datasets (MLDs). Using these measures we will be able to conclude which MLDs are imbalanced, and therefore would need an appropriate treatment. The second objective is to propose several algorithms designed to reduce the imbalance in MLDs in a classifier-independent way, by means of resampling techniques. Two different approaches to divide the instances in minority and majority groups are studied. One of them considers each label combination as class identifier, whereas the other one performs an individual evaluation of each label imbalance level. A random undersampling and a random oversampling algorithm are proposed for each approach, giving as result four different algorithms. All of them are experimentally tested and their effectiveness is statistically evaluated. From the results obtained, a set of guidelines directed to show when these methods should be applied is also provided.},
  Doi                      = {10.1016/j.neucom.2014.08.091},
  File                     = {:home/jan/Documents/Mendeley Desktop/Charte et al/Neurocomputing/Charte et al. - 2015 - Addressing imbalance in multilabel classification Measures and random resampling algorithms.pdf:pdf},
  ISSN                     = {09252312},
  Keywords                 = {Imbalanced classification,Multilabel classification,Oversampling,Resampling algorithms,Undersampling,imbalanced classi fi cation,multilabel classi fi cation,resampling algorithms},
  Url                      = {http://sci2s.ugr.es/sites/default/files/ficherosPublicaciones/1790{\_}2015-Neuro-Charte-MultiLabel{\_}Imbalanced.pdf http://linkinghub.elsevier.com/retrieve/pii/S0925231215004269}
}

@InProceedings{Chekina2011,
  Title                    = {{Meta-learning for selecting a multi-label classification algorithm}},
  Author                   = {Chekina, Lena and Rokach, Lior and Shapira, Bracha},
  Booktitle                = {Proceedings - IEEE International Conference on Data Mining, ICDM},
  Year                     = {2011},
  Pages                    = {220--227},

  Abstract                 = {Although various algorithms for multi-label classification have been developed in recent years, there is little, if any, information as to when each method is beneficial. The main goal of this paper is to compare the classification performance of several multi-label algorithms and to develop a set of rules or tools that will help in selecting the optimal algorithm according to a specific dataset and target evaluation measure. We utilize a meta-learning approach allowing fast automatic selection of the most appropriate algorithm for an unseen dataset based on its descriptive characteristics. We also define a list of characteristics specific for multi-label datasets. The experimental results indicate the applicability and usefulness of the meta-learning approach.},
  Doi                      = {10.1109/ICDMW.2011.118},
  File                     = {:home/jan/Documents/Mendeley Desktop/Chekina, Rokach, Shapira/Proceedings - IEEE International Conference on Data Mining, ICDM/06137383.pdf:pdf},
  ISBN                     = {9780769544090},
  ISSN                     = {15504786},
  Keywords                 = {Dataset characteristics,Evaluation measures,Meta-learning,Multi-label classification}
}

@Article{Chen2007,
  Title                    = {{Document transformation for multi-label feature selection in text categorization}},
  Author                   = {Chen, Weizhu and Yan, Jun and Zhang, Benyu and Chen, Zheng and Yang, Qiang},
  Journal                  = {Proceedings - IEEE International Conference on Data Mining, ICDM},
  Year                     = {2007},
  Pages                    = {451--456},

  Abstract                 = {Feature selection on multi-label documents for automatic text categorization$\backslash$nis an under-explored research area. This paper presents a systematic$\backslash$ndocument transformation framework, whereby the multi-label documents$\backslash$nare transformed into single-label documents before applying standard$\backslash$nfeature selection algorithms, to solve the multi-label feature selection$\backslash$nproblem. Under this framework, we undertake a comparative study on$\backslash$nfour intuitive document transformation approaches and propose a novel$\backslash$napproach called entropy-based label assignment (ELA), which assigns$\backslash$nthe labels weights to a multi-label document based on label entropy.$\backslash$nThree standard feature selection algorithms are utilized for evaluating$\backslash$nthe document transformation approaches in order to verify its impact$\backslash$non multi-class text categorization problems. Using a SVM classifier$\backslash$nand two multi-label evaluation benchmark text collections, we show$\backslash$nthat the choice of document transformation approaches can significantly$\backslash$ninfluence the performance of multi-class categorization and that$\backslash$nour proposed document transformation approach ELA can achieve better$\backslash$nperformance than all other approaches.},
  Doi                      = {10.1109/ICDM.2007.18},
  File                     = {:home/jan/Documents/Mendeley Desktop/Chen et al/Proceedings - IEEE International Conference on Data Mining, ICDM/04470272.pdf:pdf},
  ISBN                     = {0769530184},
  ISSN                     = {15504786}
}

@Article{Chen2012,
  Title                    = {{Feature-aware Label Space Dimension Reduction for Multi-label Classification}},
  Author                   = {Chen, Yn and Lin, Ht},
  Journal                  = {Advances in Neural Information Processing Systems},
  Year                     = {2012},
  Pages                    = {1538--1546},

  Abstract                 = {Label space dimension reduction (LSDR) is an efficient and effective paradigm for multi-label classification with many classes. Existing approaches to LSDR, such as compressive sensing and principal label space transformation, exploit only the label part of the dataset, but not the feature part. In this paper, we propose a novel approach to LSDR that considers both the label and the feature parts. The approach, called conditional principal label space transformation, is based on minimizing an upper bound of the popular Hamming loss. The minimization step of the approach can be carried out efficiently by a simple use of singular value decomposition. In addition, the approach can be extended to a kernelized version that allows the use of sophisticated feature combinations to assist LSDR. The experimental results verify that the proposed approach is more effective than existing ones to LSDR across many real-world datasets. 1},
  Annote                   = {NULL},
  File                     = {:home/jan/Documents/Mendeley Desktop/Chen, Lin/Advances in Neural Information Processing Systems/Chen, Lin - 2012 - Feature-aware Label Space Dimension Reduction for Multi-label Classification.pdf:pdf},
  ISBN                     = {9781627480031},
  ISSN                     = {10495258},
  Url                      = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2012{\_}0728.pdf}
}

@Article{Cherman2011a,
  Title                    = {{Multi-label Problem Transformation Methods: a Case Study}},
  Author                   = {Cherman, Everton Alvares and Monard, Maria Carolina and Metz, Jean},
  Journal                  = {CLEI ELECTRONIC JOURNAL},
  Year                     = {2011},
  Number                   = {4},
  Volume                   = {14},

  Abstract                 = {Traditional classification algorithms consider learning problems that contain only one label, i.e., each example is associated with one single nominal target variable characterizing its property. However, the number of practical applications involving data with multiple target variables has increased. To learn from this sort of data, multi-label classification algorithms should be used. The task of learning from multi-label data can be addressed by methods that transform the multi-label classification problem into several single-label classification problems. In this work, two well known methods based on this approach are used, as well as a third method we propose to overcome some deficiencies of one of them, in a case study using textual data related to medical findings, which were structured using the bag-of-words approach. The experimental study using these three methods shows an improvement on the results obtained by our proposed multi-label classification method.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Cherman, Monard, Metz/CLEI ELECTRONIC JOURNAL/Cherman, Monard, Metz - 2011 - Multi-label Problem Transformation Methods a Case Study.pdf:pdf},
  Keywords                 = {binary relevance,label dependency,machine learning,multi-label classification}
}

@Article{Clare2001,
  Title                    = {{Knowledge Discovery in Multi-label Phenotype Data}},
  Author                   = {Clare, Amanda and King, Ross D.},
  Journal                  = {Pkdd},
  Year                     = {2001},
  Pages                    = {42--53},
  Volume                   = {2168},

  Abstract                 = {The biological sciences are undergoing an explosion in the amount of available data. New data analysis methods are needed to deal with the data. We present work using KDD to analyse data from mutant phenotype growth experiments with the yeast S. cerevisiae to predict novel gene functions. The analysis of the data presented a number of challenges: multi-class labels, a large number of sparsely populated classes, the need to learn a set of accurate rules (not a complete classification), and a very large amount of missing values. We developed resampling strategies and modified the algorithm C4.5 to deal with these problems. Rules were learnt which are accurate and biologically meaningful. The rules predict function of 83 putative genes of currently unknown function at an estimated accuracy of {\textgreater} 80{\%}.},
  Doi                      = {10.1007/3-540-44794-6_4},
  File                     = {:home/jan/Documents/Mendeley Desktop/Clare, King/Pkdd/Clare, King - Unknown - Knowledge Discovery in Multi-Label Phenotype Data.pdf:pdf},
  ISBN                     = {3-540-42534-9},
  ISSN                     = {16113349},
  Url                      = {http://www.springerlink.com/index/10.1007/3-540-44794-6}
}

@Article{Comite2003,
  Title                    = {{Learning multi-label alternating decision trees from texts and data}},
  Author                   = {Comite, Francesco De and Gilleron, Remi and Tommasi, Marc},
  Journal                  = {Third International Conference Machine Learning and Data Mining in Pattern Recognition},
  Year                     = {2003},
  Pages                    = {35--49},

  Abstract                 = {Multi-label decision procedures are the target of the supervised learning algorithm we propose in this paper. Multi-label decision procedures map examples to a finite set of labels. Our learning algorithm extends Schapire and Singer's Adaboost.MH and produces sets of rules that can be viewed as trees like Alternating Decision Trees (invented by Freund and Mason). Experiments show that we take advantage of both performance and readability using boosting techniques as well as tree representations of large set of rules. Moreover, a key feature of our algorithm is the ability to handle heterogenous input data: discrete and continuous values and text data.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Comite, Gilleron, Tommasi/Third International Conference Machine Learning and Data Mining in Pattern Recognition/De Comite, Gilleron, Tommasi - 2003 - Learning Multi-label Alternating Decision Trees from Texts and Data.pdf:pdf},
  ISBN                     = {978-3-540-40504-7},
  ISSN                     = {03029743},
  Keywords                 = {alternating decision trees,boosting,multi-label,text mining},
  Publisher                = {Springer},
  Url                      = {https://hal.inria.fr/inria-00536733}
}

@Article{DeComite2003,
  Title                    = {{Learning multi-label alternating decision trees from texts and data}},
  Author                   = {{De Comite}, Fran{\c{c}}esco and Gilleron, Remi R{\'{e}}mi and Tommasi, Marc and Comite, Francesco De and Gilleron, Remi R{\'{e}}mi and Tommasi, Marc},
  Journal                  = {Third International Conference Machine Learning and Data Mining in Pattern Recognition},
  Year                     = {2003},
  Pages                    = {35--49},

  Abstract                 = {Multi-label decision procedures are the target of the supervised learning algorithm we propose in this paper. Multi-label decision procedures map examples to a finite set of labels. Our learning algorithm extends Schapire and Singer's Adaboost.MH and produces sets of rules that can be viewed as trees like Alternating Decision Trees (invented by Freund and Mason). Experiments show that we take advantage of both performance and readability using boosting techniques as well as tree representations of large set of rules. Moreover, a key feature of our algorithm is the ability to handle heterogenous input data: discrete and continuous values and text data.},
  Annote                   = {From Duplicate 1 (Learning multi-label alternating decision trees from texts and data - Comite, Francesco De; Gilleron, Remi; Tommasi, Marc)

NULL},
  File                     = {:home/jan/Documents/Mendeley Desktop/Comite, Gilleron, Tommasi/Third International Conference Machine Learning and Data Mining in Pattern Recognition/De Comite, Gilleron, Tommasi - 2003 - Learning Multi-label Alternating Decision Trees from Texts and Data.pdf:pdf},
  ISBN                     = {978-3-540-40504-7},
  ISSN                     = {03029743},
  Keywords                 = {alternating decision trees,boosting,multi-label,text mining},
  Publisher                = {Springer},
  Url                      = {https://hal.inria.fr/inria-00536733}
}

@Article{Dembcz,
  Title                    = {{An Exact Algorithm for F-Measure Maximization}},
  Author                   = {Dembcz, Krzysztof and Waegeman, Willem and Cheng, Weiwei},

  Abstract                 = {The F-measure, originally introduced in information retrieval, is nowadays rou-tinely used as a performance metric for problems such as binary classification, multi-label classification, and structured output prediction. Optimizing this mea-sure remains a statistically and computationally challenging problem, since no closed-form maximizer exists. Current algorithms are approximate and typically rely on additional assumptions regarding the statistical distribution of the binary response variables. In this paper, we present an algorithm which is not only com-putationally efficient but also exact, regardless of the underlying distribution. The algorithm requires only a quadratic number of parameters of the joint distribu-tion (with respect to the number of binary responses). We illustrate its practical performance by means of experimental results for multi-label classification.},
  Annote                   = {NULL},
  File                     = {:home/jan/Documents/Mendeley Desktop/Dembcz, Waegeman, Cheng/Unknown/Dembcz, Waegeman, Cheng - Unknown - An Exact Algorithm for F-Measure Maximization.pdf:pdf},
  ISBN                     = {978-1-61839-599-3}
}

@Article{Dembcz2012,
  Title                    = {{On label dependence and loss minimization in multi-label classification}},
  Author                   = {Dembcz, Krzysztof and Waegeman, Willem and Cheng, Weiwei and H{\"{u}}llermeier, Eyke and Tsoumakas, Grigorios and Zhang, Min-Ling and Zhou, Zhi-Hua and Dembczyski, K and Waegeman, W and Cheng, W and H{\"{u}}llermeier, E},
  Journal                  = {Mach Learn},
  Year                     = {2012},
  Pages                    = {5--45},
  Volume                   = {88},

  Abstract                 = {Most of the multi-label classification (MLC) methods proposed in recent years intended to exploit, in one way or the other, dependencies between the class labels. Compar-ing to simple binary relevance learning as a baseline, any gain in performance is normally explained by the fact that this method is ignoring such dependencies. Without questioning the correctness of such studies, one has to admit that a blanket explanation of that kind is hiding many subtle details, and indeed, the underlying mechanisms and true reasons for the improvements reported in experimental studies are rarely laid bare. Rather than propos-ing yet another MLC algorithm, the aim of this paper is to elaborate more closely on the idea of exploiting label dependence, thereby contributing to a better understanding of MLC. Adopting a statistical perspective, we claim that two types of label dependence should be distinguished, namely conditional and marginal dependence. Subsequently, we present three scenarios in which the exploitation of one of these types of dependence may boost the pre-dictive performance of a classifier. In this regard, a close connection with loss minimization is established, showing that the benefit of exploiting label dependence does also depend on the type of loss to be minimized. Concrete theoretical results are presented for two repre-Editors: Mach Learn (2012) 88:5–45 sentative loss functions, namely the Hamming loss and the subset 0/1 loss. In addition, we give an overview of state-of-the-art decomposition algorithms for MLC and we try to re-veal the reasons for their effectiveness. Our conclusions are supported by carefully designed experiments on synthetic and benchmark data.},
  Doi                      = {10.1007/s10994-012-5285-8},
  File                     = {:home/jan/Documents/Mendeley Desktop/Dembcz et al/Mach Learn/Dembcz et al. - 2012 - On label dependence and loss minimization in multi-label classification.pdf:pdf},
  Keywords                 = {Label dependence {\textperiodcentered},Loss functions,Multi-label classification {\textperiodcentered}}
}

@Article{Dembczyski2010,
  Title                    = {{Bayes Optimal Multilabel Classification via Probabilistic Classifier Chains}},
  Author                   = {Dembczy, Krzysztof},
  Journal                  = {Proceedings of the 27th international conference on machine learning (ICML-10)},
  Year                     = {2010},
  Pages                    = {279--286},

  Abstract                 = {In the realm of multilabel classification (MLC), it has become an opinio communis that optimal predictive performance can only be achieved by learners that explicitly take label dependence into account. The goal of this paper is to elaborate on this postulate in a critical way. To this end, we formal- ize and analyze MLC within a probabilistic setting. Thus, it becomes possible to look at the problem from the point of view of risk minimization and Bayes optimal predic- tion. Moreover, inspired by our probabilistic setting, we propose a new method for MLC that generalizes and outperforms another ap- proach, called classifier chains, that was re- cently introduced in the literature.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Dembczyski, Cheng, H{\"{u}}llermeier/Unknown/Dembczyski, Cheng, H{\"{u}}llermeier - 2010 - Bayes Optimal Multilabel Classification via Probabilistic Classifier Chains.pdf:pdf},
  ISBN                     = {9781605589077},
  Keywords                 = {chain classifiers,label correlation,multilabel classification,risk minimization},
  Url                      = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/icml2010{\_}DembczynskiCH10.pdf http://www.uni-marburg.de/fb12/kebi/people/cheng/cheng-icml10c.pdf}
}

@Article{Dembczynski2013,
  Title                    = {{Optimizing the F-measure in multi-label classification: Plug-in rule approach versus structured loss minimization}},
  Author                   = {Dembczy{\'{n}}ski, Krzysztof and Jachnik, Arkadiusz and Kot{\l}owski, Wojciech and Waegeman, Willem and H{\"{u}}llermeier, Eyke},
  Journal                  = {30th International Conference on Machine Learning, ICML 2013},
  Year                     = {2013},
  Number                   = {PART 3},
  Pages                    = {2167--2175},
  Volume                   = {28},

  Abstract                 = {We compare the plug-in rule approach for optimizing the F $\beta$-measure in multi-label classification with an approach based on structured loss minimization, such as the structured support vector machine (SSVM). Whereas the former derives an optimal prediction from a probabilistic model in a separate inference step, the latter seeks to optimize the F $\beta$-mcasurc directly during the training phase. We introduce a novel plug-in rule algorithm that estimates all parameters required for a Bayes-optimal prediction via a set of multinomial regression models, and we compare this algorithm with SSVMs in terms of computational complexity and statistical consistency. As a main theoretical result, we show that our plug-in rule algorithm is consistent, whereas the SSVM approaches are not. Finally, we present results of a large experimental study showing the benefits of the introduced algorithm. Copyright 2013 by the author(s).},
  Annote                   = {NULL},
  File                     = {:home/jan/Documents/Mendeley Desktop/Dembczy{\'{n}}ski et al/30th International Conference on Machine Learning, ICML 2013/Dembczyski et al. - Unknown - Optimizing the F-Measure in Multi-Label Classification Plug-in Rule Approach versus Structured Loss Minimi.pdf:pdf},
  Url                      = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84897550056{\&}partnerID=tZOtx3y1}
}

@Article{Dembczynski2010,
  Title                    = {{Regret analysis for performance metrics in multi-label classification: The case of hamming and subset zero-one loss}},
  Author                   = {Dembczy{\'{n}}ski, Krzysztof and Waegeman, Willem and Cheng, Weiwei and H{\"{u}}llermeier, Eyke},
  Journal                  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  Year                     = {2010},
  Number                   = {PART 1},
  Pages                    = {280--295},
  Volume                   = {6321 LNAI},

  Abstract                 = {In multi-label classification (MLC), each instance is associated with$\backslash$na subset of labels instead of a single class, as in conventional$\backslash$nclassification, and this generalization enables the definition of$\backslash$na multitude of loss functions. Indeed, a large number of losses has$\backslash$nalready been proposed and is commonly applied as performance metrics$\backslash$nin experimental studies. However, even though these loss functions$\backslash$nare of a quite different nature, a concrete connection between the$\backslash$ntype of multi-label classifier used and the loss to be minimized$\backslash$nis rarely established, implicitly giving the misleading impression$\backslash$nthat the same method can be optimal for different loss functions.$\backslash$nIn this paper, we elaborate on risk minimization and the connection$\backslash$nbetween loss functions in MLC, both theoretically and empirically.$\backslash$nIn particular, we compare two important loss functions, namely the$\backslash$nHamming loss and the subset 0/1 loss. We perform a regret analysis,$\backslash$nshowing how poor a classifier intended to minimize the subset 0/1$\backslash$nloss can become in terms of Hamming loss and vice versa. The theoretical$\backslash$nresults are corroborated by experimental studies, and their implications$\backslash$nfor MLC methods are discussed in a broader context. 漏 2010 Springer-Verlag$\backslash$nBerlin Heidelberg.},
  Doi                      = {10.1007/978-3-642-15880-3_24},
  ISBN                     = {364215879X},
  ISSN                     = {03029743}
}

@InProceedings{Dembczyski,
  Title                    = {{Regret analysis for performance metrics in multi-label classification: The case of hamming and subset zero-one loss}},
  Author                   = {Dembczy{\'{n}}ski, Krzysztof and Waegeman, Willem and Cheng, Weiwei and H{\"{u}}llermeier, Eyke},
  Booktitle                = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  Year                     = {2010},
  Number                   = {PART 1},
  Pages                    = {280--295},
  Volume                   = {6321 LNAI},

  Abstract                 = {In multi-label classification (MLC), each instance is associated with$\backslash$na subset of labels instead of a single class, as in conventional$\backslash$nclassification, and this generalization enables the definition of$\backslash$na multitude of loss functions. Indeed, a large number of losses has$\backslash$nalready been proposed and is commonly applied as performance metrics$\backslash$nin experimental studies. However, even though these loss functions$\backslash$nare of a quite different nature, a concrete connection between the$\backslash$ntype of multi-label classifier used and the loss to be minimized$\backslash$nis rarely established, implicitly giving the misleading impression$\backslash$nthat the same method can be optimal for different loss functions.$\backslash$nIn this paper, we elaborate on risk minimization and the connection$\backslash$nbetween loss functions in MLC, both theoretically and empirically.$\backslash$nIn particular, we compare two important loss functions, namely the$\backslash$nHamming loss and the subset 0/1 loss. We perform a regret analysis,$\backslash$nshowing how poor a classifier intended to minimize the subset 0/1$\backslash$nloss can become in terms of Hamming loss and vice versa. The theoretical$\backslash$nresults are corroborated by experimental studies, and their implications$\backslash$nfor MLC methods are discussed in a broader context. 漏 2010 Springer-Verlag$\backslash$nBerlin Heidelberg.},
  Doi                      = {10.1007/978-3-642-15880-3_24},
  File                     = {:home/jan/Documents/Mendeley Desktop/Dembczy{\'{n}}ski et al/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Dembczyski et al. - Unknown - Regret Analysis for Performance Metrics in Multi-Label Classification The Case of Hamming and Subset Zero-.pdf:pdf},
  ISBN                     = {364215879X},
  ISSN                     = {03029743},
  Url                      = {https://biblio.ugent.be/publication/1155381/file/1210780.pdf}
}

@Article{Dembczynski,
  Title                    = {{On Label Dependence in Multi-Label Classification}},
  Author                   = {Dembczynski, Krzysztof and Waegeman, Willem and Cheng, Weiwei and H{\"{u}}llermeier, Eyke},

  Abstract                 = {The aim of this paper is to elaborate on the important issue of label dependence in multi-label classification (MLC). Looking at the problem from a statistical perspective, we claim that two different types of label depen-dence should be distinguished, namely con-ditional and unconditional. We formally ex-plain the differences and connections between both types of dependence and illustrate them by means of simple examples. Moreover, we given an overview of state-of-the-art algo-rithms for MLC and categorize them accord-ing to the type of label dependence they seek to capture.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Dembczynski et al/Unknown/Dembczynski - 2010 - On Label Dependence in Multi-Label Classification.pdf:pdf}
}

@InProceedings{Dembczynski2012,
  Title                    = {{An analysis of chaining in multi-label classification}},
  Author                   = {Dembczynski, Krzysztof and Waegeman, Willem and H{\"{u}}llermeier, Eyke},
  Booktitle                = {Frontiers in Artificial Intelligence and Applications},
  Year                     = {2012},
  Pages                    = {294--299},
  Volume                   = {242},

  Abstract                 = {The idea of classifier chains has recently been introduced as a promising technique for multi-label classification. However, de-spite being intuitively appealing and showing strong performance in empirical studies, still very little is known about the main prin-ciples underlying this type of method. In this paper, we provide a detailed probabilistic analysis of classifier chains from a risk mini-mization perspective, thereby helping to gain a better understanding of this approach. As a main result, we clarify that the original chain-ing method seeks to approximate the joint mode of the conditional distribution of label vectors in a greedy manner. As a result of a the-oretical regret analysis, we conclude that this approach can perform quite poorly in terms of subset 0/1 loss. Therefore, we present an en-hanced inference procedure for which the worst-case regret can be upper-bounded far more tightly. In addition, we show that a proba-bilistic variant of chaining, which can be utilized for any loss func-tion, becomes tractable by using Monte Carlo sampling. Finally, we present experimental results confirming the validity of our theoretical findings.},
  Doi                      = {10.3233/978-1-61499-098-7-294},
  File                     = {:home/jan/Documents/Mendeley Desktop/Dembczynski, Waegeman, H{\"{u}}llermeier/Frontiers in Artificial Intelligence and Applications/Dembcz Nski, Waegeman, Ullermeier - Unknown - An Analysis of Chaining in Multi-Label Classification.pdf:pdf},
  ISBN                     = {9781614990970},
  ISSN                     = {09226389},
  Url                      = {https://biblio.ugent.be/publication/3132158/file/3132170}
}

@Article{Dimou2009,
  Title                    = {{An empirical study of multi-label learning methods for video annotation}},
  Author                   = {Dimou, Anastasios and Tsoumakas, Grigorios and Mezaris, Vasileios and Kompatsiaris, Ioannis and Vlahavas, L},
  Journal                  = {Content-Based Multimedia Indexing, 2009. CBMI'09. Seventh International Workshop on},
  Year                     = {2009},
  Number                   = {June},
  Pages                    = {19--24},

  Abstract                 = {This paper presents an experimental comparison of dif- ferent approaches to learning from multi-labeled video data. We compare state-of-the-art multi-label learning methods on the Mediamill Challenge dataset. We employ MPEG-7 and SIFT-based global image descriptors independently and in conjunction using variations of the stacking approach for their fusion. We evaluate the results comparing the different classifiers using both MPEG-7 and SIFT-based descriptors and their fusion. A variety of multi-label evaluation mea- sures is used to explore advantages and disadvantages of the examined classifiers. Results give rise to interesting conclusions.},
  Annote                   = {NULL},
  File                     = {:home/jan/Documents/Mendeley Desktop/Dimou et al/Content-Based Multimedia Indexing, 2009. CBMI'09. Seventh International Workshop on/Dimou et al. - 2009 - An Empirical Study of Multi-Label Learning Methods for Video Annotation.pdf:pdf},
  ISBN                     = {1424442656},
  Url                      = {http://lpis.csd.auth.gr/publications/tsoumakas-cbmi09.pdf}
}

@Article{Diplaris2005,
  Title                    = {{Protein classification with multiple algorithms}},
  Author                   = {Diplaris, Sotiris and Tsoumakas, Grigorios and Mitkas, Pericles A. and Vlahavas, Ioannis},
  Journal                  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  Year                     = {2005},
  Pages                    = {448--456},
  Volume                   = {3746 LNCS},

  Abstract                 = {Nowadays, the number of protein sequences being stored in central protein databases from labs all over the world is constantly increasing. From these proteins only a fraction has been experimentally analyzed in order to de- tect their structure and hence their function in the corresponding organism. The reason is that experimental determination of structure is labor-intensive and quite time-consuming. Therefore there is the need for automated tools that can classify new proteins to structural families. This paper presents a comparative evaluation of several algorithms that learn such classification models from data concerning patterns of proteins with known structure. In addition, several ap- proaches that combine multiple learning algorithms to increase the accuracy of predictions are evaluated. The results of the experiments provide insights that can help biologists and computer scientists design high-performance protein classification systems of high quality.},
  Doi                      = {10.1007/11573036_42},
  File                     = {:home/jan/Documents/Mendeley Desktop/Diplaris et al/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Diplaris et al. - 2005 - Protein classification with multiple algorithms.pdf:pdf},
  ISBN                     = {3540296735},
  ISSN                     = {03029743}
}

@Article{Elisseeff2001,
  Title                    = {{A kernel method for multi-labelled classification}},
  Author                   = {Elisseeff, Andr{\'{e}} and Weston, Jason},
  Journal                  = {Advances in neural information processing systems},
  Year                     = {2001},
  Pages                    = {681--687},

  Abstract                 = {This article presents a Support Vector Machine (SVM) like learning sys-tem to handle multi-label problems. Such problems are usually decom-posed into many two-class problems but the expressive power of such a system can be weak [5, 7]. We explore a new direct approach. It is based on a large margin ranking system that shares a lot of common proper-ties with SVMs. We tested it on a Yeast gene functional classification problem with positive results.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Elisseeff, Weston/Advances in neural information processing systems/Elisseeff, Weston - 2001 - A kernel method for multi-labelled classification.pdf:pdf;:home/jan/Documents/Mendeley Desktop/Elisseeff, Weston/Advances in neural information processing systems/Elisseeff, Weston - Unknown - A kernel method for multi-labelled classification.pdf:pdf},
  ISBN                     = {0262042088},
  ISSN                     = {10495258},
  Url                      = {https://pdfs.semanticscholar.org/e925/33e4adca54e31d7b3726088469b493c2282e.pdf?{\_}ga=1.194163567.379343330.1490351020}
}

@Article{Furnkranz2008,
  Title                    = {{Multilabel classification via calibrated label ranking}},
  Author                   = {F{\"{u}}rnkranz, Johannes and H{\"{u}}llermeier, Eyke and {Loza Menc{\'{i}}a}, Eneldo and Brinker, Klaus and {Fawcett F{\"{u}}rnkranz}, Tom J and {Loza Menc{\'{i}}a}, E and H{\"{u}}llermeier, E and Brinker, K},
  Journal                  = {Mach Learn},
  Year                     = {2008},
  Number                   = {73},
  Pages                    = {133--153},
  Volume                   = {73},

  Abstract                 = {Label ranking studies the problem of learning a mapping from instances to rank-ings over a predefined set of labels. Hitherto existing approaches to label ranking implicitly operate on an underlying (utility) scale which is not calibrated in the sense that it lacks a natural zero point. We propose a suitable extension of label ranking that incorporates the calibrated scenario and substantially extends the expressive power of these approaches. In particular, our extension suggests a conceptually novel technique for extending the common learning by pairwise comparison approach to the multilabel scenario, a setting previously not being amenable to the pairwise decomposition technique. The key idea of the approach is to introduce an artificial calibration label that, in each example, separates the relevant from the irrelevant labels. We show that this technique can be viewed as a combination of pair-wise preference learning and the conventional relevance classification technique, where a separate classifier is trained to predict whether a label is relevant or not. Empirical results in the area of text categorization, image classification and gene analysis underscore the merits of the calibrated model in comparison to state-of-the-art multilabel learning methods.},
  Doi                      = {10.1007/s10994-008-5064-8},
  File                     = {:home/jan/Documents/Mendeley Desktop/F{\"{u}}rnkranz et al/Mach Learn/F{\"{u}}rnkranz et al. - 2008 - Multilabel classification via calibrated label ranking.pdf:pdf},
  Keywords                 = {Multi-label classification {\textperiodcentered},Preference learning {\textperiodcentered},Ranking},
  Url                      = {http://download.springer.com/static/pdf/878/art{\%}253A10.1007{\%}252Fs10994-008-5064-8.pdf?originUrl=http{\%}3A{\%}2F{\%}2Flink.springer.com{\%}2Farticle{\%}2F10.1007{\%}2Fs10994-008-5064-8{\&}token2=exp=1490609445{~}acl={\%}2Fstatic{\%}2Fpdf{\%}2F878{\%}2Fart{\%}25253A10.1007{\%}25252Fs10994-008-506}
}

@Article{Feng2006,
  Title                    = {{Correlated label propagation with application to multi-label learning}},
  Author                   = {Feng, Kang and Rong, Jin and Sukthankar, Rahul},
  Journal                  = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  Year                     = {2006},
  Number                   = {c},
  Pages                    = {1719--1726},
  Volume                   = {2},

  Abstract                 = { Many computer vision applications, such as scene analysis and medical image interpretation, are ill-suited for traditional classification where each image can only be associated with a single class. This has stimulated recent work in multi-label learning where a given image can be tagged with multiple class labels. A serious problem with existing approaches is that they are unable to exploit correlations between class labels. This paper presents a novel framework for multi-label learning termed Correlated Label Propagation (CLP) that explicitly models interactions between labels in an efficient manner. As in standard label propagation, labels attached to training data points are propagated to test data points; however, unlike standard algorithms that treat each label independently, CLP simultaneously co-propagates multiple labels. Existing work eschews such an approach since naive algorithms for label co-propagation are intractable. We present an algorithm based on properties of submodular functions that efficiently finds an optimal solution. Our experiments demonstrate that CLP leads to significant gains in precision/recall against standard techniques on two real-world computer vision tasks involving several hundred labels.},
  Doi                      = {10.1109/CVPR.2006.90},
  File                     = {:home/jan/Documents/Mendeley Desktop/Feng, Rong, Sukthankar/Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition/Feng, Rong, Sukthankar - 2006 - Correlated label propagation with application to multi-label learning.pdf:pdf},
  ISBN                     = {0769525970},
  ISSN                     = {10636919}
}

@Article{Gao2011,
  Title                    = {{On the Consistency of Multi-Label Learning}},
  Author                   = {Gao, Wei and Zhou, Zhi-Hua},
  Journal                  = {Annals of Statistics},
  Year                     = {2011},

  Abstract                 = {Multi-label learning has attracted much attention during the past few years. Many multilabel learning approaches have been developed, mostly working with surrogate loss functions since multi-label loss functions are usually difficult to optimize directly owing to non-convexity and discontinuity. Though these approaches are effective, to the best of our knowledge, there is no theoretical result on the convergence of risk of the learned functions to the Bayes risk. In this paper, focusing on two well-known multi-label loss functions, i.e., ranking loss and hamming loss, we prove a necessary and sufficient condition for the consistency of multi-label learning based on surrogate loss functions. Our results disclose that, surprisingly, none convex surrogate loss is consistent with the ranking loss. Inspired by the finding, we introduce the partial ranking loss, with which some surrogate functions are consistent. For hamming loss, we show that some recent multi-label learning approaches are inconsistent even for deterministic multi-label classification, and give a surrogate loss function which is consistent for the deterministic case. Finally, we discuss on the consistency of learning approaches which address multi-label learning by decomposing into a set of binary classification problems},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1204.1688},
  Doi                      = {10.1214/13-AOS1142},
  Eprint                   = {1204.1688},
  File                     = {:home/jan/Documents/Mendeley Desktop/Gao, Zhou/Annals of Statistics/Gao, Zhou - 2011 - On the Consistency of Multi-Label Learning.pdf:pdf},
  ISBN                     = {9781605589077},
  ISSN                     = {00905364},
  Keywords                 = {Consistency,hamming loss,multi-label learning,ranking loss,surrogate loss},
  Pmid                     = {14121148}
}

@Article{Gasse2015,
  Title                    = {{On the Optimality of Multi-Label Classification under Subset Zero-One Loss for Distributions Satisfying the Composition Property}},
  Author                   = {Gasse, Maxime and Aussem, Alex and Elghazel, Haytham},
  Journal                  = {ICML},
  Year                     = {2015},
  Volume                   = {37},

  Abstract                 = {The benefit of exploiting label dependence in multi-label classification is known to be closely dependent on the type of loss to be minimized. In this paper, we show that the subsets of labels that appear as irreducible factors in the factor-ization of the conditional distribution of the la-bel set given the input features play a pivotal role for multi-label classification in the context of 0/1 loss minimization, as they divide the learning task into simpler independent multi-class prob-lems. We establish theoretical results to charac-terize and identify these irreducible label factors for any given probability distribution satisfying the Composition property. The analysis lays the foundation for generic multi-label classification and optimal feature subset selection procedures under this subclass of distributions. Our conclu-sions are supported by carefully designed exper-iments on synthetic and benchmark data.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Gasse, Aussem, Elghazel/ICML/Gasse, Aussem, Elghazel - 2015 - On the Optimality of Multi-Label Classification under Subset Zero-One Loss for Distributions Satisfying.pdf:pdf},
  ISBN                     = {9781510810587},
  Keywords                 = {Markov bounda,Multi-label learning,Zero-one loss}
}

@Article{Gibaja2015,
  Title                    = {{A Tutorial on Multilabel Learning}},
  Author                   = {Gibaja, Eva and Ventura, Sebasti{\'{a}}n},
  Journal                  = {ACM Computing Surveys (CSUR)},
  Year                     = {2015},
  Number                   = {3},
  Pages                    = {52:1----52:38},
  Volume                   = {47},

  Abstract                 = {Multilabel learning has become a relevant learning paradigm in the past years due to the increasing number of fields where it can be applied and also to the emerging number of techniques that are being developed. This article presents an up-to-date tutorial about multilabel learning that introduces the paradigm and describes the main contributions developed. Evaluation measures, fields of application, trending topics, and resources are also presented.},
  Doi                      = {10.1145/2716262},
  File                     = {:home/jan/Documents/Mendeley Desktop/Gibaja, Ventura/ACM Computing Surveys (CSUR)/Gibaja, Ventura - 2010 - A Tutorial on Multi-Label Learning.pdf:pdf;:home/jan/Documents/Mendeley Desktop/Gibaja, Ventura/ACM Computing Surveys (CSUR)/Gibaja - 2013 - 39 A Tutorial on Multi - Label Learning.pdf:pdf},
  ISBN                     = {0360-0300},
  ISSN                     = {0360-0300},
  Keywords                 = {Multilabel learning,classification,data mining,machine learning,ranking},
  Url                      = {https://www.researchgate.net/profile/Sebastian{\_}Ventura/publication/270337594{\_}A{\_}Tutorial{\_}on{\_}Multi-Label{\_}Learning/links/54bcd8460cf253b50e2d697b.pdf http://doi.acm.org/10.1145/2716262}
}

@Article{Gibaja2014,
  Title                    = {{Multi-label learning: A review of the state of the art and ongoing research}},
  Author                   = {Gibaja, Eva and Ventura, Sebasti{\'{a}}n},
  Journal                  = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  Year                     = {2014},
  Number                   = {6},
  Pages                    = {411--444},
  Volume                   = {4},

  Abstract                 = {Multi-label learning is quite a recent supervised learning paradigm. Owing to its capabilities to improve performance in problems where a pattern may have more than one associated class, it has attracted the attention of researchers, producing an increasing number of publications. This study presents an up-to-date overview about multi-label learning with the aim of sorting and describing the main approaches developed till now. The formal definition of the paradigm, the analysis of its impact on the literature, its main applications, works developed, pitfalls and guidelines, and ongoing research are presented.},
  Doi                      = {10.1002/widm.1139},
  File                     = {:home/jan/Documents/Mendeley Desktop/Gibaja, Ventura/Wiley Interdisciplinary Reviews Data Mining and Knowledge Discovery/Gibaja, Ventura - 2014 - Multi-label learning A review of the state of the art and ongoing research.pdf:pdf},
  ISSN                     = {19424795},
  Keywords                 = {multi - label learning {\textperiodcentered} review}
}

@Article{Godbole2007,
  Title                    = {{Large-scale sentiment analysis for news and blogs}},
  Author                   = {Godbole, N and Srinivasaiah, M},
  Journal                  = {Conference on Weblogs and Social Media (ICWSM 2007)},
  Year                     = {2007},
  Pages                    = {219--222},

  Abstract                 = {Newspapers and blogs express opinion of news entities (peo- ple, places, things) while reporting on recent events. We present a system that assigns scores indicating positive or negative opinion to each distinct entity in the text corpus. Our system consists of a sentiment identication phase, which associates expressed opinions with each relevant entity, and a sentiment aggregation and scoring phase, which scores each entity relative to others in the same class. Finally, we evalu- ate the signicance of our scoring techniques over large corpus of news and blogs.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {cond-mat/0112110},
  Doi                      = {10.1177/01461079070370040501},
  Eprint                   = {0112110},
  File                     = {:home/jan/Documents/Mendeley Desktop/Godbole, Srinivasaiah/Conference on Weblogs and Social Media (ICWSM 2007)/Godbole, Srinivasaiah - 2007 - Large-scale sentiment analysis for news and blogs.pdf:pdf},
  ISSN                     = {0146-1079},
  Keywords                 = {News,Sentiment Analysis,ge-scale sentiment analysis for,news and blogs},
  Mendeley-tags            = {News,Sentiment Analysis},
  Pmid                     = {12060727},
  Primaryclass             = {cond-mat},
  Url                      = {http://student.bus.olemiss.edu/files/conlon/Others/{\_}{\_}BookChapter{\_}SocialMEsia{\_}EBusiness/Large-Scale Sentiment Analysis for News and Blogs.pdf}
}

@Article{Godbole2004,
  Title                    = {{Discriminative Methods for Multi-labeled Classification}},
  Author                   = {Godbole, Shantanu and Sarawagi, Sunita},
  Journal                  = {Lecture Notes in Computer Science},
  Year                     = {2004},
  Pages                    = {22--30},
  Volume                   = {3056},

  Abstract                 = {In this paper we present methods of enhancing existing discriminative classifiers for multi-labeled predictions. Discriminative methods like support vector machines perform very well for uni-labeled text classification tasks. Multi-labeled classification is a harder task subject to relatively less attention. In the multi-labeled setting, classes are often related to each other or part of a is-a hierarchy. We present a new technique for combining text features and features indicating relationships between classes, which can be used with any discriminative algorithm. We also present two enhancements to the margin of SVMs for building better models in the presence of overlapping classes. We present results of experiments on real world text benchmark datasets. Our new methods beat accuracy of existing methods with statistically significant improvements.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {10.1007/978-3-540-24775-3{\_}5},
  Doi                      = {10.1007/978-3-540-24775-3_5},
  Eprint                   = {978-3-540-24775-3{\_}5},
  File                     = {:home/jan/Documents/Mendeley Desktop/Godbole, Sarawagi/Lecture Notes in Computer Science/Godbole, Sarawagi - 2004 - Discriminative Methods for Multi-labeled Classification.pdf:pdf},
  ISBN                     = {978-3-540-22064-0},
  ISSN                     = {03029743},
  Primaryclass             = {10.1007},
  Url                      = {http://link.springer.com/10.1007/978-3-540-24775-3{\_}5}
}

@Article{Goncalves,
  Title                    = {{Simpler is Better : a Novel Genetic Algorithm to Induce Compact Multi-label Chain Classifiers}},
  Author                   = {Gon{\c{c}}alves, Eduardo C and Plastino, Alexandre and Freitas, Alex A},
  Journal                  = {Proceedings of the 2015 on Genetic and Evolutionary Computation Conference (GECCO 2015)},
  Year                     = {2015},
  Pages                    = {559--566},

  Abstract                 = {Multi-label classification (MLC) is the task of assigning multiple class labels to an object based on the features that describe the object. One of the most effective MLC methods is known as Classifier Chains (CC). This approach consists in training q binary classifiers linked in a chain, y1 → y2 → ... → yq, with each responsible for classifying a specific label in {\{}l1, l2, ..., lq{\}}. The chaining mechanism allows each individual classifier to incorporate the predictions of the previous ones as additional information at classification time. Thus, possible correlations among labels can be automatically exploited. Nevertheless, CC suffers from two important drawbacks: (i) the label ordering is decided at random, although it usually has a strong effect on predictive accuracy; (ii) all labels are inserted into the chain, although some of them might carry irrelevant information to discriminate the others. In this paper we tackle both problems at once, by proposing a novel genetic algorithm capable of searching for a single optimized label ordering, while at the same time taking into consideration the utilization of partial chains. Experiments on benchmark datasets demonstrate that our approach is able to produce models that are both simpler and more accurate.},
  Doi                      = {10.1145/2739480.2754650},
  File                     = {:home/jan/Documents/Mendeley Desktop/Gon{\c{c}}alves, Plastino, Freitas/Unknown/Gon{\c{c}}alves, Plastino, Freitas - Unknown - Simpler is Better a Novel Genetic Algorithm to Induce Compact Multi - label Chain Classifiers.pdf:pdf},
  ISBN                     = {9781450334723},
  Keywords                 = {classifier chains,genetic algorithms,multi-label classification},
  Url                      = {https://www.researchgate.net/profile/Eduardo{\_}Goncalves17/publication/301290182{\_}Simpler{\_}is{\_}Better{\_}a{\_}Novel{\_}Genetic{\_}Algorithm{\_}to{\_}Induce{\_}Compact{\_}Multi-label{\_}Chain{\_}Classifiers/links/570fb19a08aec95f06158862.pdf}
}

@Article{Gong,
  Title                    = {{Deep Convolutional Ranking for Multilabel Image Annotation}},
  Author                   = {Gong, Yunchao and Jia, Yangqing and Leung, Thomas K and Research, Google and Toshev, Alexander and Ioffe, Sergey},

  Abstract                 = {Multilabel image annotation is one of the most important challenges in computer vision with many real-world applications. While existing work usually use con-ventional visual features for multilabel annotation, features based on Deep Neural Networks have shown potential to significantly boost performance. In this work, we propose to leverage the advantage of such features and analyze key compo-nents that lead to better performances. Specifically, we show that a significant per-formance gain could be obtained by combining convolutional architectures with approximate top-k ranking objectives, as thye naturally fit the multilabel tagging problem. Our experiments on the NUS-WIDE dataset outperforms the conven-tional visual features by about 10{\%}, obtaining the best reported performance in the literature.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Gong et al/Unknown/Gong et al. - Unknown - Deep Convolutional Ranking for Multilabel Image Annotation.pdf:pdf},
  Url                      = {https://pdfs.semanticscholar.org/3b04/9d8cfea6c3bed377090e0e7fa677d282a361.pdf https://arxiv.org/pdf/1312.4894.pdf}
}

@InProceedings{Gu2011,
  Title                    = {{Correlated multi-label feature selection}},
  Author                   = {Gu, Quanquan and Li, Zhenhui and Han, Jiawei},
  Booktitle                = {Proceedings of the 20th ACM international conference on Information and knowledge management - CIKM '11},
  Year                     = {2011},
  Pages                    = {1087},

  Abstract                 = {Multi-label learning studies the problem where each instance is associated with a set of labels. There are two challenges in multi-label learning: (1) the labels are interdependent and correlated, and (2) the data are of high dimensionality. In this paper, we aim to tackle these challenges in one shot. In particular, we propose to learn the label correlation and do feature selection simultaneously. We introduce a matrix-variate Normal prior distribution on the weight vectors of the classifier to model the label correlation. Our goal is to find a subset of features, based on which the label correlation regularized loss of label ranking is minimized. The resulting multi-label feature selection problem is a mixed integer programming, which is reformulated as quadratically constrained linear programming (QCLP). It can be solved by cutting plane algorithm, in each iteration of which a minimax optimization problem is solved by dual coordinate descent and projected sub-gradient descent alternatively. Experiments on benchmark data sets illustrate that the proposed methods outperform single-label feature selection method and many other state-of-the-art multi-label learning methods. {\textcopyright} 2011 ACM.},
  Doi                      = {10.1145/2063576.2063734},
  File                     = {:home/jan/Documents/Mendeley Desktop/Gu, Li, Han/Proceedings of the 20th ACM international conference on Information and knowledge management - CIKM '11/Gu, Li, Han - Unknown - Correlated Multi-Label Feature Selection.pdf:pdf},
  ISBN                     = {9781450307178},
  Keywords                 = {cutting plane,dual coordinate descent,feature selection,label correlation,multi-label learning},
  Url                      = {http://delivery.acm.org.ez.sun.ac.za/10.1145/2070000/2063734/p1087-gu.pdf?ip=146.232.129.75{\&}id=2063734{\&}acc=ACTIVE SERVICE{\&}key=646D7B17E601A2A5.C011CE1E941E2524.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=744899709{\&}CFTOKEN=20823743{\&}{\_}{\_}acm{\_}{\_}=1490876009{\_}2f58a46477}
}

@Article{Hsu2009,
  Title                    = {{Multi-Label Prediction via Compressed Sensing}},
  Author                   = {Hsu, Daniel and Kakade, Sham M and Langford, John and Zhang, Tong},
  Year                     = {2009},

  Abstract                 = {We consider multi-label prediction problems with large output spaces under the assumption of output sparsity – that the target (label) vectors have small support. We develop a general theory for a variant of the popular error correcting output code scheme, using ideas from compressed sensing for exploiting this sparsity. The method can be regarded as a simple reduction from multi-label regression problems to binary regression problems. We show that the number of subproblems need only be logarithmic in the total number of possible labels, making this approach radically more efficient than others. We also state and prove robustness guarantees for this method in the form of regret transform bounds (in general), and also provide a more detailed analysis for the linear prediction setting.},
  Annote                   = {NULL},
  File                     = {:home/jan/Documents/Mendeley Desktop/Hsu et al/Unknown/Hsu et al. - 2009 - Multi-Label Prediction via Compressed Sensing.pdf:pdf},
  Url                      = {http://www.cs.columbia.edu/{~}djhsu/papers/mlcs.pdf}
}

@Article{Huangc,
  Title                    = {{S NAPSHOT E NSEMBLES : T RAIN 1, GET M FOR FREE}},
  Author                   = {Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E and Weinberger, Kilian Q},

  Abstract                 = {Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural net-work, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with tradi-tional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4{\%} and 17.4{\%} respectively.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Huang et al/Unknown/Huang et al. - Unknown - S NAPSHOT E NSEMBLES T RAIN 1, GET M FOR FREE.pdf:pdf},
  Url                      = {https://arxiv.org/pdf/1704.00109.pdf}
}

@Article{Huang2010,
  Title                    = {{Optimization method based extreme learning machine for classification}},
  Author                   = {Huang, Guang-Bin and Ding, Xiaojian and Zhou, Hongming},
  Journal                  = {Neurocomputing},
  Year                     = {2010},
  Number                   = {1-3},
  Pages                    = {155--163},
  Volume                   = {74},

  Abstract                 = {Extreme learning machine (ELM) as an emergent technology has shown its good performance in regression applications as well as in large dataset (and/or multi-label) classification applications. The ELM theory shows that the hidden nodes of the ‘‘generalized'' single-hidden layer feedforward networks (SLFNs), which need not be neuron alike, can be randomly generated and the universal approximation capability of such SLFNs can be guaranteed. This paper further studies ELM for classification in the aspect of the standard optimization method and extends ELM to a specific type of ‘‘generalized'' SLFNs—support vector network. This paper shows that: (1) under the ELM learning framework, SVM's maximal margin property and the minimal norm of weights theory of feedforward neural networks are actually consistent; (2) from the standard optimization method point of view ELM for classification and SVM are equivalent but ELM has less optimization constraints due to its special separability feature; (3) as analyzed in theory and further verified by the simulation results, ELM for classification tends to achieve better generalization performance than traditional SVM. ELM for classification is less sensitive to user specified parameters and can be implemented easily.},
  Doi                      = {10.1016/j.neucom.2010.02.019},
  File                     = {:home/jan/Documents/Mendeley Desktop/Huang, Ding, Zhou/Neurocomputing/Optimization method based extreme learning machine for classification.pdf:pdf},
  ISBN                     = {0925-2312},
  ISSN                     = {09252312},
  Keywords                 = {ELM feature space,ELM kernel,Equivalence between ELM and SVM,Extreme learning machine,Maximal margin,Minimal norm of weights,Primal and dual ELM networks,Support vector machine,Support vector network,extreme learning machine,support vector machine,support vector network},
  Publisher                = {Elsevier},
  Url                      = {http://dx.doi.org/10.1016/j.neucom.2010.02.019}
}

@Article{Huangd,
  Title                    = {{Speed/accuracy trade-offs for modern convolutional object detectors}},
  Author                   = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and Murphy, Kevin and Research, Google},

  Abstract                 = {The goal of this paper is to serve as a guide for se-lecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern con-volutional object detection systems. A number of successful systems have been proposed in recent years, but apples-to-apples comparisons are difficult due to different base fea-ture extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [31], R-FCN [6] and SSD [26] systems, which we view as " meta-architectures " and trace out the speed/accuracy trade-off curve created by using alterna-tive feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and mem-ory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance mea-sured on the COCO detection task.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Huang et al/Unknown/Huang et al. - Unknown - Speedaccuracy trade-offs for modern convolutional object detectors.pdf:pdf},
  Url                      = {https://arxiv.org/pdf/1611.10012.pdf}
}

@Article{Huangb,
  Title                    = {{Cost-sensitive Label Embedding for Multi-label Classification}},
  Author                   = {Huang, Kuan-Hao and Lin, Hsuan-Tien},

  Abstract                 = {Label embedding (LE) is an important family of multi-label classification algorithms that digest the label information jointly for better perfor-mance. Different real-world applications evalu-ate performance by different cost functions of in-terest. Current LE algorithms often aim to opti-mize one specific cost function, but they can suf-fer from bad performance with respect to other cost functions. In this paper, we resolve the performance issue by proposing a novel cost-sensitive LE algorithm that takes the cost func-tion of interest into account. The proposed algo-rithm, cost-sensitive label embedding with multi-dimensional scaling (CLEMS), approximates the cost information with the distances of the embed-ded vectors using the classic multidimensional scaling approach for manifold learning. CLEMS is able to deal with both symmetric and asym-metric cost functions, and effectively makes cost-sensitive decisions by nearest-neighbor decoding within the embedded vectors. Theoretical results justify that CLEMS achieves the cost-sensitivity and extensive experimental results demonstrate that CLEMS is significantly better than a wide spectrum of existing LE algorithms and state-of-the-art cost-sensitive algorithms across different cost functions.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Huang, Lin/Unknown/Huang, Lin - Unknown - Cost-sensitive Label Embedding for Multi-label Classification.pdf:pdf}
}

@Article{Huang2013,
  Title                    = {{Sentiment and Topic Analysis on Social Media : A Multi-Task Multi-Label Classification Approach}},
  Author                   = {Huang, Shu and Peng, Wei and Li, Jingxuan and Lee, Dongwon},
  Journal                  = {Proceedings of the 5th Annual ACM Web Science Conference},
  Year                     = {2013},
  Pages                    = {172--181},

  Abstract                 = {Both sentiment analysis and topic classification are frequently used in customer care and marketing. They can help people understand the brand perception and customer opinions from social media, such as online posts, tweets, forums, and blogs. As such, in recent years, many solutions have been proposed for both tasks. However, we believe that the following two problems have not been addressed adequately: (1) Conventional solutions usually treat the two tasks in isolation. When the two tasks are closely related (e.g., posts about "customer care" often have a "negative" tone), exploring their correlation may yield a better accuracy; (2) Each post is usually assigned with only one sentiment label and one topic label. Since social media is, compared to traditional document corpus, more noisy, ambiguous, and sparser, single label classification may not be able to capture the post classes accurately. To address these two problems, in this paper, we propose a multi-task multi-label (MTML) classification model that performs classification of both sentiments and topics concurrently. It incorporates results of each task from prior steps to promote and reinforce the other iteratively. For each task, the model is trained with multiple labels so that they can help address class ambiguity. In the empirical validation, we compare the accuracy of MTML model against four competing methods in two different settings. Results show that MTML produces a much higher accuracy of both sentiment and topic classifications.},
  Doi                      = {10.1145/2464464.2464512},
  File                     = {:home/jan/Documents/Mendeley Desktop/Huang et al/Proceedings of the 5th Annual ACM Web Science Conference/Huang et al. - 2013 - Sentiment and Topic Analysis on Social Media A Multi-Task Multi-Label Classification Approach.pdf:pdf},
  ISBN                     = {9781450318891},
  Keywords                 = {Multi-Label,Sentiment Analysis,Topic Classification},
  Mendeley-tags            = {Multi-Label,Sentiment Analysis,Topic Classification}
}

@Article{Huang,
  Title                    = {{Multi-label hypothesis reuse}},
  Author                   = {Huang, Sheng-jun and Yu, Yang and Zhou, Zhi-hua},
  Journal                  = {Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '12},
  Year                     = {2012},
  Pages                    = {525},

  Abstract                 = {Multi-label learning arises in many real-world tasks where an object is naturally associated with multiple concepts. It is well-accepted that, in order to achieve a good performance, the relationship among labels should be exploited. Most existing approaches require the label relationship as prior knowledge, or exploit by counting the label co-occurrence. In this paper, we propose the MAHR approach, which is able to automatically discover and exploit label relationship. Our basic idea is that, if two labels are related, the hypothesis generated for one label can be helpful for the other label. MAHR implements the idea as a boosting approach with a hypothesis reuse mechanism. In each boosting round, the base learner for a label is generated by not only learning on its own task but also reusing the hypotheses from other labels, and the amount of reuse across labels provides an estimate of the label relationship. Extensive experimental results validate that MAHR is able to achieve superior performance and discover reasonable label relationship. Moreover, we disclose that the label relationship is usually asymmetric.},
  Doi                      = {10.1145/2339530.2339615},
  File                     = {:home/jan/Documents/Mendeley Desktop/Huang, Yu, Zhou/Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '12/Huang, Yu, Zhou - 2012 - Multi-label hypothesis reuse.pdf:pdf},
  ISBN                     = {9781450314626},
  Keywords                 = {hypothesis reuse,label relationship,multi-label learning},
  Url                      = {http://dl.acm.org/citation.cfm?id=2339530.2339615}
}

@Article{Huang2012,
  Title                    = {{Multi-Label Learning by Exploiting Label Correlations Locally}},
  Author                   = {Huang, Sheng-Jun and Zhou, Zhi-Hua},
  Journal                  = {AAAI Conference on Artificial Intelligence},
  Year                     = {2012},
  Pages                    = {949--955},

  Abstract                 = {It is well known that exploiting label correlations is important for multi-label learning. Existing approaches typically exploit label correlations globally, by assuming that the label correlations are shared by all the instances. In real-world tasks, however, different instances may share different label correlations, and few correlations are globally applicable. In this paper, we propose the ML-LOC approach which allows label correlations to be exploited locally. To encode the local influence of label correlations, we derive a LOC code to enhance the feature representation of each instance. The global discrimination fitting and local correlation sensitivity are incorporated into a unified framework, and an alternating solution is developed for the optimization. Experimental results on a number of image, text and gene data sets validate the effectiveness of our approach.},
  Doi                      = {10.1145/1835804.1835930},
  File                     = {:home/jan/Documents/Mendeley Desktop/Huang, Zhou/AAAI Conference on Artificial Intelligence/Huang, Zhou - 2012 - Multi-Label Learning by Exploiting Label Correlations Locally.pdf:pdf},
  ISBN                     = {9781577355687},
  ISSN                     = {9781577355687},
  Keywords                 = {Machine Learning (Main Track)}
}

@article{Wang2016,
  author    = {Jiang Wang and
               Yi Yang and
               Junhua Mao and
               Zhiheng Huang and
               Chang Huang and
               Wei Xu},
  title     = {{CNN-RNN:} {A} Unified Framework for Multi-label Image Classification},
  journal   = {CoRR},
  volume    = {abs/1604.04573},
  year      = {2016},
  url       = {http://arxiv.org/abs/1604.04573},
  timestamp = {Wed, 07 Jun 2017 14:42:20 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/WangYMHHX16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@Article{Karalas2015,
  Title                    = {{Deep learning for multi-label land cover classification}},
  Author                   = {Karalas, Konstantinos and Tsagkatakis, Grigorios and Zervakis, Michalis and Tsakalides, Panagiotis},
  Year                     = {2015},
  Pages                    = {96430Q--96430Q},
  Volume                   = {9643},

  Abstract                 = {While single-class classification has been a highly active topic in optical remote sensing, much less effort has been given to the multi-label classification framework, where pixels are associated with more than one labels, an approach closer to the reality than single-label classification. Given the complexity of this problem, identifying representative features extracted from raw images is of paramount importance. In this work, we investigate feature learning as a feature extraction process in order to identify the underlying explanatory patterns hidden in low-level satellite data for the purpose of multi-label classification. Sparse autoencoders composed of a single hidden layer, as well as stacked in a greedy layer-wise fashion formulate the core concept of our approach. The results suggest that learning such sparse and abstract representations of the features can aid in both remote sensing and multi-label problems. The results presented in the paper correspond to a novel real dataset of annotated spectral imagery naturally leading to the multi-label formulation.},
  Doi                      = {10.1117/12.2195082},
  File                     = {:home/jan/Documents/Mendeley Desktop/Karalas et al/Unknown/Karalas et al. - Unknown - Feature learning for multi-label land cover classification.pdf:pdf},
  ISBN                     = {9781628418538},
  ISSN                     = {1996756X},
  Keywords                 = {autoencoders,corine,deep,feature learning,learning,modis,multi-label classification,remote sensing,representation learning,sparse autoencoders},
  Url                      = {http://users.ics.forth.gr/tsakalid/PAPERS/CNFRS/2015-SPIE.pdf http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=2464537}
}

@Article{Katakis2008,
  Title                    = {{Multilabel Text Classification for Automated Tag Suggestion}},
  Author                   = {Katakis, Ioannis and Tsoumakas, Grigorios and Vlahavas, Ioannis},
  Journal                  = {Proceedings of the ECMLPKDD 2008 Discovery Challenge (2008)},
  Year                     = {2008},
  Number                   = {3},
  Pages                    = {1--9},
  Volume                   = {9},

  Abstract                 = {The increased popularity of tagging during the last few years can be mainly attributed to its embracing by most of the recently thriving user-centric content publishing and management Web 2.0 applications. However, tagging systems have some limitations that have led researchers to develop methods that assist users in the tagging process, by automat- ically suggesting an appropriate set of tags. We have tried to model the automated tag suggestion problem as a multilabel text classification task in order to participate in the ECML/PKDD 2008 Discovery Challenge.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Katakis, Tsoumakas, Vlahavas/Proceedings of the ECMLPKDD 2008 Discovery Challenge (2008)/Katakis, Tsoumakas, Vlahavas - 2008 - Multilabel Text Classification for Automated Tag Suggestion.pdf:pdf},
  ISSN                     = {10675027}
}

@Article{Koyejo2015,
  Title                    = {{Consistent Multilabel Classification}},
  Author                   = {Koyejo, Oluwasanmi O. and Natarajan, Nagarajan and Ravikumar, Pradeep K. and Dhillon, Inderjit S},
  Journal                  = {Advances in Neural Information Processing Systems},
  Year                     = {2015},
  Pages                    = {3303--3311},

  Abstract                 = {Multilabel classification is rapidly developing as an important aspect of modern predictive modeling, motivating study of its theoretical aspects. To this end, we propose a framework for constructing and analyzing multilabel classification met-rics which reveals novel results on a parametric form for population optimal clas-sifiers, and additional insight into the role of label correlations. In particular, we show that for multilabel metrics constructed as instance-, micro-and macro-averages, the population optimal classifier can be decomposed into binary classi-fiers based on the marginal instance-conditional distribution of each label, with a weak association between labels via the threshold. Thus, our analysis extends the state of the art from a few known multilabel classification metrics such as Ham-ming loss, to a general framework applicable to many of the classification metrics in common use. Based on the population-optimal classifier, we propose a compu-tationally efficient and general-purpose plug-in classification algorithm, and prove its consistency with respect to the metric of interest. Empirical results on synthetic and benchmark datasets are supportive of our theoretical findings.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Koyejo et al/Advances in Neural Information Processing Systems/Koyejo et al. - 2015 - Consistent Multilabel Classification.pdf:pdf},
  ISSN                     = {10495258},
  Url                      = {http://papers.nips.cc/paper/5883-consistent-multilabel-classification}
}

@Article{Lee2017a,
  Title                    = {{SCLS: Multi-label feature selection based on scalable criterion for large label set}},
  Author                   = {Lee, Jaesung and Kim, Dae-Won},
  Journal                  = {Pattern Recognition},
  Year                     = {2017},
  Number                   = {August 2016},
  Pages                    = {342--352},
  Volume                   = {66},

  Abstract                 = {Multi-label feature selection involves the selection of relevant features from multi-labeled datasets, resulting in a potential improvement of multi-label learning accuracy. In conventional multi-label feature selection methods, the fi nal feature subset is obtained by identifying the features of high relevance with low redundancy. Thus, accurate score evaluation is a key factor for obtaining an e ff ective feature subset. However, conventional methods su ff er from inaccurate conditional relevance evaluation when a large number of labels are involved. As a result, irrelevant features can be a member of the fi nal feature subset, leading to low multi-label learning accuracy. In this paper, we propose a new multi-label feature selection method. Using a scalable relevance evaluation process that evaluates conditional relevance more accurately, the proposed method signi fi cantly improves multi-label learning accuracy compared with conventional multi-label feature selection methods.},
  Doi                      = {10.1016/j.patcog.2017.01.014},
  File                     = {:home/jan/Documents/Mendeley Desktop/Lee, Kim/Pattern Recognition/Lee, Kim - Unknown - SCLS Multi-label feature selection based on scalable criterion for large label set.pdf:pdf},
  ISSN                     = {00313203},
  Keywords                 = {Conditional relevance,Machine learning,Multi-label feature selection,Multi-label learning,Relevance evaluation},
  Publisher                = {Elsevier Ltd},
  Url                      = {http://ac.els-cdn.com.ez.sun.ac.za/S003132031730016X/1-s2.0-S003132031730016X-main.pdf?{\_}tid=72e6d1d6-1573-11e7-a49b-00000aacb361{\&}acdnat=1490897305{\_}4ff82d83a1296a46c537536304a7e929 http://linkinghub.elsevier.com/retrieve/pii/S003132031730016X}
}

@Article{Lee2015,
  Title                    = {{Fast multi-label feature selection based on information-theoretic feature ranking}},
  Author                   = {Lee, Jaesung and Kim, Dae-Won},
  Journal                  = {Pattern Recognition},
  Year                     = {2015},
  Pages                    = {2761--2771},
  Volume                   = {48},

  Abstract                 = {a b s t r a c t Multi-label feature selection involves selecting important features from multi-label data sets. This can be achieved by ranking features based on their importance and then selecting the top-ranked features. Many multi-label feature selection methods for finding a feature subset that can improve multi-label learning accuracy have been proposed. In contrast, computationally efficient multi-label feature selection methods have not been studied extensively. In this study, we propose a fast multi-label feature selection method based on information-theoretic feature ranking. Experimental results demon-strate that the proposed method generates a feature subset significantly faster than several other multi-label feature selection methods for large multi-label data sets.},
  Doi                      = {10.1016/j.patcog.2015.04.009},
  File                     = {:home/jan/Documents/Mendeley Desktop/Lee, Kim/Pattern Recognition/Lee, Kim - 2015 - Fast multi-label feature selection based on information-theoretic feature ranking.pdf:pdf},
  Keywords                 = {Entropy,Interaction information,Multi-label feature selection,Mutual information},
  Url                      = {http://ac.els-cdn.com.ez.sun.ac.za/S0031320315001338/1-s2.0-S0031320315001338-main.pdf?{\_}tid=1581c368-15e9-11e7-964a-00000aab0f01{\&}acdnat=1490947829{\_}7fc74eb90a545073fe28ca290654e346}
}

@Article{Lewis2004,
  Title                    = {{RCV1: A New Benchmark Collection for Text Categorization Research}},
  Author                   = {Lewis, David D and Yang, Yiming and Rose, Tony G and Li, Fan and Lewis, Fan Li},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2004},
  Pages                    = {361--397},
  Volume                   = {5},

  Abstract                 = {Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters doc-umentation, we describe the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as RCV1-v1, and the corrected data as RCV1-v2. We benchmark several widely used supervised learning methods on RCV1-v2, illus-trating the collection's properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Lewis et al/Journal of Machine Learning Research/Lewis et al. - 2004 - RCV1 A New Benchmark Collection for Text Categorization Research.pdf:pdf},
  Keywords                 = {Rocchio,SCut,SCutFBR,SVMs,applications,automated indexing,controlled vocabulary indexing,effectiveness mea-sures,evaluation,feature selection,k-NN,methodology,multiclass,multilabel,nearest neighbor,news articles,operational systems,support vector machines,term weighting,test collection,text classification,thresholding},
  Url                      = {http://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf}
}

@Article{Li2013,
  Title                    = {{Multi-label ensemble based on variable pairwise constraint projection}},
  Author                   = {Li, Ping and Li, Hong and Wu, Min},
  Journal                  = {Information Sciences},
  Year                     = {2013},
  Pages                    = {269--281},
  Volume                   = {222},

  Abstract                 = {Multi-label classification has attracted an increasing amount of attention in recent years. To this end, many algorithms have been developed to classify multi-label data in an effective manner. However, they usually do not consider the pairwise relations indicated by sample labels, which actually play important roles in multi-label classification. Inspired by this, we naturally extend the traditional pairwise constraints to the multi-label scenario via a flexible thresholding scheme. Moreover, to improve the generalization ability of the classifier, we adopt a boosting-like strategy to construct a multi-label ensemble from a group of base classifiers. To achieve these goals, this paper presents a novel multi-label classification framework named Variable Pairwise Constraint projection for Multi-label Ensemble (VPCME). Specifically, we take advantage of the variable pairwise constraint projection to learn a lower-dimensional data representation, which preserves the correlations between samples and labels. Thereafter, the base classifiers are trained in the new data space. For the boosting-like strategy, we employ both the variable pairwise constraints and the bootstrap steps to diversify the base classifiers. Empirical studies have shown the superiority of the proposed method in comparison with other approaches. ?? 2012 Elsevier Inc. All rights reserved.},
  Doi                      = {10.1016/j.ins.2012.07.066},
  File                     = {:home/jan/Documents/Mendeley Desktop/Li, Li, Wu/Information Sciences/Li, Li, Wu - 2013 - Multi-label ensemble based on variable pairwise constraint projection.pdf:pdf},
  ISSN                     = {00200255},
  Keywords                 = {Boosting,Constraint projection,Ensemble learning,Multi-Label,Multi-label classification,Variable pairwise constraints},
  Mendeley-tags            = {Multi-Label}
}

@Article{Lim2017,
  Title                    = {{Optimization approach for feature selection in multi-label classification}},
  Author                   = {Lim, Hyunki and Lee, Jaesung and Kim, Dae-Won},
  Journal                  = {Pattern Recognition Letters},
  Year                     = {2017},
  Pages                    = {25--30},
  Volume                   = {89},

  Abstract                 = {a b s t r a c t Nowadays, many data sources that include multi-label learning and multi-label classification have emerged in recent application areas. To achieve high classification accuracy, the multi-label feature selec-tion method has received much attention because its accuracy can be significantly improved by selecting important features. In previous multi-label feature selection studies, a score function was designed based on the measure of the dependency between features and labels. However, identifying the optimal feature subset is an impractical task because all possible feature subsets are 2 N , where N is the number of total features in a given dataset. Thus, the conventional methods utilized a greedy search approach that can be stuck in local optima. To circumvent the drawback of the greedy approaches, we design a score function based on mutual information and present a numerical optimization approach to avoid being stuck in the local optima. The experimental results demonstrate the superiority of the proposed multi-label feature selection method.},
  Doi                      = {10.1016/j.patrec.2017.02.004},
  File                     = {:home/jan/Documents/Mendeley Desktop/Lim, Lee, Kim/Pattern Recognition Letters/Lim, Lee, Kim - 2017 - Optimization approach for feature selection in multi-label classification.pdf:pdf},
  Keywords                 = {Multi-label feature selection,Mutual information,Numerical optimization},
  Url                      = {http://ac.els-cdn.com.ez.sun.ac.za/S016786551730034X/1-s2.0-S016786551730034X-main.pdf?{\_}tid=24f68d72-1528-11e7-9527-00000aacb35d{\&}acdnat=1490864962{\_}d69b92b5449c6c0acc46c9a4c4c63940}
}

@Article{Liu2015,
  Title                    = {{A multi-label classification based approach for sentiment classification}},
  Author                   = {Liu, Shuhua Monica and Chen, Jiun-Hung},
  Journal                  = {Expert Systems with Applications},
  Year                     = {2015},
  Number                   = {3},
  Pages                    = {1083--1093},
  Volume                   = {42},

  Abstract                 = {A multi-label classification based approach for sentiment analysis is proposed in this paper. To the best of our knowledge, this work is the first to propose to use multi-label classification for sentiment classification of microblogs. The proposed prototype has three main components, text segmentation, feature extraction, and multi-label classification. Raw segmented words and sentiment features based on the three different sentiment dictionaries, Dalian University of Technology Sentiment Dictionary, National Taiwan University Sentiment Dictionary and HowNet Dictionary, are the features and the bag of words is the feature representation. A detailed empirical study of different multi-label classification methods on sentiment classification is conducted to compare their classification performances. Specifically, total 11 state of the art multi-label classification methods are compared on two microblog datasets and 8 evaluation metrics are used. The effects of the three sentiment dictionaries for multi-label classification are empirically studied and compared, which, to the best of our knowledge, have not been performed. The performed empirical comparisons show that Dalian University of Technology Sentiment Dictionary has the best performance among the three different sentiment dictionaries.},
  Doi                      = {10.1016/j.eswa.2014.08.036},
  File                     = {:home/jan/Documents/Mendeley Desktop/Liu, Chen/Expert Systems with Applications/Liu, Chen - 2015 - A multi-label classification based approach for sentiment classification.pdf:pdf},
  ISSN                     = {09574174},
  Keywords                 = {Microblogs,Multi-Label,Multi-label classification,Sentiment Classification,Sentiment analysis},
  Mendeley-tags            = {Multi-Label,Sentiment Classification},
  Publisher                = {Elsevier Ltd},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957417414005181}
}

@Article{Lo2013,
  Title                    = {{Generalized k-Labelsets Ensemble for Multi-Label and Cost-Sensitive Classification}},
  Author                   = {Lo, Hung-Yi and Lin, Shou-De and Wang, Hsin-Min},
  Journal                  = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING},
  Year                     = {2013},
  Number                   = {X},

  Abstract                 = {—Label powerset (LP) method is one category of multi-label learning algorithm. This paper presents a basis expansions model for multi-label classification, where a basis function is a LP classifier trained on a random k-labelset. The expansion coefficients are learned to minimize the global error between the prediction and the ground truth. We derive an analytic solution to learn the coefficients efficiently. We further extend this model to handle the cost-sensitive multi-label classification problem, and apply it in social tagging to handle the issue of the noisy training set by treating the tag counts as the misclassification costs. We have conducted experiments on several benchmark datasets and compared our method with other state-of-the-art multi-label learning methods. Experimental results on both multi-label classification and cost-sensitive social tagging demonstrate that our method has better performance than other methods.},
  Doi                      = {10.1109/TKDE.2013.112},
  File                     = {:home/jan/Documents/Mendeley Desktop/Lo, Lin, Wang/IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING/Lo, Lin, Wang - 2013 - Generalized k-Labelsets Ensemble for Multi-Label and Cost-Sensitive Classification.pdf:pdf},
  Keywords                 = {Index Terms—Multi-label classification,cost-sensitive learning,ensemble method,hypergraph,labelset,social tag,tag count}
}

@Article{Luaces,
  Title                    = {{Binary Relevance Efficacy for Multilabel Classification}},
  Author                   = {Luaces, Oscar and D{\'{i}}ez, Jorge and Barranquero, Jos{\'{e}} and {Del Coz}, Juan Jos{\'{e}} and Bahamonde, Antonio},

  Abstract                 = {The goal of multilabel (ML) classification is to induce models able to tag objects with the labels that better describe them. The main baseline for ML classi-fication is Binary Relevance (BR), which is commonly criticized in the literature because of its label inde-pendence assumption. Despite this fact, this paper dis-cusses some interesting properties of BR, mainly that it produces optimal models for several ML loss functions. Additionally, we present an analytical study about ML benchmarks datasets, pointing out some shortcomings. As a result, this paper proposes the use of synthetic datasets to better analyze the behavior of ML meth-ods in domains with different characteristics. To sup-port this claim, we perform some experiments using synthetic data proving the competitive performance of BR with respect to a more complex method in difficult problems with many labels, a conclusion which was not stated by previous studies.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Luaces et al/Unknown/Luaces et al. - Unknown - Binary Relevance Efficacy for Multilabel Classification.pdf:pdf},
  Keywords                 = {Binary,Classification {\textperiodcentered},Label dependency,Multilabel,Rele-vance {\textperiodcentered},Synthetic datasets {\textperiodcentered}}
}

@Article{Madjarov2011,
  Title                    = {{Two Stage Architecture for Multi - label Learning}},
  Author                   = {Madjarov, Gjorgji and Gjorgjevikj, Dejan and D{\v{z}}eroski, Sa{\v{s}}o},
  Year                     = {2011},
  Pages                    = {1--24},
  Volume                   = {00},

  Abstract                 = {See, stats, and : https : / / www . researchgate . net / publication / 232630058 Two - label Article DOI : 10 . 1016 / j . patcog . 2011 . 08 . 011 : DBLP CITATIONS 13 READS 99 3 , including : Gjorgji Ss . Cyril 31 SEE Dejan Ss . Cyril 57 SEE All . The . All - text and , letting . Abstract A common approach to solving multi - label learning problems is to use problem transformation methods and dichotomizing classifiers as in the pair - wise decomposition strategy . One of the problems with this strategy is the need for querying a quadratic number of binary classifiers for making a prediction that can be quite time consuming , especially in learning problems with a large number of labels . To tackle this problem , we propose a Two Stage Architecture (TSA) for efficient multi - label learning . We analyze three implementations of this architecture the Two Stage Voting Method (TSVM) , the Two Stage Classifier Chain Method (TSCCM) and the Two Stage Pruned Classifier Chain Method (TSPCCM) . Eight different real - world datasets are used to evaluate the performance of the proposed methods . The performance of our approaches is compared with the performance of two algorithm adaptation methods (Multi - Label k - NN and Multi - Label C4 . 5) and five problem transformation methods (Binary Relevance , Classifier Chain , Calibrated Label Ranking with majority voting , the Quick Weighted method for pair - wise multi - label learning and the Label Powerset method) . The results suggest that TSCCM and TSPCCM outperform the competing algorithms in terms of predictive accuracy , while TSVM has comparable predictive performance . In terms of testing speed , all three methods show better performance as compared to the pair - wise methods for multi - label learning .},
  Doi                      = {10.1016/j.patcog.2011.08.011},
  File                     = {:home/jan/Documents/Mendeley Desktop/Madjarov, Gjorgjevikj, D{\v{z}}eroski/Unknown/Madjarov, Gjorgjevikj, D{\v{z}}eroski - 2011 - Two Stage Architecture for Multi - label Learning.pdf:pdf},
  Keywords                 = {classifier chain,multi - label classification,multi - label learning,multi - label ranking,two stage architecture}
}

@Article{Madjarov2012,
  Title                    = {{An extensive experimental comparison of methods for multi-label learning}},
  Author                   = {Madjarov, Gjorgji and Kocev, Dragi and Gjorgjevikj, Dejan and D{\v{z}}eroski, Sa{\v{s}}o},
  Year                     = {2012},

  Abstract                 = {This article appeared in a journal published by Elsevier. The attached copy is furnished to the author for internal non-commercial research and education use, including for instruction at the authors institution and sharing with colleagues. Other uses, including reproduction and distribution, or selling or licensing copies, or posting to personal, institutional or third party websites are prohibited. In most cases authors are permitted to post their version of the article (e.g. in Word or Tex form) to their personal website or institutional repository. Authors requiring further information regarding Elsevier's archiving and manuscript policies are encouraged to visit: a b s t r a c t Multi-label learning has received significant attention in the research community over the past few years: this has resulted in the development of a variety of multi-label learning methods. In this paper, we present an extensive experimental comparison of 12 multi-label learning methods using 16 evaluation measures over 11 benchmark datasets. We selected the competing methods based on their previous usage by the community, the representation of different groups of methods and the variety of basic underlying machine learning methods. Similarly, we selected the evaluation measures to be able to assess the behavior of the methods from a variety of view-points. In order to make conclusions independent from the application domain, we use 11 datasets from different domains. Furthermore, we compare the methods by their efficiency in terms of time needed to learn a classifier and time needed to produce a prediction for an unseen example. We analyze the results from the experiments using Friedman and Nemenyi tests for assessing the statistical significance of differences in performance. The results of the analysis show that for multi-label classification the best performing methods overall are random forests of predictive clustering trees (RF-PCT) and hierarchy of multi-label classifiers (HOMER), followed by binary relevance (BR) and classifier chains (CC). Furthermore, RF-PCT exhibited the best performance according to all measures for multi-label ranking. The recommendation from this study is that when new methods for multi-label learning are proposed, they should be compared to RF-PCT and HOMER using multiple evaluation measures.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Madjarov et al/Unknown/Madjarov et al. - 2012 - Author's personal copy An extensive experimental comparison of methods for multi-label learning.pdf:pdf},
  Keywords                 = {Comparison of multi-label learning methods,Multi-label classification,Multi-label ranking},
  Url                      = {http://www.elsevier.com/copyright}
}

@Article{Marchetti2007,
  Title                    = {{SemKey : A Semantic Collaborative Tagging System}},
  Author                   = {Marchetti, Andrea and Rosella, Marco},
  Journal                  = {Proceedings of the 16th international conference on World Wide Web - WWW '07},
  Year                     = {2007},
  Pages                    = {8--12},
  Volume                   = {7},

  Abstract                 = {By analysing the current structure and the usage patterns of collaborative tagging systems, we can ﬁnd out many im- portant aspects which still need to be improved. Problems related to synonymy, polysemy, diﬀerent lexical forms, mis- pelling errors or alternate spellings, diﬀerent levels of preci- sion and diﬀerent kinds of tag-to-resource association cause inconsistencies and reduce the eﬃciency of content search and the eﬀectiveness of the tag space structuring and orga- nization. They are mainly caused by the lack of semantic information inclusion in the tagging process. We propose a new way to describe resources: the semantic tagging. It allows user to state semantic assertions: each of them ex- presses a deﬁned characteristic of a resource associating it with a concept. We present SemKey, a semantic collabora- tive tagging system, describing its global architecture and functioning along with the most relevant organizational is- sues faced. We explore the adequacy of the support oﬀered by the entries of Wikipedia and WordNet in order to access to and reference concepts.},
  Annote                   = {Discusses the limitations of collaborative tagging},
  File                     = {:home/jan/Documents/Mendeley Desktop/Marchetti, Rosella/Proceedings of the 16th international conference on World Wide Web - WWW '07/Marchetti, Rosella - 2007 - SemKey A Semantic Collaborative Tagging System.pdf:pdf},
  Keywords                 = {19,are,collaborative tagging,in groups with common,interests,resources and tags,resources may be related,semantics,users,users may be connected}
}

@Article{McCallum1999a,
  Title                    = {{Multi-label text classification with a mixture model trained by EM}},
  Author                   = {McCallum, Andrew},
  Journal                  = {AAAI'99 Workshop on Text Learning},
  Year                     = {1999},
  Pages                    = {1--7},

  Abstract                 = {In many important document classication tasks, documents may each be associated with multiple class labels. This paper describes a Bayesian classication approach in which the multiple classes that comprise a document are represented by a mixture model. While the labeled training data indicates which classes were responsible for generating a document, it does not indicate which class was responsible for generating each word. Thus we use EM to ll in this missing value, learning both the...},
  Doi                      = {10.1.1.35.888},
  File                     = {:home/jan/Documents/Mendeley Desktop/McCallum/AAAI'99 Workshop on Text Learning/McCallum - 1999 - “Multi-label text classification with a mixture model trained by EM.pdf:pdf},
  Url                      = {http://www.kyriakides.net/CBCL/references/Papers/mccallum99multilabel.pdf}
}

@InProceedings{McCallum1999,
  Title                    = {{Multi-label text classification with a mixture model trained by EM}},
  Author                   = {McCallum, Andrew Kachites},
  Booktitle                = {AAAI'99 Workshop on Text Learning},
  Year                     = {1999},
  Pages                    = {1--7},

  Abstract                 = {In many important document classication tasks, documents may each be associated with multiple class labels. This paper describes a Bayesian classication approach in which the multiple classes that comprise a document are represented by a mixture model. While the labeled training data indicates which classes were responsible for generating a document, it does not indicate which class was responsible for generating each word. Thus we use EM to ll in this missing value, learning both the...},
  Doi                      = {10.1.1.35.888},
  File                     = {:home/jan/Documents/Mendeley Desktop/McCallum/AAAI'99 Workshop on Text Learning/McCallum - 1999 - “Multi-label text classification with a mixture model trained by EM.pdf:pdf},
  Url                      = {http://www.kyriakides.net/CBCL/references/Papers/mccallum99multilabel.pdf http://www.eecs.yorku.ca/course{\_}archive/2005-06/F/6002B/Readings/multilabel.pdf}
}

@InProceedings{Monta??es2014,
  Title                    = {{Dependent binary relevance models for multi-label classification}},
  Author                   = {Monta??es, Elena and Senge, Robin and Barranquero, Jose and {Ram??n Quevedo}, Jos?? and {Jos?? Del Coz}, Juan and H??llermeier, Eyke},
  Booktitle                = {Pattern Recognition},
  Year                     = {2014},
  Number                   = {3},
  Pages                    = {1494--1508},
  Volume                   = {47},

  Abstract                 = {Several meta-learning techniques for multi-label classification (MLC), such as chaining and stacking, have already been proposed in the literature, mostly aimed at improving predictive accuracy through the exploitation of label dependencies. In this paper, we propose another technique of that kind, called dependent binary relevance (DBR) learning. DBR combines properties of both, chaining and stacking. We provide a careful analysis of the relationship between these and other techniques, specifically focusing on the underlying dependency structure and the type of training data used for model construction. Moreover, we offer an extensive empirical evaluation, in which we compare different techniques on MLC benchmark data. Our experiments provide evidence for the good performance of DBR in terms of several evaluation measures that are commonly used in MLC. ?? 2013 Elsevier Ltd. All rights reserved.},
  Doi                      = {10.1016/j.patcog.2013.09.029},
  File                     = {:home/jan/Documents/Mendeley Desktop/Montaes et al/Pattern Recognition/Unknown - Unknown - Dependent binary relevance models for multi-label classification.pdf:pdf},
  ISSN                     = {00313203},
  Keywords                 = {Chaining,Label dependence,Multi-label classification,Stacking},
  Url                      = {http://ac.els-cdn.com.ez.sun.ac.za/S0031320313004019/1-s2.0-S0031320313004019-main.pdf?{\_}tid=37459642-16bf-11e7-b2fb-00000aab0f6b{\&}acdnat=1491039798{\_}e130263764b6a9af87e0d2e25ee5285e}
}

@Article{Karalas,
  Title                    = {{Land Classification Using Remotely Sensed Data: Going Multilabel}},
  Author                   = {Multi-label, Going and Karalas, Konstantinos and Tsagkatakis, Grigorios and Zervakis, Michalis and Tsakalides, Panagiotis},
  Journal                  = {IEEE Transactions on Geoscience and Remote Sensing},
  Year                     = {2016},
  Number                   = {6},
  Pages                    = {3548--3563},
  Volume                   = {54},

  Abstract                 = {Obtaining an up-to-date high-resolution description of land cover is a challenging task due to the high cost and labor-intensive process of human annotation through field studies. This work introduces a radically novel approach for achieving this goal by exploiting the proliferation of remote sensing satellite imagery, allowing for the up-to-date generation of global-scale land cover maps. We propose the application of multilabel classification, a powerful framework in machine learning, for inferring the complex relationships between the acquired satellite images and the spectral profiles of different types of surface materials. Introducing a drastically different approach compared to unsupervised spectral unmixing, we employ contemporary ground-collected data from the European Environment Agency to generate the label set and multispectral images from the MODIS sensor to generate the spectral features, under a supervised classification framework. To validate the merits of our approach, we present results using several state-of-the-art multilabel learning classifiers and evaluate their predictive performance with respect to the number of annotated training examples, as well as their capability to exploit examples from neighboring regions or different time instances. We also demonstrate the application of our method on hyperspectral data from the Hyperion sensor for the urban land cover estimation of New York City. Experimental results suggest that the proposed framework can achieve excellent prediction accuracy, even from a limited number of diverse training examples, surpassing state-of-the-art spectral unmixing methods.},
  Doi                      = {10.1109/TGRS.2016.2520203},
  File                     = {:home/jan/Documents/Mendeley Desktop/Multi-label et al/IEEE Transactions on Geoscience and Remote Sensing/Karalas et al. - Unknown - Land Classification Using Remotely Sensed Data Going Multi-Label.pdf:pdf},
  ISBN                     = {0196-2892 VO - 54},
  ISSN                     = {01962892},
  Keywords                 = {CORINE,Europe,Hyperspectral imaging,MODIS,Satellites,Spatial resolution,data processing,land cover,pattern classification,remote sensing,satellite applications,time series,unmixing},
  Url                      = {http://users.isc.tuc.gr/{~}kkaralas/TGRS{\_}2016.pdf}
}

@Article{Ng2015,
  Title                    = {{Beyond Short Snippets : Deep Networks for Video Classification}},
  Author                   = {Ng, Joe Yue-Hei},
  Year                     = {2015},

  Abstract                 = {Beyond Short Snippets: Deep Networks for Video Classification Joe Yue-Hei Ng 1 yhng@umiacs.umd.edu Matthew Hausknecht 2 mhauskn@cs.utexas.edu Sudheendra Vijayanarasimhan 3 svnaras@google.com Oriol Vinyals 3 vinyals@google.com Rajat Monga 3 rajatmonga@google.com George Toderici 3 gtoderici@google.com 1 University of Maryland, College Park 2 University of Texas at Austin 3 Google, Inc. Abstract Convolutional neural networks (CNNs) have been exten- sively applied for image recognition problems giving state- of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image infor- mation across a video over longer time periods than previ- ously attempted. We propose two methods capable of han- dling full length videos. The first method explores various convolutional temporal feature pooling architectures, ex- amining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improve- ments over previously published results on the Sports 1 mil- lion dataset (73.1{\%} vs. 60.9{\%}) and the UCF-101 datasets with (88.6{\%} vs. 88.0{\%}) and without additional optical flow information (82.6{\%} vs. 73.0{\%})},
  Archiveprefix            = {arXiv},
  Arxivid                  = {arXiv:1503.08909v2},
  Doi                      = {10.1109/CVPR.2015.7299101},
  Eprint                   = {arXiv:1503.08909v2},
  File                     = {:home/jan/Documents/Mendeley Desktop/Ng/Unknown/Ng - Unknown - Beyond Short Snippets Deep Networks for Video Classification.pdf:pdf},
  ISBN                     = {9781467369640},
  ISSN                     = {10636919},
  Url                      = {https://pdfs.semanticscholar.org/57ed/4de2c8ea9c865dcf4273f0576eb746263475.pdf?{\_}ga=1.106149697.379343330.1490351020}
}

@Article{Pachet2009,
  Title                    = {{Improving Multilabel Analysis of Music Titles: A Large-Scale Validation of the Correction Approach}},
  Author                   = {Pachet, Fran{\c{c}}ois and Roy, Pierre},
  Journal                  = {IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING},
  Year                     = {2009},
  Number                   = {2},
  Volume                   = {17},

  Abstract                 = {—This paper addresses the problem of automatically ex-tracting perceptive information from acoustic signals, in a super-vised classification context. Global labels, i.e., atomic information describing a music title in its entirety, such as its genre, mood, main instruments, or type of vocals, are entered by humans. Classifiers are trained to map audio features to these labels. However, the per-formances of these classifiers on individual labels are rarely satis-factory. In the case we have to predict several labels simultane-ously, we introduce a correction scheme to improve these perfor-mances. In this scheme—an instance of the classifier fusion par-adigm—an extra layer of classifiers is built to exploit redundan-cies between labels and correct some of the errors coming from the individual acoustic classifiers. We describe a series of experi-ments aiming at validating this approach on a large-scale database of music and metadata (about 30 000 titles and 600 labels per title). The experiments show that the approach brings statistically signif-icant improvements.},
  Doi                      = {10.1109/TASL.2008.2008734},
  File                     = {:home/jan/Documents/Mendeley Desktop/Pachet, Roy/IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING/Pachet, Roy - 2009 - Improving Multilabel Analysis of Music Titles A Large-Scale Validation of the Correction Approach.pdf:pdf},
  Keywords                 = {Index Terms—Feature extraction,learning systems,music,pat-tern classification}
}

@Article{Papanikolaou2014,
  Title                    = {{Ensemble approaches for large-scale multi-label classification and question answering in biomedicine}},
  Author                   = {Papanikolaou, Yannis and Dimitriadis, Dimitrios and Tsoumakas, Grigorios and Laliotis, Manos and Markantonatos, Nikos and Vlahavas, Ioannis},
  Journal                  = {CEUR Workshop Proceedings},
  Year                     = {2014},
  Pages                    = {1348--1360},
  Volume                   = {1180},

  Abstract                 = {This paper documents the systems that we developed for our participation in the BioASQ 2014 large-scale bio-medical semantic indexing and question answering challenge. For the large-scale semantic indexing task, we employed a novel multi-label ensemble method con-sisting of support vector machines, labeled Latent Dirichlet Allocation models and meta-models predicting the number of relevant labels. This method proved successful in our experiments as well as during the compe-tition. For the question answering task we combined different techniques for scoring of candidate answers based on recent literature.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Papanikolaou et al/CEUR Workshop Proceedings/Papanikolaou et al. - 2014 - Ensemble approaches for large-scale multi-label classification and question answering in biomedicine.pdf:pdf},
  ISSN                     = {16130073},
  Keywords                 = {BioASQ,Ensemble methods,Latent Dirichlet Allocation,Multi-label learning,Support vector machines}
}

@Article{Papanikolaoua,
  Title                    = {{Hierarchical Partitioning of the Output Space in Multi-label Data}},
  Author                   = {Papanikolaou, Yannis and Katakis, Ioannis and Tsoumakas, Grigorios},

  Abstract                 = {—Hierarchy Of Multi-label classifiERs (HOMER) is a multi-label learning algorithm that breaks the initial learning task to several, easier sub-tasks by first constructing a hierarchy of labels from a given label set and secondly employing a given base multi-label classifier (MLC) to the resulting sub-problems. The primary goal is to effectively address class imbalance and scalability issues that often arise in real-world multi-label classification problems. In this work, we present the general setup for a HOMER model and a simple extension of the algorithm that is suited for MLCs that output rankings. Furthermore, we provide a detailed analysis of the properties of the algorithm, both from an aspect of effectiveness and computational complexity. A secondary contribution involves the presentation of a balanced variant of the k means algorithm, which serves in the first step of the label hierarchy construction. We conduct extensive experiments on six real-world datasets, studying empirically HOMER's parameters and providing examples of instantiations of the algorithm with different clustering approaches and MLCs, The empirical results demonstrate a significant improvement over the given base MLC.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Papanikolaou, Katakis, Tsoumakas/Unknown/Papanikolaou, Katakis, Tsoumakas - Unknown - Hierarchical Partitioning of the Output Space in Multi-label Data.pdf:pdf},
  Keywords                 = {Index Terms—Knowledge discovery,Machine learning,Supervised learning,Text mining ✦},
  Url                      = {https://arxiv.org/pdf/1612.06083.pdf}
}

@InProceedings{Qi,
  Title                    = {{Correlative multi-label video annotation}},
  Author                   = {Qi, Guo-Jun and Hua, Xian-Sheng and Rui, Yong and Tang, Jinhui and Mei, Tao and Zhang, Hong-Jiang},
  Booktitle                = {Proceedings of the 15th international conference on Multimedia - MULTIMEDIA '07},
  Year                     = {2007},
  Pages                    = {17},

  Abstract                 = {Automatically annotating concepts for video is a key to semantic-level video browsing, search and navigation. The research on this topic evolved through two paradigms. The first paradigm used binary classification to detect each in-dividual concept in a concept set. It achieved only limited success, as it did not model the inherent correlation between concepts, e.g., urban and building. The second paradigm added a second step on top of the individual-concept de-tectors to fuse multiple concepts. However, its performance varies because the errors incurred in the first detection step can propagate to the second fusion step and therefore de-grade the overall performance. To address the above issues, we propose a third paradigm which simultaneously classi-fies concepts and models correlations between them in a single step by using a novel Correlative Multi-Label (CML) framework. We compare the performance between our pro-posed approach and the state-of-the-art approaches in the first and second paradigms on the widely used TRECVID data set. We report superior performance from the proposed approach.},
  Annote                   = {NULL},
  Doi                      = {10.1145/1291233.1291245},
  File                     = {:home/jan/Documents/Mendeley Desktop/Qi et al/Proceedings of the 15th international conference on Multimedia - MULTIMEDIA '07/Qi et al. - Unknown - Correlative Multi-Label Video Annotation.pdf:pdf},
  ISBN                     = {9781595937025},
  ISSN                     = {15516857},
  Keywords                 = {Concept Correlation,Content Analysis and Indexing-indexing methods,Experimentation Keywords Video Annotation,I210 [Artificial Intelligence],Multi-Labeling,Theory},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.88.4803{\&}rep=rep1{\&}type=pdf http://portal.acm.org/citation.cfm?doid=1291233.1291245}
}

@Article{Qiao2017,
  Title                    = {{Selecting Label-dependent Features for Multi-label Classification}},
  Author                   = {Qiao, Lishan and Zhang, Limei and Sun, Zhonggui and Liu, Xueyan},
  Journal                  = {Neurocomputing},
  Year                     = {2017},

  Abstract                 = {JID: NEUCOM [m5G; February 14, 2017;20:37 ] Neurocomputing 0 0 0 (2017) 1–7 a b s t r a c t An instance is often represented from different aspects (views or modalities), which leads to high-dimensional features and even multiple labels. In this paper, we focus on the feature selection problem in multi-label classification, for which a trivial solution is handling the labels dividedly. Obviously, such a scheme may not work well by leaving the label relationship out of consideration. Recently, several re-search works conduct feature selection directly under a multi-label framework by implicitly or explicitly modeling label relationship. However, these works assume that all labels share the same feature subset or subspace, which is not reasonable enough for some scenarios since different labels tend to convey different semantics. To address this problem, we develop a novel approach in this paper to select label-dependent features for multi-label classification. Specifically, we (1) formulate a convex model based on a more general and practical assumption that different labels convey different semantics with specific fea-tures; (2) design an alternating optimization algorithm based on Nesterov's method and L 1 -ball projection for efficiently finding the optimal solution, which can realize multi-label classification, feature selection, and label relationship estimation simultaneously. Finally, experiments on publicly available datasets show that the proposed algorithm achieves better performance than several related methods.},
  Doi                      = {10.1016/j.neucom.2016.08.122},
  File                     = {:home/jan/Documents/Mendeley Desktop/Qiao et al/Neurocomputing/Qiao et al. - 2016 - ARTICLE IN PRESS Selecting label-dependent features for multi-label classification.pdf:pdf},
  ISSN                     = {09252312},
  Keywords                 = {Feature selection,Label dependency,Multi-label classification,Nesterov's method},
  Url                      = {http://ac.els-cdn.com.ez.sun.ac.za/S0925231217302321/1-s2.0-S0925231217302321-main.pdf?{\_}tid=accb469e-15f9-11e7-ba35-00000aab0f27{\&}acdnat=1490954955{\_}6f85d9237b965dfa7948a47fde97b589 http://linkinghub.elsevier.com/retrieve/pii/S0925231217302321}
}

@Article{Read2008,
  Title                    = {{A pruned problem transformation method for multi-label classification}},
  Author                   = {Read, J},
  Journal                  = {New Zealand Computer Science Research Student Conference, NZCSRSC 2008 - Proceedings},
  Year                     = {2008},
  Number                   = {April},
  Pages                    = {143--150},

  Abstract                 = {Multi-label classification has gained significant interest in recent years, paralleled by the increasing use of manual multi-labelling, often known as applying "tags" to documents. Well known examples include Flickr 1, YouTube2, CiteULike3 and Google Bookmarks4. This paper focuses on Problem Transformation (PT) as an approach to multi-label classification and details these methods as well as their respective advantages and disadvantages. A Pruned Problem Transformation method (PPT) is presented, along with several extensions, designed to overcome such disadvantages. This new method is empirically compared with existing methods, both in terms of accuracy and training time, and the results are encouraging.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Read/New Zealand Computer Science Research Student Conference, NZCSRSC 2008 - Proceedings/Read - 2008 - A pruned problem transformation method for multi-label classification.pdf:pdf},
  Keywords                 = {Computer science,Information retrieval systems,Multi-label classifications; Problem transformatio},
  Url                      = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84880106608{\&}partnerID=40{\&}md5=ccdd19f7d0f111a2fdda8df7af0a8075}
}

@Article{Reade,
  Title                    = {{Multi-label Classification using Labels as Hidden Nodes}},
  Author                   = {Read, Jesse and Hollm{\'{e}}n, Jaakko},
  Year                     = {2015},
  Pages                    = {1--23},

  Abstract                 = {Competitive methods for multi-label data typically invest in learning labels together. To do so in a beneficial way, analysis of label dependence is often seen as a fundamental step, separate and prior to constructing a classifier. Some methods invest up to hundreds of times more computational effort in building dependency models, than training the final classifier itself. We extend some recent discussion in the literature and provide a deeper analysis, namely, developing the view that label dependence is often introduced by an inadequate base classifier, rather than being inherent to the data or underlying concept; showing how even an exhaustive analysis of label dependence may not lead to an optimal classification structure. Viewing labels as additional features (a transformation of the input), we create neural-network inspired novel methods that remove the emphasis of a prior dependency structure. Our methods take an important advantage particular to multi-label data: they leverage labels to create effective units in middle layers, rather than learning these units from scratch in an unsupervised fashion with gradient-based methods. Results are promising. The methods we propose perform competitively, and also have very important qualities of scalability.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1503.09022},
  Eprint                   = {1503.09022},
  File                     = {:home/jan/Documents/Mendeley Desktop/Read, Hollm{\'{e}}n/Unknown/Read, Hollm{\'{e}}n - Unknown - Multi-label Classification using Labels as Hidden Nodes.pdf:pdf},
  Keywords                 = {meta-labels,multi-label classification,neural net-,problem transformation},
  Url                      = {https://arxiv.org/pdf/1503.09022.pdf http://arxiv.org/abs/1503.09022}
}

@Article{Read2014a,
  Title                    = {{A Deep Interpretation of Classifier Chains}},
  Author                   = {Read, Jesse and Hollmen, Jaakko},
  Journal                  = {Advances in Intelligent Data Analysis Xiii},
  Year                     = {2014},
  Pages                    = {251--262},
  Volume                   = {8819},

  Abstract                 = {In the ``classifier chains{\{}''{\}} (CC) approach for multi-label$\backslash$nclassification, the predictions of binary classifiers are cascaded along$\backslash$na chain as additional features. This method has attained high predictive$\backslash$nperformance, and is receiving increasing analysis and attention in the$\backslash$nrecent multi-label literature, although a deep understanding of its$\backslash$nperformance is still taking shape. In this paper, we show that CC gets$\backslash$npredictive power from leveraging labels as additional stochastic$\backslash$nfeatures, contrasting with many other methods, such as stacking and$\backslash$nerror correcting output codes, which use label dependence only as kind$\backslash$nof regularization. CC methods can learn a concept which these cannot,$\backslash$neven supposing the same base classifier and hypothesis space. This leads$\backslash$nus to connections with deep learning (indeed, we show that CC is$\backslash$ncompetitive precisely because it is a deep learner), and we employ deep$\backslash$nlearning methods -showing that they can supplement or even replace a$\backslash$nclassifier chain. Results are convincing, and throw new insight into$\backslash$npromising future directions.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Read, Hollmen/Advances in Intelligent Data Analysis Xiii/Read, Hollm{\'{e}}n - Unknown - A Deep Interpretation of Classifier Chains.pdf:pdf},
  ISBN                     = {978-3-319-12571-8; 978-3-319-12570-1},
  ISSN                     = {0302-9743},
  Url                      = {http://jmread.github.io/papers/Read, Holmen - A Deep Interpretation of Classifier Chains.pdf}
}

@Article{Readc,
  Title                    = {{Deep Learning for Multi-label Classification}},
  Author                   = {Read, Jesse and Perez-Cruz, Fernando},
  Journal                  = {arXiv preprint arXiv:1502.05988},
  Year                     = {2014},
  Pages                    = {1--8},

  Abstract                 = {In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1502.05988},
  Eprint                   = {1502.05988},
  File                     = {:home/jan/Documents/Mendeley Desktop/Read, Perez-Cruz/arXiv preprint arXiv1502.05988/Read, Perez-Cruz - Unknown - Deep Learning for Multi-label Classification.pdf:pdf},
  Url                      = {https://arxiv.org/pdf/1502.05988.pdf http://arxiv.org/abs/1502.05988}
}

@Article{Read2011,
  Title                    = {{Classifier chains for multi-label classification}},
  Author                   = {Read, Jesse and Pfahringer, Bernhard and Holmes, Geoff and Frank, Eibe and {Brodley Read}, Carla J and Pfahringer, Bernhard and Holmes, Geoff and Frank, Eibe},
  Journal                  = {Mach Learn},
  Year                     = {2011},
  Number                   = {85},
  Pages                    = {333--359},
  Volume                   = {85},

  Abstract                 = {The widely known binary relevance method for multi-label classification, which considers each label as an independent binary problem, has often been overlooked in the literature due to the perceived inadequacy of not directly modelling label correlations. Most current methods invest considerable complexity to model interdependencies between labels. This paper shows that binary relevance-based methods have much to offer, and that high predictive performance can be obtained without impeding scalability to large datasets. We exemplify this with a novel classifier chains method that can model label correlations while maintaining acceptable computational complexity. We extend this approach further in an ensemble framework. An extensive empirical evaluation covers a broad range of multi-label datasets with a variety of evaluation metrics. The results illustrate the competitiveness of the chaining method against related and state-of-the-art methods, both in terms of predictive performance and time complexity.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {arXiv:1207.6324},
  Doi                      = {10.1007/s10994-011-5256-5},
  Eprint                   = {arXiv:1207.6324},
  File                     = {:home/jan/Documents/Mendeley Desktop/Read et al/Machine Learning/Read et al. - 2011 - Classifier chains for multi-label classification(2).pdf:pdf;:home/jan/Documents/Mendeley Desktop/Read et al/Mach Learn/Read et al. - 2011 - Classifier chains for multi-label classification.pdf:pdf},
  ISBN                     = {9783642041730},
  ISSN                     = {08856125},
  Keywords                 = {Ensemble methods,Ensemble methods {\textperiodcentered},Multi-label classification,Multi-label classification {\textperiodcentered},Problem transformation,Problem transformation {\textperiodcentered},Scalable methods},
  Pmid                     = {22183238},
  Url                      = {http://download.springer.com/static/pdf/44/art{\%}253A10.1007{\%}252Fs10994-011-5256-5.pdf?originUrl=http{\%}3A{\%}2F{\%}2Flink.springer.com{\%}2Farticle{\%}2F10.1007{\%}2Fs10994-011-5256-5{\&}token2=exp=1490608886{~}acl={\%}2Fstatic{\%}2Fpdf{\%}2F44{\%}2Fart{\%}25253A10.1007{\%}25252Fs10994-011-5256-}
}

@Article{Read2014,
  Title                    = {{Multi-label Classification with Meta-Labels}},
  Author                   = {Read, Jesse and Puurula, Antti and Bifet, Albert},
  Journal                  = {2014 IEEE International Conference on Data Mining},
  Year                     = {2014},
  Pages                    = {941--946},

  Abstract                 = {—The area of multi-label classification has rapidly developed in recent years. It has become widely known that the baseline binary relevance approach can easily be outperformed by methods which learn labels together. A number of methods have grown around the label powerset approach, which models label combinations together as class values in a multi-class problem. We describe the label-powerset-based solutions under a general framework of meta-labels and provide some theoret-ical justification for this framework which has been lacking; explaining how meta-labels essentially allow a random projection into a space where non-linearities can easily be tackled with established linear learning algorithms. The proposed framework enables comparison and combination of related approaches to different multi-label problems. We present a novel model in the framework and evaluate it empirically against several high-performing methods, with respect to predictive performance and scalability, on a number of datasets and evaluation metrics. This deployment obtains competitive accuracy for a fraction of the computation required by the current meta-label methods for multi-label classification.},
  Doi                      = {10.1109/ICDM.2014.38},
  File                     = {:home/jan/Documents/Mendeley Desktop/Read, Puurula, Bifet/2014 IEEE International Conference on Data Mining/Read, Puurula, Bifet - 2014 - Multi-label Classification with Meta-Labels.pdf:pdf},
  ISBN                     = {978-1-4799-4302-9},
  Url                      = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7023427}
}

@Article{Redmon,
  Title                    = {{YOLO9000: Better, Faster, Stronger}},
  Author                   = {Redmon, Joseph and Farhadi, Ali},

  Abstract                 = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Us-ing a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster R-CNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on ob-ject detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes; it predicts de-tections for more than 9000 different object categories. And it still runs in real-time.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Redmon, Farhadi/Unknown/Redmon, Farhadi - Unknown - YOLO9000 Better, Faster, Stronger.pdf:pdf},
  Url                      = {http://pjreddie.com/yolo9000/}
}

@Article{Reed,
  Title                    = {{TRAINING DEEP NEURAL NETWORKS ON NOISY LABELS WITH BOOTSTRAPPING}},
  Author                   = {Reed, Scott E and Lee, Honglak and Anguelov, Dragomir and Szegedy, Christian and Erhan, Dumitru and Rabinovich, Andrew},

  Abstract                 = {Current state-of-the-art deep learning systems for visual object recognition and detection use purely supervised training with regularization such as dropout to avoid overfitting. The performance depends critically on the amount of labeled examples, and in current practice the labels are assumed to be unambiguous and accurate. However, this assumption often does not hold; e.g. in recognition, class labels may be missing; in detection, objects in the image may not be localized; and in general, the labeling may be subjective. In this work we propose a generic way to handle noisy and incomplete labeling by augmenting the prediction ob-jective with a notion of consistency. We consider a prediction consistent if the same prediction is made given similar percepts, where the notion of similarity is between deep network features computed from the input data. In experiments we demonstrate that our approach yields substantial robustness to label noise on sev-eral datasets. On MNIST handwritten digits, we show that our model is robust to label corruption. On the Toronto Face Database, we show that our model handles well the case of subjective labels in emotion recognition, achieving state-of-the-art results, and can also benefit from unlabeled face images with no modification to our method. On the ILSVRC2014 detection challenge data, we show that our approach extends to very deep networks, high resolution images and structured outputs, and results in improved scalable detection.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Reed et al/Unknown/Reed et al. - Unknown - TRAINING DEEP NEURAL NETWORKS ON NOISY LABELS WITH BOOTSTRAPPING.pdf:pdf},
  Url                      = {https://arxiv.org/pdf/1412.6596.pdf}
}

@Article{Rice2013,
  Title                    = {{Corpus-Based Dictionaries for Sentiment Analysis of Specialized Vocabularies}},
  Author                   = {Rice, Douglas R. and Zorn, Christopher},
  Journal                  = {Proceedings of NDATAD},
  Year                     = {2013},
  Pages                    = {1--17},

  File                     = {:home/jan/Documents/Mendeley Desktop/Rice, Zorn/Proceedings of NDATAD/Rice, Zorn - 2013 - Corpus-Based Dictionaries for Sentiment Analysis of Specialized Vocabularies.pdf:pdf},
  Keywords                 = {Sentiment Analysis,Sentiment Dictionaries},
  Mendeley-tags            = {Sentiment Analysis,Sentiment Dictionaries}
}

@Article{Roth2007,
  Title                    = {{Improved functional prediction of proteins by learning kernel combinations in multilabel settings.}},
  Author                   = {Roth, Volker and Fischer, Bernd},
  Journal                  = {BMC bioinformatics},
  Year                     = {2007},
  Pages                    = {S12},
  Volume                   = {8 Suppl 2},

  Abstract                 = {BACKGROUND: We develop a probabilistic model for combining kernel matrices to predict the function of proteins. It extends previous approaches in that it can handle multiple labels which naturally appear in the context of protein function.$\backslash$n$\backslash$nRESULTS: Explicit modeling of multilabels significantly improves the capability of learning protein function from multiple kernels. The performance and the interpretability of the inference model are further improved by simultaneously predicting the subcellular localization of proteins and by combining pairwise classifiers to consistent class membership estimates.$\backslash$n$\backslash$nCONCLUSION: For the purpose of functional prediction of proteins, multilabels provide valuable information that should be included adequately in the training process of classifiers. Learning of functional categories gains from co-prediction of subcellular localization. Pairwise separation rules allow very detailed insights into the relevance of different measurements like sequence, structure, interaction data, or expression data. A preliminary version of the software can be downloaded from http://www.inf.ethz.ch/personal/vroth/KernelHMM/.},
  Doi                      = {10.1186/1471-2105-8-S2-S12},
  File                     = {:home/jan/Documents/Mendeley Desktop/Roth, Fischer/BMC bioinformatics/Roth, Fischer - 2007 - Improved functional prediction of proteins by learning kernel combinations in multilabel settings.pdf:pdf},
  ISSN                     = {1471-2105},
  Keywords                 = {Algorithms,Artificial Intelligence,Computer Simulation,Fungal Proteins,Fungal Proteins: chemistry,Fungal Proteins: classification,Fungal Proteins: metabolism,Models, Biological,Sequence Analysis, Protein,Sequence Analysis, Protein: methods,Signal Transduction,Signal Transduction: physiology,Structure-Activity Relationship,Yeasts,Yeasts: metabolism},
  Pmid                     = {17493250},
  Url                      = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1892070{\&}tool=pmcentrez{\&}rendertype=abstract}
}

@Article{Schapire2000,
  Title                    = {{BoosTexter: A boosting- based system for text categorization}},
  Author                   = {Schapire, R E and Singer, Y},
  Journal                  = {Machine learning},
  Year                     = {2000},
  Pages                    = {135--168},
  Volume                   = {39},

  File                     = {:home/jan/Documents/Mendeley Desktop/Schapire, Singer/Machine learning/Schapire, Singer - 2000 - BoosTexter A boosting- based system for text categorization.pdf:pdf},
  Keywords                 = {boosting algorithms,multiclass classification problems,text and speech categorization},
  Url                      = {http://link.springer.com/article/10.1023/A:1007649029923}
}

@Misc{Schapire1999,
  Title                    = {{Improved Boosting Algorithms Using Confidence-rated Predictions}},

  Author                   = {Schapire, Robert E and Singer, Yoram},
  Year                     = {1999},

  Abstract                 = {We describe several improvements to Freund and Schapire's AdaBoost boosting algorithm, particu-larly in a setting in which hypotheses may assign confidences to each of their predictions. We give a simplified analysis of AdaBoost in this setting, and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses. We give a specific method for assigning confidences to the predictions of decision trees, a method closely related to one used by Quinlan. This method also suggests a technique for growing decision trees which turns out to be identical to one proposed by Kearns and Mansour. We focus next on how to apply the new boosting algorithms to multiclass classification problems, particularly to the multi-label case in which each example may belong to more than one class. We give two boosting methods for this problem, plus a third method based on output coding. One of these leads to a new method for handling the single-label case which is simpler but as effective as techniques suggested by Freund and Schapire. Finally, we give some experimental results comparing a few of the algorithms discussed in this paper.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Schapire, Singer/Unknown/Unknown - Unknown - Improved Boosting Algorithms Using Confidence-rated Predictions.pdf:pdf},
  Keywords                 = {Boosting algorithms,decision trees,multiclass classification,output coding},
  Url                      = {http://web.cs.iastate.edu/{~}honavar/singer99improved.pdf}
}

@Article{Sechidis2011,
  Title                    = {{On the Stratification of Multi-label Data}},
  Author                   = {Sechidis, Konstantinos and Tsoumakas, Grigorios and Vlahavas, Ioannis},
  Year                     = {2011},

  Abstract                 = {Stratified sampling is a sampling method that takes into account the existence of disjoint groups within a population and pro-duces samples where the proportion of these groups is maintained. In single-label classification tasks, groups are differentiated based on the value of the target variable. In multi-label learning tasks, however, where there are multiple target variables, it is not clear how stratified sam-pling could/should be performed. This paper investigates stratification in the multi-label data context. It considers two stratification methods for multi-label data and empirically compares them along with random sampling on a number of datasets and based on a number of evaluation criteria. The results reveal some interesting conclusions with respect to the utility of each method for particular types of multi-label datasets.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Sechidis, Tsoumakas, Vlahavas/Unknown/Sechidis, Tsoumakas, Vlahavas - Unknown - On the Stratification of Multi-Label Data.pdf:pdf},
  Url                      = {http://download.springer.com/static/pdf/229/chp{\%}253A10.1007{\%}252F978-3-642-23808-6{\_}10.pdf?originUrl=http{\%}3A{\%}2F{\%}2Flink.springer.com{\%}2Fchapter{\%}2F10.1007{\%}2F978-3-642-23808-6{\_}10{\&}token2=exp=1489589222{~}acl={\%}2Fstatic{\%}2Fpdf{\%}2F229{\%}2Fchp{\%}25253A10.1007{\%}25252F978-3-64}
}

@Article{Snoek2006,
  Title                    = {{The challenge problem for automated detection of 101 semantic concepts in multimedia}},
  Author                   = {Snoek, Cees G M and Worring, Marcel and {Van Gemert}, Jan C and Geusebroek, Jan-Mark and Smeulders, Arnold W M},
  Journal                  = {Proceedings of the 14th annual ACM international conference on Multimedia MULTIMEDIA 06},
  Year                     = {2006},
  Pages                    = {421--430},

  Abstract                 = {We introduce the challenge problem for generic video indexing to gain insight in intermediate steps that affect performance of multimedia analysis methods, while at the same time fostering repeatability of experiments. To arrive at a challenge problem, we provide a general scheme for the systematic examination of automated concept detection methods, by decomposing the generic video indexing problem into 2 unimodal analysis experiments, 2 multimodal analysis experiments, and 1 combined analysis experiment. For each experiment, we evaluate generic video indexing performance on 85 hours of international broadcast news data, from the TRECVID 2005/2006 benchmark, using a lexicon of 101 semantic concepts. By establishing a minimum performance on each experiment, the challenge problem allows for component-based optimization of the generic indexing issue, while simultaneously offering other researchers a reference for comparison during indexing methodology development. To stimulate further investigations in intermediate analysis steps that inuence video indexing performance, the challenge offers to the research community a manually annotated concept lexicon, pre-computed low-level multimedia features, trained classifier models, and five experiments together with baseline performance, which are all available at http://www.mediamill.nl/challenge/.},
  Annote                   = {NULL},
  Doi                      = {10.1145/1180639.1180727},
  File                     = {:home/jan/Documents/Mendeley Desktop/Snoek et al/Proceedings of the 14th annual ACM international conference on Multimedia MULTIMEDIA 06/Snoek et al. - 2006 - The Challenge Problem for Automated Detection of 101 Semantic Concepts in Multimedia.pdf:pdf},
  ISBN                     = {1595934472},
  Keywords                 = {baseline,capture,field multimedia,fueled ever increasing,generic concept detection,growth recent years,indexing has witnessed a,rapid,video analysis},
  Url                      = {http://www.mediamill.nl/challenge/. http://portal.acm.org/citation.cfm?doid=1180639.1180727}
}

@Article{Sorower2010,
  Title                    = {{A literature survey on algorithms for multi-label learning}},
  Author                   = {Sorower, Ms},
  Journal                  = {Oregon State University, Corvallis},
  Year                     = {2010},
  Pages                    = {1--25},

  Abstract                 = {Multi-label Learning is a form of supervised learning where the classification al- gorithm is required to learn from a set of instances, each instance can belong to multiple classes and so after be able to predict a set of class labels for a new in- stance. This is a generalized version of most popular multi-class problems where each instances is restricted to have only one class label. There exists a wide range of applications for multi-labelled predictions, such as text categorization, seman- tic image labeling, gene functionality classification etc. and the scope and interest is increasing with modern applications. This survey paper introduces the task of multi-label prediction (classification), presents the sparse literature in this area in an organized manner, discusses different evaluation metrics and performs a com- parative analysis of the existing algorithms. This paper also relates multi-label problems with similar but different problems that are often reduced to multi-label problems to have access to wide range of multi-label algorithms.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Sorower/Oregon State University, Corvallis/Sorower - Unknown - A Literature Survey on Algorithms for Multi-label Learning.pdf:pdf},
  Url                      = {http://people.oregonstate.edu/{~}sorowerm/pdf/Qual-Multilabel-Shahed-CompleteVersion.pdf}
}

@Article{Spolaor2013,
  Title                    = {{A comparison of multi-label feature selection methods using the problem transformation approach}},
  Author                   = {Spola{\^{o}}r, Newton and Cherman, Everton Alvares and Monard, Maria Carolina and Lee, Huei Diana},
  Journal                  = {Electronic Notes in Theoretical Computer Science},
  Year                     = {2013},
  Pages                    = {135--151},
  Volume                   = {292},

  Abstract                 = {Feature selection is an important task in machine learning, which can effectively reduce the dataset dimensionality by removing irrelevant and/or redundant features. Although a large body of research deals with feature selection in single-label data, in which measures have been proposed to filter out irrelevant features, this is not the case for multi-label data. This work proposes multi-label feature selection methods which use the filter approach. To this end, two standard multi-label feature selection approaches, which transform the multi-label data into single-label data, are used. Besides these two problem transformation approaches, we use ReliefF and Information Gain to measure the goodness of features. This gives rise to four multi-label feature selection methods. A thorough experimental evaluation of these methods was carried out on 10 benchmark datasets. Results show that ReliefF is able to select fewer features without diminishing the quality of the classifiers constructed using the features selected. {\textcopyright} 2013 Elsevier B.V.},
  Annote                   = {From Duplicate 1 (A comparison of multi-label feature selection methods using the problem transformation approach - Spola{\^{o}}r, Newton; Cherman, Everton Alvares; Monard, Maria Carolina; Lee, Huei Diana)

NULL},
  Doi                      = {10.1016/j.entcs.2013.02.010},
  File                     = {:home/jan/Documents/Mendeley Desktop/Spola{\^{o}}r et al/Electronic Notes in Theoretical Computer Science/Spola{\^{o}}r et al. - 2013 - A Comparison of Multi-label Feature Selection Methods using the Problem Transformation Approach.pdf:pdf;:home/jan/Documents/Mendeley Desktop/Spola{\^{o}}r et al/Electronic Notes in Theoretical Computer Science/Spola{\^{o}}r et al. - 2013 - A Comparison of Multi-label Feature Selection Methods using the Problem Transformation Approach(2).pdf:pdf},
  ISBN                     = {9783319259383},
  ISSN                     = {15710661},
  Keywords                 = {Feature Ranking,Information Gain,Multi-label learning,ReliefF},
  Url                      = {http://ac.els-cdn.com/S1571066113000121/1-s2.0-S1571066113000121-main.pdf?{\_}tid=20bd543e-15e1-11e7-b65c-00000aacb362{\&}acdnat=1490944412{\_}9864553197edbe2f350763a8098154b4}
}

@Article{Spolaor2016,
  Title                    = {{A systematic review of multi-label feature selection and a new method based on label construction}},
  Author                   = {Spola{\^{o}}r, Newton and Monard, M.C. Maria Carolina and Tsoumakas, Grigorios and Lee, H.D. Huei Diana},
  Journal                  = {Neurocomputing},
  Year                     = {2016},
  Pages                    = {3--15},
  Volume                   = {180},

  Abstract                 = {Each example in a multi-label dataset is associated with multiple labels, which are often correlated. Learning from this data can be improved when dimensionality reduction tasks, such as feature selection, are applied. The standard approach for multi-label feature selection transforms the multi-label dataset into single-label datasets before using traditional feature selection algorithms. However, this approach often ignores label dependence. In this work, we propose an alternative method, LCFS, that constructs new labels based on relations between the original labels. By doing so, the label set from the data is augmented with second-order information before applying the standard approach. To assess LCFS, an experimental evaluation using Information Gain as a measure to estimate the importance of features was carried out on 10 benchmark multi-label datasets. This evaluation compared four LCFS settings with the standard approach, using random feature selection as a reference. For each dataset, the performance of a feature selection method is estimated by the quality of the classifiers built from the data described by the features selected by the method. The results show that a simple LCFS setting gave rise to classifiers similar to, or better than, the ones built using the standard approach. Furthermore, this work also pioneers the use of the systematic review method to survey the related work on multi-label feature selection. The summary of the 99 papers found promotes the idea that exploring label dependence during feature selection can lead to good results.},
  Doi                      = {10.1016/j.neucom.2015.07.118},
  File                     = {:home/jan/Documents/Mendeley Desktop/Spola{\^{o}}r et al/Neurocomputing/1-s2.0-S0925231215016197-main.pdf:pdf;:home/jan/Documents/Mendeley Desktop/Spola{\^{o}}r et al/Neurocomputing/Spolaor et al. - Unknown - A systematic review of multi-label feature selection and a new method based on label construction.pdf:pdf},
  ISBN                     = {0925-2312},
  ISSN                     = {18728286},
  Keywords                 = {Binary relevance,Feature ranking,Filter feature,Filter feature selection,Information gain,Systematic review,[Binary relevance},
  Publisher                = {Elsevier},
  Url                      = {http://ac.els-cdn.com.ez.sun.ac.za/S0925231215016197/1-s2.0-S0925231215016197-main.pdf?{\_}tid=d82dcdb8-160b-11e7-87ff-00000aab0f26{\&}acdnat=1490962758{\_}6fd0abd75e28b70f59f2db5863d099a7 http://dx.doi.org/10.1016/j.neucom.2015.07.118}
}

@Article{Spyromitros-Xioufis2016,
  Title                    = {{Multi-Target Regression via Input Space Expansion: Treating Targets as Inputs}},
  Author                   = {Spyromitros-Xioufis, Eleftherios and Tsoumakas, Grigorios and Groves, William and Vlahavas, Ioannis and Spyromitros-Xioufis, E and Tsoumakas, G and Vlahavas, I and Groves, W},
  Year                     = {2016},

  Abstract                 = {In many practical applications of supervised learning the task involves the predic-tion of multiple target variables from a common set of input variables. When the prediction targets are binary the task is called multi-label classification, while when the targets are con-tinuous the task is called multi-target regression. In both tasks, target variables often exhibit statistical dependencies and exploiting them in order to improve predictive accuracy is a core challenge. A family of multi-label classification methods address this challenge by building a separate model for each target on an expanded input space where other targets are treated as additional input variables. Despite the success of these methods in the multi-label classi-fication domain, their applicability and effectiveness in multi-target regression has not been studied until now. In this paper, we introduce two new methods for multi-target regression, called Stacked Single-Target and Ensemble of Regressor Chains, by adapting two popu-lar multi-label classification methods of this family. Furthermore, we highlight an inherent problem of these methods -a discrepancy of the values of the additional input variables be-tween training and prediction -and develop extensions that use out-of-sample estimates of the target variables during training in order to tackle this problem. The results of an extensive experimental evaluation carried out on a large and diverse collection of datasets show that, when the discrepancy is appropriately mitigated, the proposed methods attain consistent im-provements over the independent regressions baseline. Moreover, two versions of Ensemble of Regression Chains perform significantly better than four state-of-the-art methods includ-ing regularization-based multi-task learning methods and a multi-objective random forest approach.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {arXiv:1211.6581v5},
  Doi                      = {10.1007/s10994-016-5546-z},
  Eprint                   = {arXiv:1211.6581v5},
  File                     = {:home/jan/Documents/Mendeley Desktop/Spyromitros-Xioufis et al/Unknown/Spyromitros-Xioufis et al. - 2016 - Multi-Target Regression via Input Space Expansion Treating Targets as Inputs.pdf:pdf},
  Keywords                 = {Chaining,Classification {\textperiodcentered},Multi-label,Multi-target,Regression {\textperiodcentered},Stacking {\textperiodcentered}},
  Url                      = {http://dx.doi.org/10.1007/s10994-016-5546-z.}
}

@Article{Sucar2013,
  Title                    = {{Author's personal copy Multi-label classification with Bayesian network-based chain classifiers}},
  Author                   = {Sucar, L Enrique and Bielza, Concha and Morales, Eduardo F and Hernandez-Leal, Pablo and Zaragoza, Julio H and Larra{\~{n}}aga, Pedro},
  Year                     = {2013},

  Abstract                 = {a b s t r a c t In multi-label classification the goal is to assign an instance to a set of different classes. This task is normally addressed either by defining a compound class variable with all the possible combinations of labels (label power-set methods) or by building independent classifiers for each class (binary relevance methods). The first approach suffers from high computationally complexity, while the second approach ignores possible dependencies among classes. Chain classifiers have been recently proposed to address these problems, where each classifier in the chain learns and predicts the label of one class given the attributes and all the predictions of the previous classifiers in the chain. In this paper we introduce a method for chaining Bayesian classifiers that combines the strengths of classifier chains and Bayesian networks for multi-label classification. A Bayesian network is induced from data to: (i) represent the probabilistic dependency relationships between classes, (ii) constrain the number of class variables used in the chain classifier by considering conditional independence conditions, and (iii) reduce the number of possible chain orders. The effects in the Bayesian chain classifier performance of considering different chain orders, training strategies, number of class variables added in the base classifiers, and different base classifiers, are experimentally assessed. In particular, it is shown that a random chain order considering the constraints imposed by a Bayesian network with a simple tree-based structure can have very competitive results in terms of predictive performance and time complexity against related state-of-the-art approaches.},
  Doi                      = {10.1016/j.patrec.2013.11.007},
  File                     = {:home/jan/Documents/Mendeley Desktop/Sucar et al/Unknown/Sucar et al. - 2013 - Author's personal copy Multi-label classification with Bayesian network-based chain classifiers.pdf:pdf},
  Keywords                 = {Bayesian networks,Chain classifier,Multi-label classification}
}

@Article{Sun,
  Title                    = {{Canonical Correlation Analysis for Multilabel Classification: A Least-Squares Formulation, Extensions, and Analysis}},
  Author                   = {Sun, Liang and Ji, Shuiwang and Ye, Jieping},

  Abstract                 = {—Canonical Correlation Analysis (CCA) is a well-known technique for finding the correlations between two sets of multidimensional variables. It projects both sets of variables onto a lower-dimensional space in which they are maximally correlated. CCA is commonly applied for supervised dimensionality reduction in which the two sets of variables are derived from the data and the class labels, respectively. It is well-known that CCA can be formulated as a least-squares problem in the binary class case. However, the extension to the more general setting remains unclear. In this paper, we show that under a mild condition which tends to hold for high-dimensional data, CCA in the multilabel case can be formulated as a least-squares problem. Based on this equivalence relationship, efficient algorithms for solving least-squares problems can be applied to scale CCA to very large data sets. In addition, we propose several CCA extensions, including the sparse CCA formulation based on the 1-norm regularization. We further extend the least-squares formulation to partial least squares. In addition, we show that the CCA projection for one set of variables is independent of the regularization on the other set of multidimensional variables, providing new insights on the effect of regularization on CCA. We have conducted experiments using benchmark data sets. Experiments on multilabel data sets confirm the established equivalence relationships. Results also demonstrate the effectiveness and efficiency of the proposed CCA extensions.},
  Annote                   = {NULL},
  File                     = {:home/jan/Documents/Mendeley Desktop/Sun, Ji, Ye/Unknown/Sun, Ji, Ye - Unknown - Canonical Correlation Analysis for Multilabel Classification A Least-Squares Formulation, Extensions, and Analys.pdf:pdf},
  Keywords                 = {Index Terms—Canonical correlation analysis,least squares,multilabel learning,partial least squares,regularization}
}

@Article{Systems2014,
  Title                    = {{Ensemble Methods for Multi-label Classification Ensemble Methods for Multi-label Classification}},
  Author                   = {Systems, Expert and Aviv-yafo, Tel},
  Year                     = {2014},
  Number                   = {July 2013},

  Abstract                 = {Ensemble methods have been shown to be an effective tool for solving multi-label classification tasks. In the RAndom k-labELsets (RAKEL) algorithm, each member of the ensemble is associated with a small randomly-selected subset of k labels. Then, a single label classifier is trained according to each combi-nation of elements in the subset. In this paper we adopt a similar approach, however, instead of randomly choosing subsets, we select the minimum required subsets of k labels that cover all labels and meet additional constraints such as coverage of inter-label correlations. Construction of the cover is achieved by formulating the subset selection as a minimum set covering problem (SCP) and solving it by using approximation algo-rithms. Every cover needs only to be prepared once by offline algorithms. Once prepared, a cover may be applied to the classification of any given multi-label dataset whose properties conform with those of the cov-er. The contribution of this paper is two-fold. First, we introduce SCP as a general framework for construct-ing label covers while allowing the user to incorporate cover construction constraints. We demonstrate the effectiveness of this framework by proposing two construction constraints whose enforcement produces co-vers that improve the prediction performance of random selection. Second, we provide theoretical bounds that quantify the probabilities of random selection to produce covers that meet the proposed construction cri-teria. The experimental results indicate that the proposed methods improve multi-label classification accura-cy and stability compared with the RAKEL algorithm and to other state-of-the-art algorithms.},
  Doi                      = {10.1016/j.eswa.2014.06.015},
  File                     = {:home/jan/Documents/Mendeley Desktop/Systems, Aviv-yafo/Unknown/Rokach, Schclar, Itach - Unknown - Ensemble Methods for Multi-label Classification.pdf:pdf}
}

@Article{Tai2010,
  Title                    = {{Multi-label classification with principle label space transformation}},
  Author                   = {Tai, Farbound and Lin, H.T.},
  Journal                  = {International Workshop on Learning from Multi-Label Data},
  Year                     = {2010},
  Pages                    = {45},

  Abstract                 = {We propose a novel hypercube view that per-ceives the label space of multi-label classifi-cation problems geometrically. The view al-lows us to not only unify many existing multi-label classification approaches, but also de-sign a novel algorithm, Principle Label Space Transformation (PLST), which seeks impor-tant correlations between labels before learn-ing. The simple and efficient PLST relies on only singular value decomposition as the key step. Experimental results demonstrate that PLST is faster than the traditional Bi-nary Relevance approach and is superior to the modern Compressive Sensing approach in terms of both performance and efficiency.},
  Annote                   = {NULL},
  File                     = {:home/jan/Documents/Mendeley Desktop/Tai, Lin/International Workshop on Learning from Multi-Label Data/Tai, Lin - Unknown - Multi-label Classification with Principle Label Space Transformation.pdf:pdf},
  Url                      = {http://ntur.lib.ntu.edu.tw/retrieve/188514/18.pdf http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.170.1860{\&}rep=rep1{\&}type=pdf{\#}page=46}
}

@Article{Tan2017,
  Title                    = {{Multi-Label Classification Based on Low Rank Representation for Image Annotation}},
  Author                   = {Tan, Qiaoyu and Liu, Yezi and Chen, Xia and Yu, Guoxian},
  Journal                  = {Remote Sensing},
  Year                     = {2017},
  Number                   = {2},
  Pages                    = {109},
  Volume                   = {9},

  Doi                      = {10.3390/rs9020109},
  File                     = {:home/jan/Documents/Mendeley Desktop/Tan et al/Remote Sensing/remotesensing-09-00109-v2.pdf:pdf},
  ISSN                     = {2072-4292},
  Keywords                 = {graph construction,image annotation,low-rank,multi-label classification,remote sensing images,representation,semantic graph},
  Url                      = {http://www.mdpi.com/2072-4292/9/2/109}
}

@Article{Tang2012,
  Title                    = {{Hamming Selection Pruned Sets (HSPS) for Efficient Multi-label Video Classification}},
  Author                   = {Tang, Tiong Yew and Alhashmi, Saadat M and Jaward, Mohamed Hisham},
  Journal                  = {PRICAI},
  Year                     = {2012},
  Number                   = {1},

  Abstract                 = {Videos have become an integral part of our life, from watching mov-ies online to the use of videos in classroom teaching. Existing machine learning techniques are constrained with this scaled up activity because of this huge up-surge in online activity. A lot of research is now focused on reducing the time and accuracy of video classification. Content-Based Video Information Retriev-al CBVIR implementation (E.g. Columbia374) is one such approach. We pro-pose a fast Hamming Selection Pruned Sets (HSPS) algorithm that efficiently transforms multi-label video dataset into single-label representation. Thus, mul-ti-label relationship between the labels can be retained for later single label classifier learning stage. Hamming distance (HD) is used to detect similarity be-tween label-sets. HSPS captures new potential label-set relationships that were previously undetected by baseline approach. Experiments show a significant 22.9{\%} dataset building time reduction and consistent accuracy improvement over the baseline method. HSPS also works on general multi-label dataset. 1 Introduction Multimedia information indexing and retrieval is an integral part of modern video search engines to describe, store and organize multimedia information and to assist people in finding multimedia information conveniently. Many research papers have been published on the area of Content-Based Video Information Retrieval (CBVIR) [1, 2]. CBVIR is an information retrieval research problem which involves machine learning over the video content (e.g. colors, shapes, textures, texts, motions and etc.). In this paper, our focus is on Video Semantic Concept Classification (VSCC) which is a sub-theme of CBVIR for detecting the correct semantic concept of a video shot. For example, let us consider a video shot containing keyframes of a car moving on a road. The detected semantic concepts for this video shot are car, weather and road. In fact this example shows that the video semantic concept classification natu-rally is a multi-label research problem. 1 Relationship between these labels can be exploited for enhancing classification accuracy as demonstrated by these research 1 Throughout this paper, we use multi-label instead of multi-concept for our research context.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Tang, Alhashmi, Jaward/PRICAI/Tang, Alhashmi, Jaward - 2012 - Hamming Selection Pruned Sets (HSPS) for Efficient Multi-label Video Classification.pdf:pdf},
  Keywords                 = {Classification,Index Terms— Semantic Concept,Index and Retrieval,Label Combinations,Multi-Label,Pruned Sets,TRECVID,Videos},
  Url                      = {https://pdfs.semanticscholar.org/6d49/8611e1d30c5d1928573c199db02fc538d8d4.pdf?{\_}ga=1.107724480.379343330.1490351020}
}

@Article{Tawiah2013,
  Title                    = {{Empirical Comparison of Multi-Label Classification Algorithms}},
  Author                   = {Tawiah, Ca and Sheng, Vs},
  Journal                  = {Proceedings of the 27th AAAI Conference on {\ldots}},
  Year                     = {2013},
  Pages                    = {1645--1646},

  Abstract                 = {Multi-label classifications exist in many real world applications. This paper empirically studies the performance of a variety of multi-label classification algorithms. Some of them are developed based on problem transformation. Some of them are developed based on adaption. Our experimental results show that the adaptive Multi-Label K-Nearest Neighbor performs the best, followed by Random k-Label Set, followed by Classifier Chain and Binary Relevance. Adaboost.MH performs the worst, followed by Pruned Problem Transformation. Our experimental results also provide us the confidence of the correlations among multilabels. These insights shed light for future research directions on multi-label classifications.},
  ISBN                     = {9781424427659},
  Keywords                 = {Student Abstract and Poster Program},
  Url                      = {http://www.aaai.org/ocs/index.php/AAAI/AAAI13/paper/download/6170/6833}
}

@Article{TorresTomas2014,
  Title                    = {{A framework to generate synthetic multi-label datasets}},
  Author                   = {Tom{\'{a}}s, Jimena Torres and Spola{\^{o}}r, Newton and Cherman, Everton Alvares and Monard, Maria Carolina},
  Journal                  = {Electronic Notes in Theoretical Computer Science},
  Year                     = {2014},
  Pages                    = {155--176},
  Volume                   = {302},

  Abstract                 = {A controlled environment based on known properties of the dataset used by a learning algorithm is useful to empirically evaluate machine learning algorithms. Synthetic (artificial) datasets are used for this purpose. Although there are publicly available frameworks to generate synthetic single-label datasets, this is not the case for multi-label datasets, in which each instance is associated with a set of labels usually correlated. This work presents Mldatagen, a multi-label dataset generator framework we have implemented, which is publicly available to the community. Currently, two strategies have been implemented in Mldatagen: hypersphere and hypercube. For each label in the multi-label dataset, these strategies randomly generate a geometric shape (hypersphere or hypercube), which is populated with points (instances) randomly generated. Afterwards, each instance is labeled according to the shapes it belongs to, which defines its multi-label. Experiments with a multi-label classification algorithm in six synthetic datasets illustrate the use of Mldatagen. {\textcopyright} 2014 Elsevier B.V.},
  Doi                      = {10.1016/j.entcs.2014.01.025},
  File                     = {:home/jan/Documents/Mendeley Desktop/Tom{\'{a}}s et al/Electronic Notes in Theoretical Computer Science/Torres Tom{\'{a}}s et al. - 2014 - A Framework to Generate Synthetic Multi-label Datasets.pdf:pdf},
  ISSN                     = {15710661},
  Keywords                 = {Java,PHP,artificial datasets,data generator,multi-label learning,publicly available framework},
  Url                      = {http://ac.els-cdn.com/S1571066114000267/1-s2.0-S1571066114000267-main.pdf?{\_}tid=207a475a-25c4-11e7-9d47-00000aacb35d{\&}acdnat=1492691174{\_}037266698571d8a927f3feb0eb432995}
}

@Article{Trohidis2008,
  Title                    = {{Multi-Label Classification of Music Into Emotions}},
  Author                   = {Trohidis, Konstantinos and Kalliris, George},
  Journal                  = {Proc. ISMIR},
  Year                     = {2008},
  Pages                    = {325--330},
  Volume                   = {2008},

  Abstract                 = {In this paper, the automated detection of emotion in music is modeled as a multilabel classification task, where a piece of music may belong to more than one class. Four algorithms are evaluated and compared in this task. Furthermore, the predictive power of several audio features is evaluated using a new multilabel feature selection method. Experiments are conducted on a set of 593 songs with 6 clusters of music emotions based on the Tellegen-Watson-Clark model. Results provide interesting insights into the quality of the discussed algorithms and features.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Trohidis, Kalliris/Proc. ISMIR/Trohidis, Kalliris - 2008 - Multi-Label Classification of Music Into Emotions.pdf:pdf},
  ISBN                     = {0615248497},
  Url                      = {http://ismir2008.ismir.net/papers/ISMIR2008{\_}275.pdf}
}

@Article{Tsoumakas2009,
  Title                    = {{Correlation-based pruning of stacked binary relevance models for multi-label learning}},
  Author                   = {Tsoumakas, Grigorios and Dimou, Anastasios and Spyromitros, Eleftherios and Mezaris, Vasileios and Kompatsiaris, Ioannis and Vlahavas, Ioannis},
  Journal                  = {Proceedings of the Workshop on Learning from Multi-Label Data (MLD'09)},
  Year                     = {2009},
  Pages                    = {101--116},

  Abstract                 = {Binary relevance (BR) learns a single binary model for each different label of multi-label data. It has linear complexity with respect to the number of labels, but does not take into account label correlations and may fail to accurately predict label combinations and rank labels according to relevance with a new instance. Stacking the models of BR in order to learn a model that associates their output to the true value of each label is a way to alleviate this problem. In this paper we propose the pruning of the models participating in the stacking process, by explicitly measuring the degree of label correlation using the phi coefficient. Exploratory analysis of phi shows that the correlations detected are meaningful and useful. Empirical evaluation of the pruning approach shows that it leads to substantial reduction of the computational cost of stacking and occasional improvements in predictive performance.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Tsoumakas et al/Proceedings of the Workshop on Learning from Multi-Label Data (MLD'09)/Tsoumakas et al. - 2009 - Correlation-based pruning of stacked binary relevance models for multi-label learning.pdf:pdf},
  ISSN                     = {1475-925X},
  Url                      = {http://www.ecmlpkdd2009.net/wp-content/uploads/2008/09/learning-from-multi-label-data.pdf{\#}page=102}
}

@Article{Tsoumakas2011,
  Title                    = {{MULAN: A Java Library for Multi-Label Learning Eleftherios Spyromitros-Xioufis Ioannis Vlahavas}},
  Author                   = {Tsoumakas, Grigorios and Gr, Greg@csd Auth and Gr, Espyromi@csd Auth and Vilcek, Jozef and Gr, Vlahavas@csd Auth},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2011},
  Pages                    = {2411--2414},
  Volume                   = {12},

  Abstract                 = {MULAN is a Java library for learning from multi-label data. It offers a variety of classification, rank-ing, thresholding and dimensionality reduction algorithms, as well as algorithms for learning from hierarchically structured labels. In addition, it contains an evaluation framework that calculates a rich variety of performance measures.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Tsoumakas et al/Journal of Machine Learning Research/Tsoumakas et al. - 2011 - MULAN A Java Library for Multi-Label Learning Eleftherios Spyromitros-Xioufis Ioannis Vlahavas.pdf:pdf},
  Keywords                 = {classification,dimensionality reduction,evaluation,hier-archical classification,multi-label data,ranking,thresholding},
  Url                      = {http://www.jmlr.org/papers/volume12/tsoumakas11a/tsoumakas11a.pdf}
}

@Article{Tsoumakas2007,
  Title                    = {{Multi-Label Classification : An Overview}},
  Author                   = {Tsoumakas, Grigorios and Katakis, Ioannis},
  Year                     = {2007},

  Abstract                 = {Nowadays, multi-label classification methods are increasingly required by modern applications, such as protein function classification, music categorization and semantic scene classification. This paper introduces the task of multi-label classification, organizes the sparse related literature into a structured presentation and performs comparative experimental results of certain multi-label classification methods. It also contributes the definition of concepts for the quantification of the multi-label nature of a data set.},
  Doi                      = {10.4018/jdwm.2007070101},
  File                     = {:home/jan/Documents/Mendeley Desktop/Tsoumakas, Katakis/Unknown/Tsoumakas, Katakis - 2007 - Multi-Label Classification An Overview.pdf:pdf},
  ISBN                     = {9781424410651},
  ISSN                     = {1548-3924}
}

@Article{Tsoumakas2008,
  Title                    = {{Effective and efficient multilabel classification in domains with large number of labels}},
  Author                   = {Tsoumakas, Grigorios and Katakis, Ioannis and Vlahavas, Ioannis},
  Journal                  = {Proc. ECML/PKDD 2008 Workshop on Mining Multidimensional Data (MMD'08)},
  Year                     = {2008},
  Pages                    = {30--44},

  Abstract                 = {This paper contributes a novel algorithm for effective and computationally efficient multilabel classification in domains with large label sets L. The HOMER algorithm constructs a Hierarchy Of Multilabel classifiERs, each one dealing with a much smaller set of labels compared to L and a more balanced example distribution. This leads to improved predictive performance along with linear training and logarithmic testing complexities with respect to |L|. Label distribution from parent to children nodes is achieved via a new balanced clustering algorithm, called balanced k means.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Tsoumakas, Katakis, Vlahavas/Proc. ECMLPKDD 2008 Workshop on Mining Multidimensional Data (MMD'08)/Tsoumakas, Katakis, Vlahavas - 2008 - Effective and efficient multilabel classification in domains with large number of labels.pdf:pdf},
  Url                      = {http://lpis.csd.auth.gr/publications/tsoumakas-mmd08.pdf}
}

@Article{Tsoumakas2006,
  Title                    = {{A Review of Multi-Label Classification Methods}},
  Author                   = {Tsoumakas, Grigorios and Katakis, Ioannis and Vlahavas, Ioannis},
  Journal                  = {Proceedings of the 2nd ADBIS Workshop on Data Mining and Knowledge Discovery (ADMKD 2006)},
  Year                     = {2006},
  Pages                    = {99--109},

  Abstract                 = {Nowadays, multi-label classification methods are increasingly required by modern applications, such as protein function classification, music categorization and semantic scene classification. This paper intro-duces the task of multi-label classification, organizes the sparse related literature into a structured presentation and performs comparative ex-perimental results of certain multi-label classification methods. It also contributes the presentation of an undocumented method and the defi-nition of a concept for the quantification of the multi-label nature of a data set.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Tsoumakas, Katakis, Vlahavas/Proceedings of the 2nd ADBIS Workshop on Data Mining and Knowledge Discovery (ADMKD 2006)/Tsoumakas, Katakis, Vlahavas - Unknown - A Review of Multi-Label Classification Methods.pdf:pdf}
}

@Article{Tsoumakas2014,
  Title                    = {{WISE 2014 challenge: Multi-label classification of print media articles to topics}},
  Author                   = {Tsoumakas, Grigorios and Papadopoulos, Apostolos and Qian, Weining and Vologiannidis, Stavros and D'yakonov, A. and Puurula, Antti and Read, Jesse and {\v{S}}vec, J. and Semenov, Stanislav},
  Journal                  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  Year                     = {2014},
  Pages                    = {541--548},
  Volume                   = {8787},

  Abstract                 = {{\textcopyright} Springer International Publishing Switzerland 2014. The WISE 2014 challenge was concerned with the task of multi-label classification of articles coming from Greek print media. Raw data comes from the scanning of print media, article segmentation, and optical character segmentation, and therefore is quite noisy. Each article is examined by a human annotator and categorized to one or more of the topics being monitored. Topics range from specific persons, products, and companies that can be easily categorized based on keywords, to more general semantic concepts, such as environment or economy. Building multi-label classifiers for the automated annotation of articles into topics can support the work of human annotators by suggesting a list of all topics by order of relevance, or even automate the annotation process for media and/or categories that are easier to predict. This saves valuable time and allows a media monitoring company to expand the portfolio of media being monitored. This paper summarizes the approaches of the top 4 among the 121 teams that participated in the competition.},
  Doi                      = {10.1007/978-3-319-11746-1_40},
  File                     = {:home/jan/Documents/Mendeley Desktop/Tsoumakas et al/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Tsoumakas et al. - 2014 - WISE 2014 challenge Multi-label classification of print media articles to topics.pdf:pdf},
  ISBN                     = {978-3-319-11745-4},
  ISSN                     = {16113349 03029743}
}

@Article{Tsoumakas2007a,
  Title                    = {{Random k-labelsets: An Ensemble Method for Multilabel Classification}},
  Author                   = {Tsoumakas, Grigorios and Vlahavas, Ioannis},
  Journal                  = {European Conference on Machine Learning},
  Year                     = {2007},
  Pages                    = {406--417},

  Abstract                 = {This paper proposes an ensemble method for multilabel clas- sification. TheRAndomk-labELsets (RAKEL) algorithm constructs each member of the ensemble by considering a small randomsubset of labels and learning a single-label classifier for the prediction of each element in the powerset of this subset. In this way, the proposed algorithm aims to take into account label correlations using single-label classifiers that are applied on subtasks with manageable number of labels and adequate number of examples per label. Experimental results on common multilabel domains involving protein, document and scene classification showthat better per- formance can be achieved compared to popular multilabel classification approaches.},
  Doi                      = {10.1007/978-3-540-74958-5_38},
  File                     = {:home/jan/Documents/Mendeley Desktop/Tsoumakas, Vlahavas/Unknown/Tsoumakas, Vlahavas - 2007 - Random k-labelsets An Ensemble Method for Multilabel Classification.pdf:pdf},
  ISBN                     = {6463501476},
  ISSN                     = {01681605},
  Pmid                     = {11518430},
  Url                      = {http://link.springer.com/chapter/10.1007/978-3-540-74958-5{\_}38}
}

@Article{Tsoumakas2012,
  Title                    = {{Introduction to the special issue on learning from multi-label data}},
  Author                   = {Tsoumakas, Grigorios and Zhang, Min-Ling and Zhou, Zhi-Hua},
  Journal                  = {Machine Learning},
  Year                     = {2012},

  Month                    = {jul},
  Number                   = {1-2},
  Pages                    = {1--4},
  Volume                   = {88},

  Doi                      = {10.1007/s10994-012-5292-9},
  ISSN                     = {0885-6125},
  Url                      = {http://link.springer.com/10.1007/s10994-012-5292-9}
}

@Article{Turnbull2008,
  Title                    = {{Semantic Annotation and Retrieval of Music and Sound Effects}},
  Author                   = {Turnbull, Douglas and Barrington, Luke and Torres, David and Lanckriet, Gert},
  Journal                  = {IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING},
  Year                     = {2008},
  Number                   = {2},
  Volume                   = {16},

  Abstract                 = {—We present a computer audition system that can both annotate novel audio tracks with semantically meaningful words and retrieve relevant tracks from a database of unlabeled audio content given a text-based query. We consider the related tasks of content-based audio annotation and retrieval as one supervised multiclass, multilabel problem in which we model the joint proba-bility of acoustic features and words. We collect a data set of 1700 human-generated annotations that describe 500 Western popular music tracks. For each word in a vocabulary, we use this data to train a Gaussian mixture model (GMM) over an audio feature space. We estimate the parameters of the model using the weighted mixture hierarchies expectation maximization algorithm. This algorithm is more scalable to large data sets and produces better density estimates than standard parameter estimation techniques. The quality of the music annotations produced by our system is comparable with the performance of humans on the same task. Our " query-by-text " system can retrieve appropriate songs for a large number of musically relevant words. We also show that our audition system is general by learning a model that can annotate and retrieve sound effects.},
  Doi                      = {10.1109/TASL.2007.913750},
  File                     = {:home/jan/Documents/Mendeley Desktop/Turnbull et al/IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING/Turnbull et al. - 2008 - Semantic Annotation and Retrieval of Music and Sound Effects.pdf:pdf},
  Keywords                 = {Index Terms—Audio annotation and retrieval,music informa-tion retrieval,semantic music analysis},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.2154{\&}rep=rep1{\&}type=pdf}
}

@Article{Vens2008,
  Title                    = {{Decision trees for hierarchical multi-label classification}},
  Author                   = {Vens, Celine and Struyf, Jan and Schietgat, Leander and D{\v{z}}eroski, Sa{\v{s}}o and Blockeel, Hendrik},
  Journal                  = {Machine Learning},
  Year                     = {2008},
  Number                   = {2},
  Pages                    = {185--214},
  Volume                   = {73},

  Abstract                 = {Hierarchical multi-label classification (HMC) is a variant of classification where instances may belong to multiple classes at the same time and these classes are organized in a hierarchy. This article presents several approaches to the induction of decision trees for HMC, as well as an empirical study of their use in functional genomics. We compare learning a single HMC tree (which makes predictions for all classes together) to two approaches that learn a set of regular classification trees (one for each class). The first approach defines an independent single-label classification task for each class (SC). Obviously, the hierarchy introduces dependencies between the classes. While they are ignored by the first approach, they are exploited by the second approach, named hierarchical single-label classification (HSC). Depending on the application at hand, the hierarchy of classes can be such that each class has at most one parent (tree structure) or such that classes may have multiple parents (DAG structure). The latter case has not been considered before and we show how the HMC and HSC approaches can be modified to support this setting. We compare the three approaches on 24 yeast data sets using as classification schemes MIPS's FunCat (tree structure) and the Gene Ontology (DAG structure). We show that HMC trees outperform HSC and SC trees along three dimensions: predictive accuracy, model size, and induction time. We conclude that HMC trees should definitely be considered in HMC tasks where interpretable models are desired.},
  Doi                      = {10.1007/s10994-008-5077-3},
  File                     = {:home/jan/Documents/Mendeley Desktop/Vens et al/Machine Learning/Vens et al. - Unknown - Decision Trees for Hierarchical Multi-label Classification.pdf:pdf},
  ISBN                     = {0885-6125$\backslash$r1573-0565},
  ISSN                     = {08856125},
  Keywords                 = {Decision trees,Functional genomics,Hierarchical classification,Multi-label classification,Precision-recall analysis},
  Url                      = {https://lirias.kuleuven.be/bitstream/123456789/186698/4/hmc.pdf}
}

@Article{Wang2008,
  Title                    = {{Transductive multi-label learning for video concept detection}},
  Author                   = {Wang, Jingdong and Zhao, Yinghai and Wu, Xiuqing and Hua, Xian-Sheng},
  Journal                  = {Proceeding of the 1st ACM international conference on Multimedia information retrieval - MIR '08},
  Year                     = {2008},
  Pages                    = {298},

  Abstract                 = {Transductive video concept detection is an effective way to handle the lack of sufficient labeled videos. However, another issue, the multi-label interdependence, is not es-sentially addressed in the existing transductive methods. Most solutions only applied the transductive single-label ap-proach to detect each individual concept separately, but ig-noring the concept relation, or simply imposed the smooth-ness assumption over the multiple labels for each video, without indeed exploring the interdependence between the concepts. On the other hand, the semi-supervised exten-sion of supervised multi-label classifiers, such as correlative multi-label support vector machines, is usually intractable and hence impractical due to the quite expensive compu-tational cost. In this paper, we propose an effective trans-ductive multi-label classification approach, which simultane-ously models the labeling consistency between the visually similar videos and the multi-label interdependence for each video in an integrated framework. We compare the per-formance between the proposed approach and several rep-resentative transductive single-label and supervised multi-label classification approaches for the video concept detec-tion task over the widely-used TRECVID data set. The comparative results demonstrate the superiority of the pro-posed approach.},
  Annote                   = {NULL},
  Doi                      = {10.1145/1460096.1460145},
  File                     = {:home/jan/Documents/Mendeley Desktop/Wang et al/Proceeding of the 1st ACM international conference on Multimedia information retrieval - MIR '08/Wang et al. - Unknown - Transductive Multi-Label Learning for Video Concept Detection.pdf:pdf},
  ISBN                     = {9781605583129},
  Keywords                 = {multi-label,transductive learning,video concept detection},
  Url                      = {https://pdfs.semanticscholar.org/c1d3/4d048074fa5b6f390715e37e822e0a5d9f25.pdf http://portal.acm.org/citation.cfm?doid=1460096.1460145}
}

@Article{Wang2016a,
  Title                    = {{Multi-label emotion recognition of weblog sentence based on Bayesian networks}},
  Author                   = {Wang, Lei and Ren, Fuji and Miao, Duoqian},
  Journal                  = {IEEJ Transactions on Electrical and Electronic Engineering},
  Year                     = {2016},
  Number                   = {2},
  Pages                    = {178--184},
  Volume                   = {11},

  Doi                      = {10.1002/tee.22204},
  File                     = {:home/jan/Documents/Mendeley Desktop/Wang, Ren, Miao/IEEJ Transactions on Electrical and Electronic Engineering/Wang, Ren, Miao - 2016 - Multi-label emotion recognition of weblog sentence based on Bayesian networks.pdf:pdf},
  ISSN                     = {19314973},
  Keywords                 = {Emotion Recognition,Multi-Label,emotion recognition,gibbs sampling,latent dirichlet allocation,multi-label classification,received 27 july 2013,revised 14 december 2014},
  Mendeley-tags            = {Emotion Recognition,Multi-Label},
  Url                      = {http://doi.wiley.com/10.1002/tee.22204}
}

@Article{Weston2010,
  Title                    = {{Web Scale Image Annotation : Learning to Rank with Joint Word-Image Embeddings Image Annotation}},
  Author                   = {Weston, Jason and Bengio, Samy and Usunier, Nicolas},
  Journal                  = {Ecml/Pkdd},
  Year                     = {2010},
  Number                   = {June},
  Pages                    = {1--26},

  Abstract                 = {Image annotation datasets are becoming larger and larger, with tens of millions of images and tens of thousands of possible anno-tations. We propose a strongly performing method that scales to such datasets by simultaneously learning to optimize precision at k of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations. Our method both outperforms several baseline methods and, in comparison to them, is faster and consumes less memory. We also demonstrate how our method learns an interpretable model, where annotations with alternate spellings or even languages are close in the embedding space. Hence, even when our model does not predict the exact annotation given by a human la-beler, it often predicts similar annotations, a fact that we try to quantify by measuring the newly introduced " sibling " precision metric, where our method also obtains excellent results.},
  Annote                   = {NULL},
  File                     = {:home/jan/Documents/Mendeley Desktop/Weston, Bengio, Usunier/EcmlPkdd/Weston, Bengio, Usunier - Unknown - Large Scale Image Annotation Learning to Rank with Joint Word-Image Embeddings.pdf:pdf},
  ISSN                     = {0885-6125},
  Keywords                 = {ranking}
}

@Article{Wu2014,
  Title                    = {{Three predictions are better than one: Sentence multi-emotion analysis from different perspectives}},
  Author                   = {Wu, Yunong and Kita, Kenji and Matsumoto, Kazuyuki},
  Journal                  = {IEEJ Transactions on Electrical and Electronic Engineering},
  Year                     = {2014},
  Number                   = {6},
  Pages                    = {642--649},
  Volume                   = {9},

  Doi                      = {10.1002/tee.22020},
  File                     = {:home/jan/Documents/Mendeley Desktop/Wu, Kita, Matsumoto/IEEJ Transactions on Electrical and Electronic Engineering/Wu, Kita, Matsumoto - 2014 - Three predictions are better than one Sentence multi-emotion analysis from different perspectives.pdf:pdf},
  ISSN                     = {19314973},
  Keywords                 = {Emotion Analysis,Multi-Label,conditional random fields,crf,integrated prediction,l-lda,labeled latent dirichlet allocation,lgr,logistic,multi-emotion,received 28 june 2013,regression,revised 4 january 2014,sentence emotion analysis},
  Mendeley-tags            = {Emotion Analysis,Multi-Label},
  Url                      = {http://doi.wiley.com/10.1002/tee.22020}
}

@Article{Xie,
  Title                    = {{Transfer Learning from Deep Features for Remote Sensing and Poverty Mapping}},
  Author                   = {Xie, Michael and Jean, Neal and Burke, Marshall and Lobell, David and Ermon, Stefano},

  Abstract                 = {The lack of reliable data in developing countries is a major obstacle to sustainable development, food secu-rity, and disaster relief. Poverty data, for example, is typically scarce, sparse in coverage, and labor-intensive to obtain. Remote sensing data such as high-resolution satellite imagery, on the other hand, is becoming in-creasingly available and inexpensive. Unfortunately, such data is highly unstructured and currently no tech-niques exist to automatically extract useful insights to inform policy decisions and help direct humanitarian ef-forts. We propose a novel machine learning approach to extract large-scale socioeconomic indicators from high-resolution satellite imagery. The main challenge is that training data is very scarce, making it difficult to apply modern techniques such as Convolutional Neural Net-works (CNN). We therefore propose a transfer learn-ing approach where nighttime light intensities are used as a data-rich proxy. We train a fully convolutional CNN model to predict nighttime lights from daytime imagery, simultaneously learning features that are use-ful for poverty prediction. The model learns filters iden-tifying different terrains and man-made structures, in-cluding roads, buildings, and farmlands, without any su-pervision beyond nighttime lights. We demonstrate that these learned features are highly informative for poverty mapping, even approaching the predictive performance of survey data collected in the field.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Xie et al/Unknown/Xie et al. - Unknown - Transfer Learning from Deep Features for Remote Sensing and Poverty Mapping.pdf:pdf},
  Url                      = {https://arxiv.org/pdf/1510.00098.pdf}
}

@Article{Xu2016,
  Title                    = {{Robust Extreme Multi-Label Learning}},
  Author                   = {Xu, Chang and Tao, Dacheng and Xu, Chao},
  Journal                  = {KDD},
  Year                     = {2016},
  Pages                    = {421--434},

  Abstract                 = {Tail labels in the multi-label learning problem undermine the low-rank assumption. Nevertheless, this problem has rarely been investigated. In addition to using the low-rank structure to depict label correlations, this paper explores and exploits an additional sparse component to handle tail labels behaving as outliers, in order to make the classical low-rank principle in multi-label learning valid. The divideand-conquer optimization technique is employed to increase the scalability of the proposed algorithm while theoretically guaranteeing its performance. A theoretical analysis of the generalizability of the proposed algorithm suggests that it can be improved by the low-rank and sparse decomposition given tail labels. Experimental results on real-world data demonstrate the significance of investigating tail labels and the effectiveness of the proposed algorithm.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {arXiv:1602.05561v1},
  Doi                      = {10.475/123},
  Eprint                   = {arXiv:1602.05561v1},
  File                     = {:home/jan/Documents/Mendeley Desktop/Xu, Tao, Xu/KDD/Xu, Tao, Xu - 2016 - Robust Extreme Multi-Label Learning.pdf:pdf},
  ISBN                     = {9781450335423},
  ISSN                     = {0146-4833},
  Keywords                 = {low-rank algorithm,multi-label learning}
}

@Article{Yang1999,
  Title                    = {{An Evaluation of Statistical Approaches to Text Categorization}},
  Author                   = {Yang, Yiming},
  Journal                  = {Information Retrieval},
  Year                     = {1999},
  Number                   = {1},
  Pages                    = {69--90},
  Volume                   = {1},

  Abstract                 = {This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance; except for a Naive Bayes approach, the other learning algorithms also performed relatively well.},
  Doi                      = {10.1023/A:1009982220290},
  File                     = {:home/jan/Documents/Mendeley Desktop/Yang/Information Retrieval/Yang - 1999 - An Evaluation of Statistical Approaches to Text Categorization.pdf:pdf},
  ISBN                     = {1386-4564},
  ISSN                     = {1612-1880},
  Keywords                 = {comparative study,evaluation,statistical learning algorithms,text categorization},
  Url                      = {http://www.cs.cmu.edu/{~}{\%}5Cnhttp://dx.doi.org/10.1023/A:1009982220290}
}

@Article{Yu,
  Title                    = {{Large-scale Multi-label Learning with Missing Labels.}},
  Author                   = {Yu, H F and Jain, P and Kar, P and Dhillon, I S},
  Journal                  = {Icml},
  Year                     = {2014},

  Abstract                 = {Abstract The multi-label classification problem has generated significant interest in recent years. However, existing approaches do not adequately address two key challenges:(a) scaling up to problems with a large number (say millions) of labels, and (b) handling data ...},
  File                     = {:home/jan/Documents/Mendeley Desktop/Yu et al/Icml/Yu et al. - 2014 - Large-scale Multi-label Learning with Missing Labels.pdf:pdf},
  Url                      = {http://www.jmlr.org/proceedings/papers/v32/yu14.pdf}
}

@Article{Yu2014,
  Title                    = {{Multi-label classification by exploiting label correlations}},
  Author                   = {Yu, Ying and Pedrycz, Witold and Miao, Duoqian},
  Journal                  = {Expert Systems with Applications},
  Year                     = {2014},
  Number                   = {6},
  Pages                    = {2989--3004},
  Volume                   = {41},

  Abstract                 = {Nowadays, multi-label classification methods are of increasing interest in the areas such as text categorization, image annotation and protein function classification. Due to the correlation among the labels, traditional single-label classification methods are not directly applicable to the multi-label classification problem. This paper presents two novel multi-label classification algorithms based on the variable precision neighborhood rough sets, called multi-label classification using rough sets (MLRS) and MLRS using local correlation (MLRS-LC). The proposed algorithms consider two important factors that affect the accuracy of prediction, namely the correlation among the labels and the uncertainty that exists within the mapping between the feature space and the label space. MLRS provides a global view at the label correlation while MLRS-LC deals with the label correlation at the local level. Given a new instance, MLRS determines its location and then computes the probabilities of labels according to its location. The MLRS-LC first finds out its topic and then the probabilities of new instance belonging to each class is calculated in related topic. A series of experiments reported for seven multi-label datasets show that MLRS and MLRS-LC achieve promising performance when compared with some well-known multi-label learning algorithms. ?? 2013 Elsevier Ltd. All rights reserved.},
  Doi                      = {10.1016/j.eswa.2013.10.030},
  File                     = {:home/jan/Documents/Mendeley Desktop/Yu, Pedrycz, Miao/Expert Systems with Applications/Yu, Pedrycz, Miao - 2014 - Multi-label classification by exploiting label correlations.pdf:pdf},
  ISSN                     = {09574174},
  Keywords                 = {Correlation,Multi-label classification,Rough sets,Uncertainty}
}

@Article{Zhang2005,
  Title                    = {{A k-nearest neighbor based algorithm for multi-label classification}},
  Author                   = {Zhang, ML},
  Journal                  = {Granular Computing, 2005 IEEE},
  Year                     = {2005},
  Pages                    = {718--721},

  Abstract                 = {— In multi-label learning, each instance in the training set is associated with a set of labels, and the task is to output a label set whose size is unknown a priori for each unseen instance. In this paper, a multi-label lazy learning approach named ML-kNN is presented, which is derived from the traditional k-nearest neighbor (kNN) algorithm. In detail, for each new instance, its k nearest neighbors are firstly identified. After that, according to the label sets of these neighboring instances, maximum a posteriori (MAP) principle is utilized to determine the label set for the new instance. Experiments on a real-world multi-label bioinformatic data show that ML-kNN is highly comparable to existing multi-label learning algorithms.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Zhang/Granular Computing, 2005 IEEE/Zhang - 2005 - A k-nearest neighbor based algorithm for multi-label classification.pdf:pdf},
  ISBN                     = {0780390172},
  Url                      = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1547385}
}

@Article{Zhang2009a,
  Title                    = {{Feature selection for multi-label naive Bayes classification}},
  Author                   = {Zhang, Min Ling and Pe{\~{n}}a, Jos{\'{e}} M and Robles, Victor},
  Journal                  = {Information Sciences},
  Year                     = {2009},
  Number                   = {19},
  Pages                    = {3218--3229},
  Volume                   = {179},

  Abstract                 = {In multi-label learning, the training set is made up of instances each associated with a set of labels, and the task is to predict the label sets of unseen instances. In this paper, this learning problem is addressed by using a method called Mlnb which adapts the traditional naive Bayes classifiers to deal with multi-label instances. Feature selection mechanisms are incorporated into Mlnb to improve its performance. Firstly, feature extraction techniques based on principal component analysis are applied to remove irrelevant and redundant features. After that, feature subset selection techniques based on genetic algorithms are used to choose the most appropriate subset of features for prediction. Experiments on synthetic and real-world data show that Mlnb achieves comparable performance to other well-established multi-label learning algorithms. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
  Doi                      = {10.1016/j.ins.2009.06.010},
  File                     = {:home/jan/Documents/Mendeley Desktop/Zhang, Pe{\~{n}}a, Robles/Information Sciences/Zhang, Pe{\~{n}}a, Robles - Unknown - Author's personal copy Feature selection for multi-label naive Bayes classification.pdf:pdf},
  ISBN                     = {0020-0255},
  ISSN                     = {00200255},
  Keywords                 = {Feature selection,Genetic algorithm,Multi-label learning,Naive Bayes,Principal component analysis},
  Url                      = {http://www.elsevier.com/copyright}
}

@Article{Zhang2014,
  Title                    = {{A review on multi-label learning algorithms}},
  Author                   = {Zhang, Min Ling and Zhou, Zhi Hua},
  Journal                  = {IEEE Transactions on Knowledge and Data Engineering},
  Year                     = {2014},
  Number                   = {8},
  Pages                    = {1819--1837},
  Volume                   = {26},

  Abstract                 = {Multi-label learning studies the problem where each example is represented by a single instance while associated with a set of labels simultaneously. During the past decade, significant amount of progresses have been made toward this emerging machine learning paradigm. This paper aims to provide a timely review on this area with emphasis on state-of-the-art multi-label learning algorithms. Firstly, fundamentals on multi-label learning including formal definition and evaluation metrics are given. Secondly and primarily, eight representative multi-label learning algorithms are scrutinized under common notations with relevant analyses and discussions. Thirdly, several related learning settings are briefly summarized. As a conclusion, online resources and open research problems on multi-label learning are outlined for reference purposes.},
  Doi                      = {10.1109/TKDE.2013.39},
  File                     = {:home/jan/Documents/Mendeley Desktop/Zhang, Zhou/IEEE Transactions on Knowledge and Data Engineering/Zhang, Zhou - 2014 - A review on multi-label learning algorithms.pdf:pdf},
  ISBN                     = {1041-4347},
  ISSN                     = {10414347},
  Keywords                 = {Multi-label learning,algorithm adaptation,label correlations-problem transformation}
}

@Article{Zhanga,
  Title                    = {{LIFT: Multi-Label Learning with Label-Specific Features}},
  Author                   = {Zhang, Min-Ling and Wu, Lei},

  Abstract                 = {—Multi-label learning deals with the problem where each example is represented by a single instance (feature vector) while associated with a set of class labels. Existing approaches learn from multi-label data by manipulating with identical feature set, i.e. the very instance representation of each example is employed in the discrimination processes of all class labels. However, this popular strategy might be suboptimal as each label is supposed to possess specific characteristics of its own. In this paper, another strategy to learn from multi-label data is studied, where label-specific features are exploited to benefit the discrimination of different class labels. Accordingly, an intuitive yet effective algorithm named LIFT, i.e. multi-label learning with Label specIfic FeaTures, is proposed. LIFT firstly constructs features specific to each label by conducting clustering analysis on its positive and negative instances, and then performs training and testing by querying the clustering results. Comprehensive experiments on a total of seventeen benchmark data sets clearly validate the superiority of LIFT against other well-established multi-label learning algorithms as well as the effectiveness of label-specific features.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Zhang, Wu/Unknown/Zhang, Wu - Unknown - LIFT Multi-Label Learning with Label-Specific Features.pdf:pdf},
  Keywords                 = {Index Terms—machine learning,label correlations,label-specific features,multi-label learning}
}

@Article{Zhang2010,
  Title                    = {{Multi-label learning by exploiting label dependency}},
  Author                   = {Zhang, Min-Ling and Zhang, Kun},
  Journal                  = {Kdd},
  Year                     = {2010},
  Pages                    = {999--1007},

  Abstract                 = {In multi-label learning, each training example is associated with a set of labels and the task is to predict the proper label set for the unseen example. Due to the tremendous (exponential) number of possible label sets, the task of learning from multi-label examples is rather challenging. Therefore, the key to successful multi-label learning is how to effectively exploit correlations between different labels to facilitate the learning process. In this paper, we propose to use a Bayesian network structure to efficiently encode the condi- tional dependencies of the labels as well as the feature set, with the feature set as the common parent of all labels. To make it practical, we give an approximate yet efficient procedure to find such a network structure. With the help of this network, multi-label learning is decomposed into a series of single-label classification problems, where a classifier is constructed for each label by incorporating its parental labels as additional features. Label sets of unseen examples are predicted recursively according to the label ordering given by the network. Extensive experiments on a broad range of data sets validate the effectiveness of our approach against other well-established methods.},
  Annote                   = {NULL},
  Doi                      = {10.1145/1835804.1835930},
  File                     = {:home/jan/Documents/Mendeley Desktop/Zhang, Zhang/Kdd/Zhang, Zhang - 2010 - Multi-label learning by exploiting label dependency.pdf:pdf},
  ISBN                     = {9781450300551},
  ISSN                     = {9781577355687},
  Keywords                 = {Learning—concept learn-ing,induction General Terms Algorithms},
  Url                      = {http://dl.acm.org/citation.cfm?doid=1835804.1835930}
}

@Article{Zhang2007,
  Title                    = {{ML-KNN: A lazy learning approach to multi-label learning}},
  Author                   = {Zhang, Min-Ling and Zhou, Zhi-Hua},
  Journal                  = {Pattern Recognition},
  Year                     = {2007},
  Pages                    = {2038--2048},
  Volume                   = {40},

  Abstract                 = {Multi-label learning originated from the investigation of text categorization problem, where each document may belong to several predefined topics simultaneously. In multi-label learning, the training set is composed of instances each associated with a set of labels, and the task is to predict the label sets of unseen instances through analyzing training instances with known label sets. In this paper, a multi-label lazy learning approach named ML-KNN is presented, which is derived from the traditional K-nearest neighbor (KNN) algorithm. In detail, for each unseen instance, its K nearest neighbors in the training set are firstly identified. After that, based on statistical information gained from the label sets of these neighboring instances, i.e. the number of neighboring instances belonging to each possible class, maximum a posteriori (MAP) principle is utilized to determine the label set for the unseen instance. Experiments on three different real-world multi-label learning problems, i.e. Yeast gene functional analysis, natural scene classification and automatic web page categorization, show that ML-KNN achieves superior performance to some well-established multi-label learning algorithms.},
  Doi                      = {10.1016/j.patcog.2006.12.019},
  File                     = {:home/jan/Documents/Mendeley Desktop/Zhang, Zhou/Pattern Recognition/Zhang, Zhou - 2007 - ML-KNN A lazy learning approach to multi-label learning.pdf:pdf},
  Keywords                 = {Functional genomics,K-nearest neighbor,Lazy learning,Machine learning,Multi-label learning,Natural scene classification,Text categorization},
  Url                      = {www.elsevier.com/locate/pr}
}

@Article{Zhang2006,
  Title                    = {{Multilabel Neural Networks with Applications to Functional Genomics and Text Categorization}},
  Author                   = {Zhang, Min-ling and Zhou, Zhi-hua and Member, Senior},
  Year                     = {2006},
  Number                   = {10},
  Pages                    = {1338--1351},
  Volume                   = {18},

  File                     = {:home/jan/Documents/Mendeley Desktop/Zhang, Zhou, Member/Unknown/Zhang, Zhou, Member - 2006 - Multilabel Neural Networks with Applications to Functional Genomics and Text Categorization.pdf:pdf}
}

@Article{Zhang2003,
  Title                    = {{System and Method for Automatic Singer Identification System and Method for Automatic Singer Identification}},
  Author                   = {Zhang, Tong},
  Journal                  = {IEEE International Conference on Multimedia and Expo},
  Year                     = {2003},
  Number                   = {July},
  Pages                    = {6--9},

  Abstract                 = {The singer's information is essential in organizing, browsing and retrieving music collections. In this technical report, a system for automatic singer identification is developed which recognizes the singer of a song by analyzing the music signal. Meanwhile, songs which are similar in terms of singer's voice are clustered. The proposed scheme follows the framework of common speaker identification systems, but special efforts are made to distinguish the singing voice from instrumental sounds in a song. A statistical model is trained for each singer's voice with typical song(s) of the singer. Then, for a song to be identified, the starting point of singing voice is detected and a portion of the song is excerpted from that point. Audio features are extracted and matched with singers' voice models in the database. The song is assigned to the model having the best match. Promising results are obtained on a small set of samples, and accuracy rates of around 80{\%} are achieved.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Zhang/IEEE International Conference on Multimedia and Expo/Zhang - 2003 - System and Method for Automatic Singer Identification System and Method for Automatic Singer Identification.pdf:pdf},
  ISBN                     = {2974619401}
}

@Article{Zhangb,
  Title                    = {{Deep Extreme Multi-label Learning}},
  Author                   = {Zhang, Wenjie and Wang, Liwei and Yan, Junchi and Wang, Xiangfeng and Zha, Hongyuan},

  Abstract                 = {Extreme multi-label learning or classification has been a practical and important problem since the boom of big data. The main challenge lies in the exponential label space which involves 2 L possible label sets when the label dimension L is very large e.g. in millions for Wikipedia labels. This paper is motivated to better explore the label space by build-ing and modeling an explicit label graph. In the meanwhile, deep learning has been widely studied and used in various classification problems includ-ing multi-label classification, however it has not been sufficiently studied in this extreme but practi-cal case, where the label space can be as large as in millions. In this paper, we propose a practical deep embedding method for extreme multi-label classifi-cation. Our method harvests the ideas of non-linear embedding and modeling label space with graph priors at the same time. Extensive experiments on public datasets for XML show that our method per-form competitively against state-of-the-art result.},
  File                     = {:home/jan/Documents/Mendeley Desktop/Zhang et al/Unknown/Zhang et al. - Unknown - Deep Extreme Multi-label Learning.pdf:pdf},
  Url                      = {https://pdfs.semanticscholar.org/67d7/8725836ff0f4b00cde92b772c0e90466dc6a.pdf https://arxiv.org/pdf/1704.03718.pdf}
}

@article{Zhu2017,
  author    = {Feng Zhu and
               Hongsheng Li and
               Wanli Ouyang and
               Nenghai Yu and
               Xiaogang Wang},
  title     = {Learning Spatial Regularization with Image-level Supervisions for
               Multi-label Image Classification},
  journal   = {CoRR},
  volume    = {abs/1702.05891},
  year      = {2017},
  url       = {http://arxiv.org/abs/1702.05891},
  archivePrefix = {arXiv},
  eprint    = {1702.05891},
  timestamp = {Wed, 07 Jun 2017 14:42:45 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/ZhuLOYW17},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@Article{Zhu,
  Title                    = {{Multi-Label Learning with Global and Local Label Correlation}},
  Author                   = {Zhu, Yue and Kwok, James T and Zhou, Zhi-Hua},
  Year                     = {2017},

  Abstract                 = {It is well-known that exploiting label correlations is important to multi-label learning. Existing approaches either assume that the label correlations are global and shared by all instances; or that the label correlations are local and shared only by a data subset. In fact, in the real-world applications, both cases may occur that some label correlations are globally applicable and some are shared only in a local group of instances. Moreover, it is also a usual case that only partial labels are observed, which makes the exploitation of the label correlations much more difficult. That is, it is hard to estimate the label correlations when many labels are absent. In this paper, we propose a new multi-label approach GLOCAL dealing with both the full-label and the missing-label cases, exploiting global and local label correlations simultaneously, through learning a latent label representation and optimizing label manifolds. The extensive experimental studies validate the effectiveness of our approach on both full-label and missing-label data.},
  Archiveprefix            = {arXiv},
  Arxivid                  = {1704.01415},
  Eprint                   = {1704.01415},
  File                     = {:home/jan/Documents/Mendeley Desktop/Zhu, Kwok, Zhou/Unknown/Zhu, Kwok, Zhou - Unknown - Multi-Label Learning with Global and Local Label Correlation.pdf:pdf},
  Keywords                 = {Global and local label correlation,label manifold,missing labels,multi-label learning},
  Url                      = {https://arxiv.org/pdf/1704.01415.pdf http://arxiv.org/abs/1704.01415}
}

@article{Zagoruyko2016,
  author    = {Sergey Zagoruyko and
               Nikos Komodakis},
  title     = {Wide Residual Networks},
  journal   = {CoRR},
  volume    = {abs/1605.07146},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.07146},
  timestamp = {Wed, 07 Jun 2017 14:41:15 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ZagoruykoK16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
