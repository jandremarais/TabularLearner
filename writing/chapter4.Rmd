# Deep Learning for Tabular Data \label{chp:td}

## Introduction

Up to now we have covered the basics of neural networks, as well as the more recent advancements in the era of deep learning.
The aim of this chapter is to explore how the approaches and techniques we discussed can be leveraged when doing deep learning with tabular data.
Tabular data is vastly different to so-called unstructured data like images, text and speech and therefore we do not expect all of the methods to be as effective.
Deep learning on tabular data is not as well researched compared to the aformentioned data domains and thus it is not always clear how to solve certain modelling challenges.
There is only a handful of publications reporting sucessful implementations of deep learning for tabular data on important applications such as recommender systems [@haldar2018], click-through rate prediction[^ctr] [@Song2018], analysis of electronic health records [@Rajkomar2018] and transport related problems [@Brebisson2015], to name a few.

The tabular data domain is still dominated by tree-based models like random forests and gradient boosted trees.
This makes us curious as to why deep learning is not as effective here as it is in most other data domains.
This chapter will help shed light on this issue and point to promising ways of improving the current state-of-the-art.

This chapter is structured based on the tabular dataset challenges introduced in \Sref{sec:tabchal}.
For each of the challenges we provide a recap of the issue, review the literature to discuss how they are currently being treated, and provide suggestions for improving these approaches, (where applicable).
In \Sref{sec:inp_rep} we look at ways of representing the input features of a tabular dataset.
\Sref{sec:feat_int} is abut approaches for leveraging feature interactions.
A large part of this chapter is on how to be more sample efficient; this we discuss in detail in \Sref{sec:samp_eff}.
Then we briefly investigate ways of interpreting deep neural networks for tabular data in \Sref{sec:interp}.
In the final section \Sref{sec:hs_tb} we discuss the 1Cycle policy and hyperparameter selection for tabular data neural networks, in addition to other miscellaneous topics that do not fit into the above categorisation.

[^ctr]: To predict the probability of a user clicking an item, critical to online applications.

## Input Representation \label{sec:inp_rep}

One of the major design considerations when building a deep neural network for tabular data is the input representation, *i.e.* how should one represent each feature numerically?
This choice may heavily influnce the model's ability to extract patterns from the input and the effectiveness of the optimisation algorithm.
What makes this decision harder is that the features in a tabular dataset are highly heterogeneous [@Shavitt2018].
What we decide for one features might not be optimal for another and we want to ensure that no feature dominates another in the training process.

A tabular dataset typically has continuous features and categorical features.
Different approaches are needed to process each.
But the processing should be done in such a way that the one feature type does not dominate the other in terms of feature extraction and optimisation.

A tabular dataset can be high-dimensional and very sparse which makes the task more difficult but even more important to do effectively, as noted by many [@Song2018, @Wang2017b, @Qu2016, @Cheng2016, @anonymous2019, @Covington2016].
An example of one such challenging tabular dataset is the Criteo dataset[^criteo].
Its feature dimension is ~30 million, exhibiting sparsity of ~99%.
Although not always as extreme, these properties are common in tabular datasets.

[^criteo]: https://www.kaggle.com/c/criteo-display-ad-challenge

### Numerical Features

One of the things that make tree-based methods so attractive is that the scale and the distribution of the features hardly matter, as long as their relative ordering is meaningful.
With neural networks, we are not that fortunate.
Neural networks are very sensitive to the scale and distribution of its inputs [@Ioffe2015]. 
If the features are on different scales, one might dominate the weight updates, and if a feature constains a large value, it may throw of the optimisation procedure and cause exploding or vanishing gradients [@Clevert2015].
This means that one should do a proper standardisation of all continuous features in a tabular dataset.

The typical standardisation approach for numerical features in deep learning is to do mean centering and variance scaling, *i.e.* $\tilde{x}=(x-\mu)/\sigma$, where $\mu$and $\sigma$ is the mean and standard deviation of $X$ respectively ($x\in X$).
One would expect this transformation to be sufficient, but in practice, many have found otherwise.

In [@haldar2018] they suggest to first inspect the distribution of each of the features.
If a feature looks gaussian, do the standard normalisation, $(x-\mu)/\sigma$, but if the feature looks more like a power law distribution, transform it by $\log\left((1+x)/(1+\text{median})\right)$.
This ensures that the bulk of the values lie between {-1,1} and that the median is close to zero.
As an example, we illustrate what effect these transformations have on 2 continuous features in the Adult dataset, Age and Hours-per-week, in \autoref{fig:contnorm}.
In \autoref{fig:contnorm} (a) we see that the features are on a roughly similar scale but their distributions are totally different.
By applying mean centering and unit variance scaling (b) we see the values of the Hours-per-week feature are mostly in {-1,1} but for the Age features there are still plenty of values outside that range.
When doing the power distribution transformation of [@haldar2018], we observe that the Age feature is now in the proposed value range.
The downside to this approach is that it is not automatic and can become cumbersome with many variables.

```{r, out.width="33%", fig.cap='Effect of normalisation on continuous variables.\\label{fig:contnorm}', fig.subcap=c('Original', 'Gaussian Norm', 'Power Norm'), fig.show='hold'}
knitr::include_graphics(c('figures/cont_vars.pdf', 'figures/gaus_norm.pdf', 'figures/power_norm.pdf'))
```

To reduce the possibly high variance of numeric features [@Song2018] suggests to transform the a numeric feature to $\log^{2}(x)$ if $x>2$.
This worked in their use-case but it is hard to imagine that this work on all other cases.
This transformation causes a discontinuity at $x=2$ and a possible overlap between values the were originally less than 2 with those that were greater than 2.
In addition, this transformation does not address the extreme values on the negative side.
[@Wang2017b] simply used a standard log transform, $\log{x}$, to normalise continuous features.

In [@Covington2016] they also found that the proper normalisation of numeric features were critical for the model to converge.
Their approach was to transform the numeric features to be equally distributed in [0,1) using the cumulative distribution $\tilde{x}=\int_{\infty}^{x}df$, where $f$ is the distribution of $x$.
They approximated the integral with linear interpolation on the quantiles of the feature values computed as a preprocessing step.

Another possibility for numeric feature normalisation that we have not seen used in the deep learning for tabular data literature, is to use a batch normalisation layer as the initial layer to numeric features.
This will apply the same type of scaling as the zero mean and unit variance transformation, but with transformation parameters learned from each batch.
Thus the need to preprocess numeric features is removed.
The caveat is that the quality of the transformation depends on the batch statistics and thus if the batch is not representative of the full data distribution (which is likely if the batch size is small), then it might derail the training procedure.

The fact is that the only way to know for sure which normalisation to apply to numeric features is by experimations, since it varies between datasets.
The standard normal scaling is a safe transformation to use if experimentation wants to be avoided.

### Categorical Features

Most of the sparsity in tabular datasets come from categorical features.
Since neural networks cannot process discrete categories or objects, we need to find a numeric representation for each class.
The standard approach is to one-hot encode categorical features.
That is, if we have a categorical feature with three possible categories, for example, the one-hot encoded form of the three categories will be: Category 1: $[1,0,0]$, Category 2: $[0,1,0]$ and $[0,0,1]$ for Category 3.

There are multiple inefficiencies when using one-hot encodings with neural networks.
The obvious one we have already mentioned is that it introduces sparsity to the data, since the dimension of the one-hot encoded form is equal to the number or categories in a feature.
Thus if we have plenty of high-cardinality features in our dataset, the data will be extremely sparse and difficult to model.
This also increases the size needed of the first linear layer and thus we need machines with greater memory and processing power.
Neural networks can easily overfit such sparse data [@Covington2016].

The other problem with one-hot encodings of categorical features is that there is no notion of similarity and distances between categories.
In this representation, all categories are equally far apart now matter how semantically similar or dissimilar they are.
This makes it harder for the model to learn useful patterns.

An alternative to one-hot encodings as representations of categorical features for neural networks are *entity embeddings*.
An entity embedding is the exact same operation as (word) embeddings we have discussed in \Sref{sec:embeds}, but applied to categories instead of words.
Therefore an entity embedding assigns a numeric vector representation to each category in a categorical feature, for example: Category 1: $[0.05,-0.1,0.2]$, Category 2: $[0.2,0.01,0.3]$ and $[-0.1,-0.2,0.05]$ for Category 3.
Once all of the categorical features have been embedded, their representations can be concatenated and passed to the rest of the network.

The first published work in modern times on entity embeddings was in the taxi destination prediction challenge [@Brebisson2015].
In another Kaggle sucess story, [@Guo2016] succesfully used entity embeddings for predicting the total sales of a store.
Companies like [Instacart](https://tech.instacart.com/deep-learning-with-emojis-not-math-660ba1ad6cdc) and [Pinterest](https://medium.com/the-graph/applying-deep-learning-to-related-pins-a6fee3c92f5e) have reported the effective use of entity embeddings on their internal datasets. 
Currently, all research on deep learning for tabular data makes use of entity embeddings - see for example [@Song2018], [@Wang2017b], [@Covington2016] and [@Zhou2017].

The reason why it is used all-round is because it does not have the same issues as the one-hot encoded representations.
A similar formulation to \Sref{sec:embeds}, we define the embedding for the $j$-th categorical feature by:
$$
\boldsymbol{e}_{j}=V_{j}\boldsymbol{x}_{j}
$$
where $\boldsymbol{x}_{j}$ is the one-hot encoded vector representation of the $j$-th categorical variable and $V_{j}$ is the associated embedding/weight matrix.
The weights in $V_{j}$ are learned along with all of the other parameters in the network.

The first advantage of entity embeddings is that it speeds up training and reduces the memory footprint which further improves the generalisation ability of the network [@Covington2016], [@Guo2016].
This especially useful when working with dimensional and sparse inputs.
Suppose we have a dataset with two categorical features, $X_{1}$ and $X_{2}$, with cardinality of $C_{1}$ and $C_{2}$, respectively.
Furthermore, suppose that the first hidden layer in the neural network accepts inputs of size $q$ and thus we need project an observation with these two features into a vector representation of the same size.
If we were to use one-hot encoded representations of $X_{1}$ and $X_{2}$ we would need a weight matrix of size $(C_{1}+C_{2})\times q$.
However, if we use entity embeddings of $X_{1}$ and $X_{2}$ we may have two weight matrices of sizes $C_{1}\times q/2$ and $C_{2}\times q/2$, which is in total half the number of parameters needed compared to the pure one-hot encoded representation.

The size of the embeddings is a hyperparamter of the model and again there is no way to tell what this value should be beforehand.
Most publications rely on a grid search to find the optimal sizes.
For example [@Song2018] experimented with embedding sizes [8,16,24,32] and found 16 to work the best, and [@Cheng2016] found that an embedding size of 32 was optimal for their use-case.
The selection complete depends on the data and the network used.

[@Wang2017b] and [@Brebisson2015] used different embedding sizes for each categorical features and suggested these rules-of-thumb:

- $6\times(\text{cardinality})^{\frac{1}{4}}$ [@Wang2017b]
- $\max(2, \min(\text{cardinality}/2,50))$

It makes sense to have different embedding sizes for categorical features with different complexities.

Entity embedding not only reduces memory usage and speeds up neural networks compared to one-hot encoding, but more importantly, by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables, which one cannot obtain with one-hot encoding.
This allows us to interpret the classes of the categorical features.
The embeddings can be visualised to gain further insight into the data and model decision making.
The weights associated with each category's projection onto the embedding space can be plotted with any dimension reduction technique like t-sne or PCA.
Then we can compare the categories based on their relative distances and positions in this reduced space.
In \autoref{fig:edu_embed} we plot a 2 component PCA of the embedding matrix of the Education categorical feature in the adult dataset.
We see that the school categories all lie in the bottom-right corner of the space, with some notion of ranking from grade 5 (top-right) to grade 12 (bottom-left).
The tertiary education classes are in a separate cluster and their levels of education correspond conveniently with the vertical axis.

![PCA of the Education entity embedding weight matrix.\label{fig:edu_embed}](figures/education_embeds.png)

To prove that these entity embeddings actually learns something useful, besides plotting the embedding matrix, one can also feed them along with the continuous features to other learning algorithms and see how it affects performance.
[@Guo2016] found that the embeddings obtained from the trained neural network boosted the performance of all tested machine learning methods considerably when used as the input features. 
These embeddings can be reused on different machine learning tasks and do not have to be re-learned for each dataset.
Instacart and Pinterest, referenced above, reported sucessful implementation of this approach.

The entity embedding approach is very flexible.
One can reuse an embedding over different categorical features if the features have overlapping categories.
[@Zhou2017] has an intersting take on multi-hot categorical features; where a feature can have more than category associated with it.
The embedding layer for that instance then outputs a list of embeddings with length the same as the number of categories associated with that instance and feature.
The list of embeddings then gets projected back into a fixed-length representation by doing a pooling operation.

### Combining Features

Once the continuous and categorical features have been processed and embedded, we need a way to combine them before passing it to the rest of the network.
The standard approach is to concatenate each categorical variable embedding to the continuous variables, as was done in [@haldar2018] and [@Wang2017b] for example, illustrated in \autoref{fig:comb_rep}.
The potential problem with this approach is that some features might be over-represented in this vector.
For example, one of the continuous features may be very important for prediction, but it gets lost when concatenated with all the entity embeddings each taking up more space in the combined representation.

In [@Song2018] they embed both the numerical and categorical features into the same sized embedding.
By mapping both types in the same feature space facilitates more effective learning of interactions between the mixed features.
The embedding for the $j$-th numerical features is obtained by:
$$
\boldsymbol{e}_{j}=\boldsymbol{v}_{j}x_{j}
$$
where $x_{j}$ is a scalar and $\boldsymbol{v}_{j}$ is the associated weight vector.

```{r, fig.cap="Combined representation of continuous and categorical features.\\label{fig:comb_rep}", fig.align='center', out.width="30%"}
knitr::include_graphics("figures/mix_rep.png")
```

## Learning Feature Interactions \label{sec:feat_int}

In most machine learning tasks the greatest performance gains can be achieved by feature engineering wheras better algorithms may only result in incremental boosts.
In feature egineering one strives to create new features from the original features based on some domain knowledge of the data or otherwise, that makes it easier for the model to estimate the target and help capture high-order interactions between features.
Although a crucial step to make the most out of the data, this can be a very laborious process.
It is widely held that 80% of the effort in an analytic model is preprocessing, merging, customizing, and cleaning datasets, not analysing them for insights [@Rajkomar2018].
There is no formal path to follow in this stage and thus usually consists of many a trial and error, benefitted by domain knowledge of the data, which is not always accessible.
A huge advantage of using neural networks on tabular data (and other data structures) is that the feature engineering process gets automated to some extent (with the caveat of numerical feature preprocessing.).
A neural network learns these optimal feature transformations and interactions implicitly during the training process.
The hidden layers of a neural network can be viewed as a feauture extractor that was optimised to map the inputs into the best possible feature space for a model (the final layer of the network) to operate in.

The standard approach is to stack a few fully-connected layers one after the other to map the input representation to the output, as was done in [@Covington2016].
Fully-connected layers model all feature interactions implicitly and in ideal world, we would expect this architecture to be sufficient.
In practice, with out current learning algorithms, a simple MLP is not good enough to learn all types of interactions.
A fully connected model structure leads to very complex optimisation hyperplanes with a high risk of falling into local optimums.

Therefore it is necessary to explicitly leverage expressive feature combinations or encourage the network to learn better high-order feature interactions.
This issue receives attention in publications such as [@Song2018], [@Wang2017b], [@Qu2016] and [@Guo2017].
The restrictions we impose on the fully-connected structure may further help to limit the model size to make learning more efficient.
They key question we asked in this section is how do we help the network to determine which features to combine to form meaningful high-order features.
We first briefly review some of the suggestions in the literature, after which we focus on the attention mechanism which we believe is the most powerful method of learning feature interactions.

The authors of [@Wang2017b] makes a case for finding a bounded-degree of feature interactions, arguing that all the Kaggle competions are won with feature engineering of low-degree interactions, whereas deep neural networks learn highly non-linear interactions implicitly.
They suggest using an automated way of building cross-features, called the cross-network.
In the cross-network each layer produces higher-order interactions based on existing ones, and keeps the interactions from previous layers. 
The cross-network consists of cross-layers that can be formalised as:
$$
\boldsymbol{x}_{l+1}=\boldsymbol{x}_{0}\boldsymbol{x}_{l}^{\intercal}\boldsymbol{w}_{l}+\boldsymbol{b}_{l}+\boldsymbol{x}_{l}
$$
where $\boldsymbol{x}_{l}$ is the output of the $l$-th cross layer and the input to the $(l+1)$-th cross-layer; $\boldsymbol{x}_{0}$ is the combined input representation; $\boldsymbol{w}_{l}$ and $\boldsymbol{b}_{l}$ are its associate weight and bias paramters respectively.
Each cross-layer adds back its input after feature in the same fashion as a skip-connection.
The degree of the cross-features grows with cross-network depth.
The authors experimented with 1-6 cross-layers and found that a depth of 6 gave the best results.

In parallel to the cross-network, they also used a standard deep neural network to learn the highly non-linear feature interactions.
The DNN accepts the same input and its output is then concatenated with that of the cross-network.
These two networks can then be trained simultaneously.

[@Qu2016] used something called a product layer, which takes pairwise inner or outer products of all feature combinations and concatenates it to all linear combinations.
The output is then fed to 2 fully-connected layers.

According to [@Guo2017] and [@Cheng2016] it is necessary to capture both low and high-order interactions.
They both achieve this by having two parallel networks, similar to [@Wang2017b], where one learns high-order interactions and the other low-order interactions.
Both use a deep neural network to learn the high-order interatcions.
[@Cheng2016] uses a shallow but wide neural network to capture the low order interactions and where [@Guo2017] uses a learnable factorisation machine to do that.
Similar to the other parallel stream networks, the output of the two streams gets concatenated before passing it to the classification layer.

### Attention \label{sec:tab_att}

Based on the results in the paper [@Song2018] and by our findings in general deep learning research \Sref{sec:attention}, we deem attention to be the most promsing mechanism to model feature interactions. [@Song2018] uses a multi-head self-attention mechanism which they call the interacting layer.
Within in the interacting layer each feature is allowed to interact with every other feature and automatically determine which of those interactions are relevant to the output.

To explain the attention mechanism, consider feature $j$ and suppose we want to determine which high-order features involving feature $j$ are meaningful.
We first define the correlation between features $j$ and $k$ under attenion head $h$ as:
$$
\alpha_{j,k}^{(h)}=\frac{\exp{\left(\phi^{(h)}(\boldsymbol{e}_{j},\boldsymbol{e}_{k})\right)}}{\sum_{l=1}^{L}
\exp{\left(\phi^{(h)}(\boldsymbol{e}_{j},\boldsymbol{e}_{l})\right)}}
$$
where $\boldsymbol{e}_{l}$,$l=1,\dots,L$ is the embedding of the $l$-th features.
$\phi^{(h)}(.,.)$ is an attention function which defines the similarity between two features.
It can be defined by a trainable layer or a simple inner product as in:
$$
\phi^{(h)}(\boldsymbol{e}_{j},\boldsymbol{e}_{k})=\left<W_{\text{query}}^{(h)}\boldsymbol{e}_{j},W_{\text{key}}^{(h)}\boldsymbol{e}_{k} \right>
$$
where $W_{\text{query}}^{(h)}$ and $W_{\text{key}}^{(h)}$ are transformation matrices which map the original embedding space into new spaces of the same dimension.
The representation of feature $j$ in subspace $h$ is then updated by combining all relevant features guided by the coefficients $\alpha_{j,k}^{(h)}$:
$$
\tilde{\boldsymbol{e}}_{j}^{(h)}=\sum_{k=1}^{K}\alpha_{j,k}^{(h)}W_{\text{value}}^{(h)}\boldsymbol{e}_{k}
$$
$\tilde{\boldsymbol{e}}_{j}^{(h)}$ is a combination of feature $j$ and its relevant features under attention head $h$.
Note that $\alpha_{j,k}^{(h)}$ for $k=1,\dots,K$ sum to 1 since they went to a logit transform/softmax operation.

$\tilde{\boldsymbol{e}}_{j}^{(h)}$ is thus a learned combinatorial feature.
Since a feature can be involved in various different combinations, we use multiple heads to extract the multiple combinations, *i.e.* $\{\tilde{\boldsymbol{e}}_{j}^{(h)}\}_{h=1}^{H}$.
In the original paper [@Song2018] used $H=2$, but this is typically a hyperparemeter one needs to tune.
All of these combinatorial features are concatenated into a single vector, $\tilde{\boldsymbol{e}}_{j}$.
Then finally the output is combined with its raw input and sent through a ReLU activation:
$$
\boldsymbol{e}_{j}^{\text{res}}=\text{ReLU}\left(\tilde{\boldsymbol{e}}_{j}+W_{\text{res}}\boldsymbol{e}_{j}\right)
$$
This mapping from $\boldsymbol{e}_{j}$ to $\boldsymbol{e}_{j}^{\text{res}}$ is done for each feature to form the interacting layer.
The activations of the interacting layer is thus a representation of the high-order features of its inputs.
These interacting layers can be stacked on-top of each other to form arbitrary order combinatorial features.

[@Zhou2017] follows a vaguely similar idea to attention by learning the weights to apply to the hidden representations.
Interestingly, they removed the softmax layer as a way to mimick probabilities to reserve the intensity of activations.

In terms of the skip-connection already mentioned a couple of times above - both [@Song2018] and [@Wang2017b] found by experimentation that by having skip-connections to connect lower-level features with higher-level ones improves the performance of the network.

### Self-Normalising Neural Networks

By studying the literature of deep neural networks for tabular data we rarely see the optimal network depth go beyond three of four layers.
The reason is that a fully connected model have very complex optimisation hyperplanes which increases the risk of falling into bad local optimums.
A proposed way of training deeper neural networks is to make use of *Self-Normalising Neural Networks* [@Klambauer2017].
They were developed as an alternative to batchnorm layers since batchnorm layers often become unstable when using SGD or stochastic regularisation techniques like dropout,
especially when using fully-connected neural networks.
This is exhibited by a high-variance in the training error.

The self-normalising neural network is simply a neural network with a novel activation function called the *SeLU*.
The SeLU helps the network to maintain zero mean and unit variance for the activations at all network levels.
The SeLU activation eliminates the need for a BatchNorm layer and is also much safer against exploding or vanishing gradients.
It is defined as:
$$
\text{selu}(x)=\lambda 
\begin{cases}
x & \text{if}~~~x>0\\
\alpha e^{x}-\alpha & \text{if}~~~x\le 0\\
\end{cases}
$$
where $\lambda=1.0507$ and $\alpha=1.6733$ are special constants derived in the paper.
When using the SeLU activation function, one should be aware of the special weight initialisation and dropout technique that should be used in tandem.

[@Klambauer2017] tested SeLUs on 121 classification datasets from the UCI Machine Learning repository.
They compared DNNs with SeLU activations to other DNNs and other classifiers like Random Forests and SVMs.
They found that on the datasets with less than 1000 obervations, random forests and SVMs performed the best.
However, for the datasets with more than 1000 observations, DNNs with SeLU activations performed the best overall.
The classifiers were compared by ranking them by their accuracy for each prediction task and doing a pairwise Wilcoxon test.
Another thing the authors found when comparing SeLUs with other activations is that the model selection approach for DNNs with SeLUs resulted in much deeper networks than DNNs with other activation functions.

These are all promising results but we have not seen SeLUs been used in other applications or papers on tabular datasets.

## Sample Effeciency \label{sec:samp_eff}

It is well know that deep neural networks require a large amout of data to generalise well.
Typically, tabular datasets are not as large as unstructured datasets like images and texts.
There is also no large tabular dataset from which knowledge can be transferred, like an ImageNet for computer vision or a Wikipedia corpora for NLP equivalent.
We suggest two techniques for overcoming this problem: data augmentaion and unsupervised pre-training.

### Data Augmentation

Data augmentation for tabular datasets is rarely studied.
Tabular data is very different to image data and the standard augmentations used in computer vision does not make sense with tabular data.
You cannot rotate or scale an observation from a tabular data without losing its meaning.
One transformation that does make sense for tabular input is the injection of random noise.

When working with images, we can randomly perturb the pixel intensities by a small amount so that it is still possible to make sense of its content.
By adding 1 for example to all pixels and all colors in an image, will only make it slightly brighter and we will still be able to make sense of it.
But with tabular data we can just randomly add a small amount to any feature.
The input features will probably not all be on the same scale and the addition of noise might result in a feature value that is not within the true distribution. 
In addition, it does not make sense to add anything to a discrete variables.
Thus in order to inject random noise to a tabular data sample, the noise should be scaled relative to each input feature range and the results should be a valid value for that feature.
This also helps the model to be more robust to small variations in the data.
[@VanDerMaaten2013] suggests an augmentation approach that does this called *Marginalised Corrupted Features* (MCF).
The MCF approach adds noise to input from some known distribution.
The process is manual and it cannot be applied to discrete variables.

In the original DAE paper [@Vincent2008], they used a blank-out corruption procedure.
Which is randomly selecting a subset of the input features and masking their values with a zero.
This similar to dropout regularisation but instead applied to the inputs.
The only conceptual problem with this approach is that for some features a zero value actually carries some meaning, so a suggestion is to blank-out features with a unique value not already belonging to that feature distribution.

Another input corruption approach shown to work empirically [here](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629) is what is a technique called *Swap Noise* [@kasar2018].
The swap noise procedure corrupts inputs by randomly swapping input values with those of other samples in the datasets.
We have illustrated the approach in \autoref{tab:swpns}.
In this way you ensure that the corrupted input have at least valid feature values.
But it still might produce combinations of features that are not actually possible according to the true data distribution.

```{r}
df = read_csv('../data/adult/adult_sample.csv')
new_row <- df[4,]
new_row[1] <- df[1,1]
new_row[5] <- df[5,5]
df <- rbind(df, new_row)

df[1,1] <- cell_spec(df[1,1], color='blue')
df[5,5] <- cell_spec(df[5,5], color='green')
df[7,1] <- cell_spec(df[7,1], color='blue')
df[7,5] <- cell_spec(df[7,5], color='green')
df[7,2] <- cell_spec(df[7,2], color='red')
df[7,3] <- cell_spec(df[7,3], color='red')
df[7,4] <- cell_spec(df[7,4], color='red')
df[7,6] <- cell_spec(df[7,6], color='red')

kable(df, 'latex', caption = 'Swap Noise Example.\\label{tab:swpns}', booktabs=TRUE, escape = FALSE) %>% 
  kable_styling(position = 'center') %>% 
  row_spec(4, color='red') %>% 
  group_rows('Original Dataset', 1,6) %>% 
  group_rows('Sample with swap noise', 7,7)
```


The final data augmentation we look at is *mixup augmentation* [@Zhang2017].
The way mixup creates artifical samples is by the following formulation:
$$
\tilde{\boldsymbol{x}}=\lambda\boldsymbol{x}_{i}+(1-\lambda)\boldsymbol{x}_{j}\\
\tilde{\boldsymbol{y}}=\lambda\boldsymbol{y}_{i}+(1-\lambda)\boldsymbol{y}_{j}\\
$$
where $\boldsymbol{x}$ is a input vector,$\boldsymbol{y}$ a one-hot encoded output vector and $\lambda\in[0,1]$.
$(\boldsymbol{x}_{i}, \boldsymbol{y}_{i})$ and $(\boldsymbol{x}_{j}, \boldsymbol{y}_{j})$ are two samples drawn at random from the training data.
Thus mixup assumes that linear interpolations of input vectors lead to linear interpolations of corresponding targets.
We visualise the creation of artificial samples through mixup on a toy dataset in  \autoref{fig:simple_dataset_mixup}.

![Illustration of points created by mixup augmentation.\label{fig:simple_dataset_mixup}](figures/simple_dataset_mixup.pdf)

$\lambda$ controls the strength of the interpolation between the input-output pairs.
The closer $\lambda$ is to 0 or 1, the closer the artificial sample will be to an actual training sample.
The authors suggest using $\lambda\sim\text{Beta}(\alpha, \alpha)$ for $\alpha\in(0,\infty)$, since this makes it more likely that the mixed-up sample lie closer to one of the original samples than in the middle of the two.
They observed best performance when $\alpha\in[0.1,0.4]$ and if $\alpha$ is too high, they experience underfitting.

Other ablation studies they did was to find at which stages of the network the interpolation should happen, *e.g.* raw input, after embedding, hidden layer, *etc.*
But the experiments are not extremely clear and therefore warrants further discussion here.

Typically data augmentation procedures are dataset dependent and therefore requires expert knowledge.
It is hard to think of ways to augment tabular data, even more so a generic way of doing so.
However, from this defnition it is clear that mixup can be used on any type of data, including tabular datasets.

Mixup data augmentation can be understood as a mechanism to encourage the model to behave linearly in-between training samples.
[@Zhang2017] shows that this linear behaviour reduces the amount of undesirable variation when predicting new samples further away from the training samples.
They also argue and show empirically how training with mixup is more stable in terms of model predicions and gradient norms.
This is because mixup leads to decision boundaries that transition linearly between classes, resulting in smoother predictions.

The authors tested mixup data augmentation on tabular datasets.
They tested it on six classification datasets from the UCI Machine Learning repository.
They used a 2-layer MLP with 128 neurons each and a batch size of 16.
They found that mixup improved the performance on four out of the six datasets.

All of these methods have hyperparameters that needs to be set.
For example, with swap noise and blank-out we must set the proportion of feature values to be corrupted and with mixup we need to set the mixup weight.
These parameters control the strength of the regularisation and thus should be tuned accordingly.

### Unsupervised Pretraining

We described unsupervised pretraining in \label{sec:tl}.
It is a very effective way of training deep neural networks.
It also addressed the feature engineering issue we face with tabular data by automatically learning useful feature combinations in an unsupervised fashion.
However, we only found two published papers using this approach for tabular data, [@Zhang2016] and [@Miotto2016].
[@Miotto2016] used a stacked denoising autoencoder as a feature learning method to derive a general-purpose patient representation for EHR data that facilitates clinical predictive modelling.
The trained DAE is the used as a fixed feature extractor to obtain representations of the data that can be passed to supervised learning algorithms.
[@Zhang2016] also did pretraining with DAEs.

Feature learniattempts to overcome limitations of supervised feature space definition by automatically identifying patterns and dependencies in the data to learn a compact and general representation that make it easier to automatically extract useful information when building predictive models [@Miotto2016].
These techniques are very familiar and effective in text, audio and image processing, but not with tabular data.

The DAEs enforce the representations to be robust to partially destroyed inputs.
By using these learned representations as input, significantly improved the performance of predictive models compared to those only using the raw inputs.

There are a lot of unknowns when it comes to using DAEs for pretraining.
First we need to decide on the structural hyperparameters of the network (number of layers, sizes, etc.) and then we need to decide what learning paramters to choose.
This is not as thoroughly explored as hyperparamter selection for supervised neural networks.
We also need to decide which loss function we need to used for the DAE.
This choice depends on the type of the input.
If all the inputs are numeric we can use the common MSE loss, but if the input has categorical features, this decision needs more consideration.
[@Miotto2016] transformed all numeric features to be in the range of [0,1] and one-hot encoded the categorical features so that they can use the binary cross-entropy as the loss function.
But we cannot follow this approach if we wish to use entity embeddings for the categorical features.
A suggestion is to have a separate multiclass cross-entropy loss for each categorical feature and use the MSE loss for the continuous features.
These lossed should then be combined to obtain a total loss.
How to weight the different losses is also something that is not yet researched.

We suggest treating the unsupervised pretraining rather as a transfer learning problem than a feature extraction method like in [@Zhang2016] and [@Miotto2016].
That is, slicing the head of the trained DAE and replacing it with the required classification layers.
Then first train only the new layers keeping the lower level DAE layers fixed and then train all of the layers simultaneously.
By doing it this way, the "feature extractor" part of the DAE can still be tweaked to be optimal for the supervised learning task.
We experiment with this approach in the next chapter.

### Regularisation

Since regularisation can also help one to be more sample efficient we have a brief discussion about it here.
A popular form of regularisation in the papers we have studied is dropout [@Song2018], [@Zhang2016], [@Qu2016], [@Guo2017].
The dropout percentage was mostly determined using a grid search and the results showed the optimal percentage depends on the data and the model being used.

Interesetingly, [@haldar2018] found that dropout was not effective in their application.
They pinned it down to dropout producing invalid input scenarious that distracted the model.
Therefore they opted for hand crafted noise shapes taking into account the distribution of the relevant feature. 

The other form of regularisation used for tabular data is weight decay [@Song2018], [@Wang2017b], [@Zhang2016], [@Qu2016] and [@Zhou2017],
[@Zhang2016] compared dropout with L2 and found dropout to be better.

For a different approach to regularisation, recally the difference between tabular data and unstructured data.
A difference between the two data types that stand out is the relative importance of each of the important features with respect to the target. 
In computer vision a large amount of pixels should change before an image is of something else.
Whereas in tabular data a very small change in a single feature may have totally different behaviour with respect to the target [@Shavitt2018].
The same authors mention that this can be addressed by including a separate reqularisation term for each of the weights in the network. 
These regularisation terms are seen as additional model hyperparameters.
It is easy to see that this approach is totally intractable since the only way to train these hyperpararmeters are brute force and repetitive tweaking and validatig which cannot be done in by a gradient based method.
A workaround is to make these regularisation parameters trainable like all of the other points in the network.
This is achieved by minimising the counterfactual loss, a novel loss function proposed by [@Shavitt2018].
They found that training neural networks by optimising the counterfactual loss, outperform other regularisation approaches for neural networks and results in neural networkss that are comparable in performance to gradient boosted trees.

## Interpretation \label{sec:interp}

We have mentioned that model explainability is important for various reasons.
It helps one to know how to improve one's model or where it goes wrong.
One can even go so far as to say if you cannot explain how a prediction is made, you cannot know how good it is.
In the literature we focussed on for this work, not much attention was given to model interpretability.
Although there are a few extra interpretation options avaible when using some of the abovementioned approaches to tabular data.

Firstly, if one uses entity embeddings, the embeddings can be visualised once projected into a low dimensional space, as done in \autoref{fig:edu_embed}.
[@Zhou2017] is an example in the literature that did this.
The attention layer can also provide insight into which feature combinations are important.
[@Song2018] and [@Zhou2017] made use of this information.
The information is obtained from the attentional weights $\alpha$ which is based on feature similarity and can indicate which features are comined to make predicions for certain observations.
Example output in [@Song2018] from this approach can be found in \autoref{fig:plt_att}.
Also see [@Shavitt2018] for how the learned regularisation parameters were used to evaluate feature importance.

![The attention weights visualised for a singe observation in the dataset.\label{fig:plt_att}](figures/att_plot.png)

[@haldar2018] used one of the model agnostic model interpretation algorithms.
They used the permuation importance algorithm to evaluate the importance of features.
However, they came to the same conclusion as what we stated in \Sref{sec:interp_ma}.
That is that the permutation test did not produce sensical results because the features were not independent.
Permuting the feature independently created examples that never occurred in real life, and the importance of features in that invalid space misled them. 
They did find it somewhat useful to identify redundant features.

+ Example of how knowledge distillation can work and compare output with permutation importance.

## Hyperparameter Selection \label{sec:hs_tb}

We have already identified a plethora of hyperparameters to be tuned if we want to sucessfully fit a deep neural network to tabular data.
Broadly speaking, there are structural hyperparamters to select, like the number of layers and the type of activation function, but then there are also learning algorithm hyperparameters, like the learning rate or the weight decay.
In term of the structural parameters, since there are no shared patterns among the diverse tabular datasets, it is hard to design a universal architecture that will fit all.
In most of the literature we have investigated the researcher performs a grid search over many combinations of of hyperparamters [@Song2018], [@Wang2017b], [@Zhang2016], [@Qu2016], [@Guo2017], [@Covington2016].
Most of these parameters are very dependent on the dataset and other modeling choices and therefore the need to tune them.
The main structural hyperparameters tuned in the majority of the papers are: hidden layer size, number of hidden layers, activation functions and the shape of the fully-connected layers (*i.e.* is it a constant shape, increasing, decreasing or a diamond shape).
The values found varied accross publications, showing again that the step is necessary when working with custom datasets and models.
We include some similar experiments in \autoref{app:B}

In terms of the learning algorithm hyperparamters, not much experimentation was shown in the publication itself.
The only choice that were shown was selecting the learning rate over a grid of values, as in [@Zhang2016] and [@Wang2017b], and kept the selection constant during training.
Most of the work used the Adam optimiser and early stopping to prevent overfitting [@Song2018], [@Wang2017b], [@Zhang2016].
Large batch sizes were chosen in [@Song2018], 1024 and [@Wang2017b], 512.

Clearly, the 1Cycle policy and the concept of superconvergence has not yet been tested in the tabular data setting.
Therefore we will test its effectiveness here using the Adult dataset.
We compare it against a constant learning rate.
The evaluation method is described in \autoref{chp:exp}.
We first do a learning rate range test and find that the optimal learning rate bounds for the one-cycle policy is 0.01 and 0.1.
In our experiment we compare training a neural network with a fixed learning rate at the lower bound, 0.01, a neural network with a fixed learning rate at the upper bound, 0.1 and a neural network with a 1Cycle learning rate schedule in these bounds.
The results are displayed in \autoref{fig:cyc_test}.
From the green line, we see that when training with too large of a (constant) learning rate, the validation loss struggles to converge.
Training with the smaller constant learning rate (blue line) works better in this case, but the losses plateau quite early on and shows no sign of improving.
The 1Cycle learning rate update policy (orange line) shows the best validation loss and accuracy of the three methods.
This proves that the 1Cycle policy is also effective when working with tabular data.

![Constant learning rate vs the 1Cycle schedule.\label{fig:cyc_test}](../data/adult/results/sgd_1cyc/all_epochs2.png)

Since the weight decay plays and important part in the super-convergence phenomena, we do an experiment to find the optimal value and to see how it influences the performance.
By the suggestion of [@Smith2018] we try weight decay values for $10^{-3}$, $10^{-5}$ and 0.
First we do a learning rate range test with the different weight decays to identify the best value.
The results are displayed in \autoref{fig:wd_range}.
From the range test it seems that a weight decay of $10^{-5}$ gives the best performance.
To check that this result holds during a full training run, we do an experiment to compare the full training runs with the different weight decays.
The results are displayed in \autoref{fig:wd_test}.
From the results it shows that the learning rate range test is a good indicator of the optimal weight decay, since in the full training run, the model with weight decay of $10^{-5}$ performed the best.
Note, that the differences between the training runs are small, and thus if we were to choose one of the other weight decays for this task, we would have only performed slightly worse.

![A learning rate range test with different weight decays.\label{fig:wd_range}](figures/wd_lr_range.pdf)

![A full training run with different weight decays.\label{fig:wd_test}](../data/adult/results/weight_decay/all_epochs.png)


