# Deep Learning for Tabular Data \label{chp:td}

## Introduction

Up to now we have covered the basics of neural networks, as well as the more recent advancements in the era of deep learning.
The aim of this chapter is to explore how the approaches and techniques we discussed can be leveraged when doing deep learning with tabular data.
Tabular data is vastly different to so-called unstructured data like images, text and speech and therefore we do not expect all of the methods to be as effective.
Deep learning on tabular data is not as well researched compared to the aformentioned data domains and thus it is not always clear how to solve certain modelling challenges.
There is only a handful of publications reporting sucessful implementations of deep learning for tabular data on important applications such as recommender systems [@haldar2018], click-through rate prediction[^ctr] [@Song2018], analysis of electronic health records [@Rajkomar2018] and transport related problems [@Brebisson2015], to name a few.

The tabular data domain is still dominated by tree-based models like random forests and gradient boosted trees.
This makes us curious as to why deep learning is not as effective here as it is in most other data domains.
This chapter will help shed light on this issue and point to promising ways of improving the current state-of-the-art.

This chapter is structured based on the tabular dataset challenges introduced in \Sref{sec:tabchal}.
For each of the challenges we provide a recap of the issue, review the literature to discuss how they are currently being treated, and provide suggestions for improving these approaches, (where applicable).
In \Sref{sec:inp_rep} we look at ways of representing the input features of a tabular dataset.
\Sref{sec:feat_int} is abut approaches for leveraging feature interactions.
A large part of this chapter is on how to be more sample efficient; this we discuss in detail in \Sref{sec:samp_eff}.
Then we briefly investigate ways of interpreting deep neural networks for tabular data in \Sref{sec:interp}.
In the final section we discuss the 1Cycle policy and hyperparameter selection for tabular data neural networks, in addition to other miscellaneous topics that do not fit into the above categorisation.

[^ctr]: To predict the probability of a user clicking an item, critical to online applications.

## Input Representation \label{sec:inp_rep}

One of the major design considerations when building a deep neural network for tabular data is the input representation, *i.e.* how should one represent each feature numerically?
This choice may heavily influnce the model's ability to extract patterns from the input and the effectiveness of the optimisation algorithm.
What makes this decision harder is that the features in a tabular dataset are highly heterogeneous [@Shavitt2018].
What we decide for one features might not be optimal for another and we want to ensure that no feature dominates another in the training process.

A tabular dataset typically has continuous features and categorical features.
Different approaches are needed to process each.
But the processing should be done in such a way that the one feature type does not dominate the other in terms of feature extraction and optimisation.

A tabular dataset can be high-dimensional and very sparse which makes the task more difficult but even more important to do effectively, as noted by many [@Song2018, @Wang2017b, @Qu2016, @Cheng2016, @anonymous2019, @Covington2016].
An example of one such challenging tabular dataset is the Criteo dataset[^criteo].
Its feature dimension is ~30 million, exhibiting sparsity of ~99%.
Although not always as extreme, these properties are common in tabular datasets.

[^criteo]: https://www.kaggle.com/c/criteo-display-ad-challenge

### Numerical Features

One of the things that make tree-based methods so attractive is that the scale and the distribution of the features hardly matter, as long as their relative ordering is meaningful.
With neural networks, we are not that fortunate.
Neural networks are very sensitive to the scale and distribution of its inputs [@Ioffe2015]. 
If the features are on different scales, one might dominate the weight updates, and if a feature constains a large value, it may throw of the optimisation procedure and cause exploding or vanishing gradients [@Clevert2015].
This means that one should do a proper standardisation of all continuous features in a tabular dataset.

The typical standardisation approach for numerical features in deep learning is to do mean centering and variance scaling, *i.e.* $\tilde{x}=(x-\mu)/\sigma$, where $\mu$and $\sigma$ is the mean and standard deviation of $X$ respectively ($x\in X$).
One would expect this transformation to be sufficient, but in practice, many have found otherwise.

In [@haldar2018] they suggest to first inspect the distribution of each of the features.
If a feature looks gaussian, do the standard normalisation, $(x-\mu)/\sigma$, but if the feature looks more like a power law distribution, transform it by $\log\left((1+x)/(1+\text{median})\right)$.
This ensures that the bulk of the values lie between {-1,1} and that the median is close to zero.
As an example, we illustrate what effect these transformations have on 2 continuous features in the Adult dataset, Age and Hours-per-week, in \autoref{fig:contnorm}.
In \autoref{fig:contnorm} (a) we see that the features are on a roughly similar scale but their distributions are totally different.
By applying mean centering and unit variance scaling (b) we see the values of the Hours-per-week feature are mostly in {-1,1} but for the Age features there are still plenty of values outside that range.
When doing the power distribution transformation of [@haldar2018], we observe that the Age feature is now in the proposed value range.
The downside to this approach is that it is not automatic and can become cumbersome with many variables.

```{r, out.width="33%", fig.cap='Effect of normalisation on continuous variables.\\label{fig:contnorm}', fig.subcap=c('Original', 'Gaussian Norm', 'Power Norm'), fig.show='hold'}
knitr::include_graphics(c('figures/cont_vars.pdf', 'figures/gaus_norm.pdf', 'figures/power_norm.pdf'))
```

To reduce the possibly high variance of numeric features [@Song2018] suggests to transform the a numeric feature to $\log^{2}(x)$ if $x>2$.
This worked in their use-case but it is hard to imagine that this work on all other cases.
This transformation causes a discontinuity at $x=2$ and a possible overlap between values the were originally less than 2 with those that were greater than 2.
In addition, this transformation does not address the extreme values on the negative side.
[@Wang2017b] simply used a standard log transform, $\log{x}$, to normalise continuous features.

In [@Covington2016] they also found that the proper normalisation of numeric features were critical for the model to converge.
Their approach was to transform the numeric features to be equally distributed in [0,1) using the cumulative distribution $\tilde{x}=\int_{\infty}^{x}df$, where $f$ is the distribution of $x$.
They approximated the integral with linear interpolation on the quantiles of the feature values computed as a preprocessing step.

Another possibility for numeric feature normalisation that we have not seen used in the deep learning for tabular data literature, is to use a batch normalisation layer as the initial layer to numeric features.
This will apply the same type of scaling as the zero mean and unit variance transformation, but with transformation parameters learned from each batch.
Thus the need to preprocess numeric features is removed.
The caveat is that the quality of the transformation depends on the batch statistics and thus if the batch is not representative of the full data distribution (which is likely if the batch size is small), then it might derail the training procedure.

The fact is that the only way to know for sure which normalisation to apply to numeric features is by experimations, since it varies between datasets.
The standard normal scaling is a safe transformation to use if experimentation wants to be avoided.

### Categorical Features

Most of the sparsity in tabular datasets come from categorical features.
Since neural networks cannot process discrete categories or objects, we need to find a numeric representation for each class.
The standard approach is to one-hot encode categorical features.
That is, if we have a categorical feature with three possible categories, for example, the one-hot encoded form of the three categories will be: Category 1: $[1,0,0]$, Category 2: $[0,1,0]$ and $[0,0,1]$ for Category 3.

There are multiple inefficiencies when using one-hot encodings with neural networks.
The obvious one we have already mentioned is that it introduces sparsity to the data, since the dimension of the one-hot encoded form is equal to the number or categories in a feature.
Thus if we have plenty of high-cardinality features in our dataset, the data will be extremely sparse and difficult to model.
This also increases the size needed of the first linear layer and thus we need machines with greater memory and processing power.
Neural networks can easily overfit such sparse data [@Covington2016].

The other problem with one-hot encodings of categorical features is that there is no notion of similarity and distances between categories.
In this representation, all categories are equally far apart now matter how semantically similar or dissimilar they are.
This makes it harder for the model to learn useful patterns.

An alternative to one-hot encodings as representations of categorical features for neural networks are *entity embeddings*.
An entity embedding is the exact same operation as (word) embeddings we have discussed in \Sref{sec:embeds}, but applied to categories instead of words.
Therefore an entity embedding assigns a numeric vector representation to each category in a categorical feature, for example: Category 1: $[0.05,-0.1,0.2]$, Category 2: $[0.2,0.01,0.3]$ and $[-0.1,-0.2,0.05]$ for Category 3.
Once all of the categorical features have been embedded, their representations can be concatenated and passed to the rest of the network.

The first published work in modern times on entity embeddings was in the taxi destination prediction challenge [@Brebisson2015].
In another Kaggle sucess story, [@Guo2016] succesfully used entity embeddings for predicting the total sales of a store.
Companies like [Instacart](https://tech.instacart.com/deep-learning-with-emojis-not-math-660ba1ad6cdc) and [Pinterest](https://medium.com/the-graph/applying-deep-learning-to-related-pins-a6fee3c92f5e) have reported the effective use of entity embeddings on their internal datasets. 
Currently, all research on deep learning for tabular data makes use of entity embeddings - see for example [@Song2018], [@Wang2017b], [@Covington2016] and [@Zhou2017].

The reason why it is used all-round is because it does not have the same issues as the one-hot encoded representations.
A similar formulation to \Sref{sec:embeds}, we define the embedding for the $j$-th categorical feature by:
$$
\boldsymbol{e}_{j}=V_{j}\boldsymbol{x}_{j}
$$
where $\boldsymbol{x}_{j}$ is the one-hot encoded vector representation of the $j$-th categorical variable and $V_{j}$ is the associated embedding/weight matrix.
The weights in $V_{j}$ are learned along with all of the other parameters in the network.

The first advantage of entity embeddings is that it speeds up training and reduces the memory footprint which further improves the generalisation ability of the network [@Covington2016], [@Guo2016].
This especially useful when working with dimensional and sparse inputs.
Suppose we have a dataset with two categorical features, $X_{1}$ and $X_{2}$, with cardinality of $C_{1}$ and $C_{2}$, respectively.
Furthermore, suppose that the first hidden layer in the neural network accepts inputs of size $q$ and thus we need project an observation with these two features into a vector representation of the same size.
If we were to use one-hot encoded representations of $X_{1}$ and $X_{2}$ we would need a weight matrix of size $(C_{1}+C_{2})\times q$.
However, if we use entity embeddings of $X_{1}$ and $X_{2}$ we may have two weight matrices of sizes $C_{1}\times q/2$ and $C_{2}\times q/2$, which is in total half the number of parameters needed compared to the pure one-hot encoded representation.

The size of the embeddings is a hyperparamter of the model and again there is no way to tell what this value should be beforehand.
Most publications rely on a grid search to find the optimal sizes.
For example [@Song2018] experimented with embedding sizes [8,16,24,32] and found 16 to work the best, and [@Cheng2016] found that an embedding size of 32 was optimal for their use-case.
The selection complete depends on the data and the network used.

[@Wang2017b] and [@Brebisson2015] used different embedding sizes for each categorical features and suggested these rules-of-thumb:

- $6\times(\text{cardinality})^{\frac{1}{4}}$ [@Wang2017b]
- $\max(2, \min(\text{cardinality}/2,50))$

It makes sense to have different embedding sizes for categorical features with different complexities.

Entity embedding not only reduces memory usage and speeds up neural networks compared to one-hot encoding, but more importantly, by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables, which one cannot obtain with one-hot encoding.
This allows us to interpret the classes of the categorical features.
The embeddings can be visualised to gain further insight into the data and model decision making.
The weights associated with each category's projection onto the embedding space can be plotted with any dimension reduction technique like t-sne or PCA.
Then we can compare the categories based on their relative distances and positions in this reduced space.
In \autoref{fig:edu_embed} we plot a 2 component PCA of the embedding matrix of the Education categorical feature in the adult dataset.
We see that the school categories all lie in the bottom-right corner of the space, with some notion of ranking from grade 5 (top-right) to grade 12 (bottom-left).
The tertiary education classes are in a separate cluster and their levels of education correspond conveniently with the vertical axis.

![PCA of the Education entity embedding weight matrix.\label{fig:edu_embed}](figures/education_embeds.png)

To prove that these entity embeddings actually learns something useful, besides plotting the embedding matrix, one can also feed them along with the continuous features to other learning algorithms and see how it affects performance.
[@Guo2016] found that the embeddings obtained from the trained neural network boosted the performance of all tested machine learning methods considerably when used as the input features. 
These embeddings can be reused on different machine learning tasks and do not have to be re-learned for each dataset.
Instacart and Pinterest, referenced above, reported sucessful implementation of this approach.

The entity embedding approach is very flexible.
One can reuse an embedding over different categorical features if the features have overlapping categories.
[@Zhou2017] has an intersting take on multi-hot categorical features; where a feature can have more than category associated with it.
The embedding layer for that instance then outputs a list of embeddings with length the same as the number of categories associated with that instance and feature.
The list of embeddings then gets projected back into a fixed-length representation by doing a pooling operation.

### Combining Features

Once the continuous and categorical features have been processed and embedded, we need a way to combine them before passing it to the rest of the network.
The standard approach is to concatenate each categorical variable embedding to the continuous variables, as was done in [@haldar2018] and [@Wang2017b] for example, illustrated in \autoref{fig:comb_rep}.
The potential problem with this approach is that some features might be over-represented in this vector.
For example, one of the continuous features may be very important for prediction, but it gets lost when concatenated with all the entity embeddings each taking up more space in the combined representation.

In [@Song2018] they embed both the numerical and categorical features into the same sized embedding.
By mapping both types in the same feature space facilitates more effective learning of interactions between the mixed features.
The embedding for the $j$-th numerical features is obtained by:
$$
\boldsymbol{e}_{j}=\boldsymbol{v}_{j}x_{j}
$$
where $x_{j}$ is a scalar and $\boldsymbol{v}_{j}$ is the associated weight vector.

```{r, fig.cap="Combined representation of continuous and categorical features.\\label{fig:comb_rep}", fig.align='center', out.width="30%"}
knitr::include_graphics("figures/mix_rep.png")
```

## Learning Feature Interactions \label{sec:feat_int}

In most machine learning tasks the greatest performance gains can be achieved by feature engineering wheras better algorithms may only result in incremental boosts.
In feature egineering one strives to create new features from the original features based on some domain knowledge of the data or otherwise, that makes it easier for the model to estimate the target and help capture high-order interactions between features.
Although a crucial step to make the most out of the data, this can be a very laborious process.
It is widely held that 80% of the effort in an analytic model is preprocessing, merging, customizing, and cleaning datasets, not analysing them for insights [@Rajkomar2018].
There is no formal path to follow in this stage and thus usually consists of many a trial and error, benefitted by domain knowledge of the data, which is not always accessible.
A huge advantage of using neural networks on tabular data (and other data structures) is that the feature engineering process gets automated to some extent (with the caveat of numerical feature preprocessing.).
A neural network learns these optimal feature transformations and interactions implicitly during the training process.
The hidden layers of a neural network can be viewed as a feauture extractor that was optimised to map the inputs into the best possible feature space for a model (the final layer of the network) to operate in.

The standard approach is to stack a few fully-connected layers one after the other to map the input representation to the output, as was done in [@Covington2016].
Fully-connected layers model all feature interactions implicitly and in ideal world, we would expect this architecture to be sufficient.
In practice, with out current learning algorithms, a simple MLP is not good enough to learn all types of interactions.
A fully connected model structure leads to very complex optimisation hyperplanes with a high risk of falling into local optimums.

Therefore it is necessary to explicitly leverage expressive feature combinations or encourage the network to learn better high-order feature interactions.
This issue receives attention in publications such as [@Song2018], [@Wang2017b], [@Qu2016] and [@Guo2017].
The restrictions we impose on the fully-connected structure may further help to limit the model size to make learning more efficient.
They key question we asked in this section is how do we help the network to determine which feautres to combine and how to form meaningful high-order features.

### Attention \label{sec:tab_att}

These layers are inefficient in learning multiplicative feature interactions.
[@Song2018] uses a multi-head self-attention mechanism which they call the interacting layer.
[@Zhou2017] follows a vaguely similar idea to attention by learning the weights to apply tothe hidden representations.
Interestingly, they removed the softmax layer as a way to mimick probabilities to reserve the intensity of activations.

Within in the interacting layer each feature is allowed to interact with every other feature and automatically determine which of those interactions are relevant to the output.
To explain the attention mechanism, consider feature $j$ and the step to determine which high-order features involving feature $j$ are meaningful.
We first define the correlation between features $j$ and $k$ under attenion head $h$ as:
$$
\alpha_{j,k}^{(h)}=\frac{\exp{\left(\phi^{(h)}(\boldsymbol{e}_{j},\boldsymbol{e}_{k})\right)}}{\sum_{l=1}^{L}
\exp{\left(\phi^{(h)}(\boldsymbol{e}_{j},\boldsymbol{e}_{l})\right)}}
$$
where $\phi^{(h)}(.,.)$ is an attention function which defines the similarity between two features.
It can be defined by a neural network or a simple inner product like in [@Song2018]:
$$
\phi^{(h)}(\boldsymbol{e}_{j},\boldsymbol{e}_{k})=\left<W_{\text{query}}^{(h)}\boldsymbol{e}_{j},W_{\text{key}}^{(h)}\boldsymbol{e}_{k} \right>
$$
where $W_{\text{query}}^{(h)}$ and $W_{\text{key}}^{(h)}$ are transformation matrices which map the original embedding space into a new space.
The representation of feature $j$ in subspace $h$ is then updated by combining all relevant features guided by coefficients $\alpha_{j,k}^{(h)}$:
$$
\tilde{\boldsymbol{e}}_{j}^{(h)}=\sum_{k=1}^{K}\alpha_{j,k}^{(h)}W_{\text{value}}^{(h)}\boldsymbol{e}_{k}
$$
$\tilde{\boldsymbol{e}}_{j}^{(h)}$ is a combination of feature $j$ and its relevant features under attention head $h$.
Therefore it is a learned combinatorial feature.
Since a feature can be involved in various different combinations, we use multiple heads to extract combinations, *i.e.* $\{\tilde{\boldsymbol{e}}_{j}^{(h)}\}_{h=1}^{H}$.
[@Song2018] used H=2.
All of these combinatorial features are concatenated into a single vector, $\tilde{\boldsymbol{e}}_{j}$.
Then finally the output is combined with its raw input (residual connection) and sent through a ReLU:
$$
\boldsymbol{e}_{j}^{\text{res}}=\text{ReLU}\left(\tilde{\boldsymbol{e}}_{j}+W_{\text{res}}\boldsymbol{e}_{j}\right)
$$
This mapping from $\boldsymbol{e}_{j}$ to $\boldsymbol{e}_{j}^{\text{res}}$ is done for each feature to form the interacting layer.
The interacting layer is thus a representation of high-order features.
These interacting layers can be stacked on-top of each other to form arbitrary order combinatorial features.

[@Wang2017b] uses the cross-network which is an automated way of building cross-features.
Each layer produces higher-order interactions based on existing ones, and keeps the interactions from previous layers.
The cross-network is trained jointly with a DNN.

[@Wang2017b] makes a case for finding a bounded-degree feature interactions, saying that all the Kaggle competions are won with feature engineering of low-degree interactions, whereas DNNs learn highly non-linear interactions implicitly.
[@Wang2017b] cross-network consists of cross-layers that can be formalised as:
$$
\boldsymbol{x}_{l+1}=\boldsymbol{x}_{0}\boldsymbol{x}_{l}^{\intercal}\boldsymbol{w}_{l}+\boldsymbol{b}_{l}+\boldsymbol{x}_{l}
$$
where $\boldsymbol{x}_{l}$ is the output of the $l$-th cross layer; $\boldsymbol{x}_{0}$ is the input vector; $\boldsymbol{w}_{l}$ and $\boldsymbol{b}_{l}$ are its associate weight and bias paramters respectively.
Each cross layer adds back its input after feature crossing in a residual connection fashion.
[@Wang2017b] experimented with 1-6 cross layers.
The degree of cross features grows with cross-network depth.
The DNN trained in parallel is just a simple network with fully-connected layers and ReLUs.
The output of the two streams are concatenated, send through a fully connected layer and a sigmoid layer.

[@Qu2016] used something called a product layer, which takes pairwise inner or outer products of all feature combinations and concatenates it to all linear combinations.
The output is then fed to 2 fully-connected layers.
According to [@Guo2017] it is necessary to capture both low and high-order interactions (and wide&deep paper).
They also have to parallel streams of networks, one the FM capturing the low order interactions and one the DNN captuting the high-order interactions.

[@Cheng2016] believes it is both important to learn to memorise and generalise.
Where memorise refers to recalling from known observations and generalise to predict accurately on unseen samples.
They attempt to achieve this again with two streams, one linear layer (wide) and one deep network (deep).
The wide stream learns to memorise and the deep network learns to generalise.
Combined by a weighted sum.


### Skip-Connection

[@Song2018] shows that residual connection gives better results.
They also combine a residual connection between layers so that different orders of feature interactions can be combined.

[@Wang2017b] also used a residual connection.

### SeLU

DNNs on tabular data also struggle to go deeper than 3 or 4 layers.

Fully connected model structure leads to very complex optimization hyper-planes with a high risk of falling into local optimums.

Batchnorm:  [@Wang2017b]

The batch normalisation layer attempts to normalise neuron activations to zero mean and unit variance [@Ioffe2015].
It has become the stanard when training deep CNNs.
Training with normalisation techniques is perturbed by stochastic gradient descent, stochastic regularisation (like dropout) and the estimation of the normalisation parameters.
Both RNNs and CNNs can stabilise learning via weight sharing, therefore they are less prone to perturbations.
Fully-connected NNs do not have this luxury and shows high variance in the training error when trained with normalisation techniques.

Fully-connected DNNs with normalisation techniques are very sensitive to perturbations.
DNNs exhibit a high variance in training error when trained using BatchNorm.
This hinders the learning process.
Combined with dropout just magnifies the effect.
[@Klambauer2017] suggested the use of SeLUs which is an activation function which helps the network to maintain zero mean and unit variance activations.
By using this activation, there is no need for a BatchNorm layer.
SNNs do not suffer from exploding or vanishing gradients.
They paper tested on 123 tabular datasets to show that on average SNNs are the best.
But it is quite finicky to get the implementation right.
It requires a very specific weight initialisation, one that does not really make sense for embedding matrices.
And even when you get it right, the improvement is not necessarily significant.
That said, it has not been tested by others on tabular data, so it is worth a try.
$$
\text{selu}(x)=\lambda 
\begin{cases}
x & \text{if}~~~x>0\\
\alpha e^{x}-\alpha & \text{if}~~~x\le 0\\
\end{cases}
$$
Give more detail if it is proved to be useful.
Also needs a specific type of dropout.

[@Klambauer2017] tested SELUs on 121 classification datasets from the UCI Machine Learning repository.
They compared DNNs with SELU activations to other DNNs and other classifiers like Random Forests and SVMs.
They found that on the datasets with less than 1000 obervations, random forests and SVMs performed the best.
However, for the datasets with more than 1000 observations, DNNs with SELU activations performed the best overall.
The classifiers were compared by ranking them by their accuracy for each prediction task and doing a pairwise Wilcoxon test.

Another thing the authors found when comparing SELUs with other activations is that the model selection approach for SELU DNNs resulted in much deeper networks than DNNs with other activations.


## Sample Effeciency \label{sec:samp_eff}

It is well know that DNNs require a large amout of data to generalise well.
Typically, tabular datasets are not as large as unstructured datasets like images and texts.
There is also no large tabular dataset from which knowledge can be transferred from like ImageNet for computer vision and wikipedia for NLP.
We suggest two techniques for overcoming this problem: data augmentaion and unsupervised pre-training.
[@Zhang2016] also did pretraining with DAEs.
DAEs enforce robustness to partially destroyed inputs.
Can also be view from a manifold learning perspective [@Vincent2008].
Should also consider VAE and GANS
[@anonymous2019] uses output from GBDT to train an initial model and then to use it a initialisation of the actual model.
They call it the transfer of structured knowledge.
Data augmentation for tabular datasets is rarely studied.
Can use corruption like DAEs or swap noise but then creates inputs that does not exist in the real data distribution.

### Unsupervised Pretraining

Feature learning problem is also relevant here like in \Sref{sec:feat_int}.

[@Miotto2016] used a stacked denoising autoencoder to learn patient representations from EHR data.
They found that these representations were useful features in predicting future health states of patients.
By using these learned representations as input significantly improved the performance of predictive models compared to those only using the raw inputs.

[@Miotto2016] presented a novel unsupervised deep feature learning method to derive a general-purpose patient representation for EHR data that facilitates clinical predictive modelling.
A stacked denoising autoencoder was used.
Unsupervised feature learning attempts to overcome limitations of supervised feature space definition by automatically identifying patterns and dependencies in the data to learn a compact and general representation that make it easier to automatically extract useful information when building classifiers or other predictors [@Miotto2016].
These techniques are very familiar and effective in text, audio and image processing, but not with tabular data.
[@Geras2014] on gradual increasing of corruption ratio; but applied to images.


### Data Augmentation

- Blankout
- Swap Noise
- [@kasar2018] - hybrid bootstrap
- using GANS are interesting by out of scope.

Tabular data is very different to image data and the standard augmentations used in computer vision does not make sense with tabular data.
You cannot rotate or scale an observation from a tabular data without losing its meaning.
One transformation that does make sense for tabular input is the injection of random noise.

When working with images, we can randomly perturb the pixel intensities by a small amount so that it is still possible to make sense of its content.
By adding 1 for example to all pixels and all colors in an image, will only make it slightly brighter and we will still be able to make sense of it.
Bu with tabular data we can just randomly add a small amount to any feature.
The input features will probably not all be on the same scale and the addition of noise might result in a feature value that is out of the true distribution. 
In addition, it does not make sense to add anything to a discrete variables.
Thus in order to inject random noise to a tabular data sample, the noise should be scaled relative to each input feature range and the results should be a valid value for that feature.
This also helps the model to be more robust to small variations in the data.

[@VanDerMaaten2013] suggests an augmentation approach that does this called Marginalised Corrupted Features (MCF).
The MCF approach adds noise to input from some known distribution.

In the original Denoising Autoencoding papaer [@Vincent2008], they used a blank-out corruption procedure.
Which is randomly selecting a subset of the input features and mask their values with a zero.
The only conceptual problem with this approach is that for some features a zero value actually carries some meaning, so a suggestion is to blank-out features with a unique value not already belonging to that feature distribution.

Another input corruption approach shown to work empirically [here](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629) is what is called Swap Noise [@kasar2018].
The swap noise procedure corrupts inputs by randomly swapping input values with those of other samples in the datasets.
In this way you ensure that the corrupted input at least have valid feature values.
But it still might produce combinations of features that are not actually possible.

All of these methods have hyperparameters that needs to be set. I haven't gone into detail as I still need to decide what is relevant to this thesis.

```{r}
df = read_csv('../data/adult/adult_sample.csv')
new_row <- df[4,]
new_row[1] <- df[1,1]
new_row[5] <- df[5,5]
df <- rbind(df, new_row)

df[1,1] <- cell_spec(df[1,1], color='blue')
df[5,5] <- cell_spec(df[5,5], color='green')
df[7,1] <- cell_spec(df[7,1], color='blue')
df[7,5] <- cell_spec(df[7,5], color='green')
df[7,2] <- cell_spec(df[7,2], color='red')
df[7,3] <- cell_spec(df[7,3], color='red')
df[7,4] <- cell_spec(df[7,4], color='red')
df[7,6] <- cell_spec(df[7,6], color='red')

kable(df, 'latex', caption = 'Swap Noise Example', booktabs=TRUE, escape = FALSE) %>% 
  kable_styling(position = 'center') %>% 
  row_spec(4, color='red') %>% 
  group_rows('Original Dataset', 1,6) %>% 
  group_rows('Sample with swap noise', 7,7)
```


**Mixup**. 
The way mixup creates artifical samples is by the following original formulation:
$$
\tilde{\boldsymbol{x}}=\lambda\boldsymbol{x}_{i}+(1-\lambda)\boldsymbol{x}_{j}\\
\tilde{\boldsymbol{y}}=\lambda\boldsymbol{y}_{i}+(1-\lambda)\boldsymbol{y}_{j}\\
$$
where $\boldsymbol{x}$ is a input vector,$\boldsymbol{y}$ a one-hot encoded output vector and $\lambda\in[0,1]$.
$(\boldsymbol{x}_{i}, \boldsymbol{y}_{i})$ and $(\boldsymbol{x}_{j}, \boldsymbol{y}_{j})$ are two samples drawn at random from the training data.
Thus mixup assumes that linear interpolations of input vectors lead to linear interpolations of corresponding targets.

$\lambda$ controls the strength of the interpolation between input-output pairs.
The closer $\lambda$ is to 0 or 1, the closer the artificial sample will be to an actual training sample.
The authors suggest using $\lambda\sim\text{Beta}(\alpha, \alpha)$ for $\alpha\in(0,\infty)$.
They observed best performance when $\alpha\in[0.1,0.4]$ and if $\alpha$ is too high, they experience underfitting.

Other ablation studies they did was to find at which stages of the network the interpolation should happen, *e.g.* raw input, after embedding, hidden layer, *etc.*
But the experiments are not extremely clear and therefore warrants further discussion here.

Typically data augmentation procedures are dataset dependent and therefore requires expert knowledge.
It is hard to think of ways to augment tabular data, even more so a generic way of doing so.
However, from this defnition it is clear that mixup can be used on any type of data, including tabular datasets.

Mixup data augmentation can be understood as a mechanism to encourage the model to behave linearly in-between training samples.
[@Zhang2017] shows that this linear behaviour reduces the amount of undesirable variation when predicting new samples further away from the training samples.
They also argue and show empirically how training with mixup is more stable in terms of model predicions and gradien norms.
This is because mixup leads to decision boundaries that transition linearly between classes, resulting in smoother predictions.

The authors [@Zhang2017] tested mixup data augmentation on tabular datasets.
They tested it on 6 classification datasets from the UCI Machine Learning repository.
They used a 2-layer MLP with 128 neuros each and a batch size of 16.
They found that mixup improved the performance on 4 out of the 6 datasets.

See \autoref{fig:simple_dataset_mixup}.

![Illustration of points created by mixup augmentation.\label{fig:simple_dataset_mixup}](figures/simple_dataset_mixup.pdf)

### Regularisation Learning

A difference between the two data types that stand out is the relative importance of each of the important features with respect to the target. 
In computer vision a large amount of pixels should change before an image is of something else.
Whereas in tabular data a very small change in a single feature may have totally different behaviour with respect to the target [@Shavitt2018].
The same authors mention that this can be addressed by including a separate reqularisation term for each of the weights in the network. 
These regularisation terms are seen as additional model hyperparameters.
It is easy to see that this approach is totally intractable since the only way to train these hyperpararmeters are brute force and repetitive tweaking and validatig (derivative free methods).
A workaround is to make these regularisation parameters trainable like all of the other points in the network.
This is achieved by minimising the counterfactual loss, a novel loss function proposed by [@Shavitt2018].
They found that training NNs by optimising the counterfactual loss, outperform other regularisation approaches for NNs and results in NNs that are comparable to gradient boosted trees.
The learned regularisation parameters can even help with interpretting feature importance.

### Dropout

[@Zhang2016] compared dropout with L2 and found dropout to be better.

Interesetingly, [@haldar2018] found that dropout was not effective in their application.
They pinned it down to dropout producing invalid input scenarious that distracted the model.
Therefore they opted for hand crafted noise shapes taking into account the distribution of the relevant feature. 

Dropout: 0.5 [@Song2018], tuned [@Zhang2016] [@Qu2016] [@Guo2017] found dependent on dataset and model.

Other regularisation: $L_{2}$ penalty [@Song2018], [@Wang2017b], [@Zhang2016], [@Qu2016] mini-batch aware l2 for large inputs [@Zhou2017],

## Interpretation \label{sec:interp}

Plot embeddings
Plot attention
Methods discussed in previous chapter.

**Interpreting DNNs.** [@Song2018]
Model explainability is important for various reasons.
Helps to know how to improve your model or where it goes wrong.
Like a sanity check.
If you cannot explain how a prediction is made, you cannot know how good it is.
[@Song2018] uses the multi-head self-attention mechanism to evaluate correlations between features - globally and locally .
[@Zhou2017] plots attention and categorical embeddings.

[@haldar2018] notes that the permutation test only produces sensical results on the assumption that the features are independent.
Permuting the feature independently created examples that never occurred in real life, and the importance of features in that invalid space sent us in the wrong direction. 
The test however is somewhat useful in determining features that were not pulling their weight.
If randomly permuting a feature did not affect the model performance at all, it was a good indication that the model is probably not dependent on it.


## Other

### Hyperparameter Selection

- 1cycle not used with tabular data before

**Choosing DNN structural hyperparameters.**
Since there are no shared patterns among the diverse tabular datasets, it is hard to design a universal architecture that will fit all.
Most of these parameters are very dependent on the dataset and other modeling choices and therefore the need to tune them.
Structural hyperparameters are usually found using some brute search.
[@Song2018] used a sigmoid layer for binary classification.
Hidden layer size: 32 [@Song2018]
Number of hidden layers: 32
[@Song2018] experimented to see how many layers they should choose [1,2,3,4].
[@Wang2017b] tested number of layers [2-5]
[@Wang2017b] tested hidden layer sizes at [32-1024]
[@Zhang2016], [@Qu2016], [@Guo2017], [@Covington2016] experimented for number and size of layers.
Activation functions: tanh [@Zhang2016], tanh vs sigmoid vs relu (tanh and relu depending on dataset) [@Qu2016], relu vs tanh (relu better) [@Guo2017]
Shapes: Diamond, constant, increasing, decreasing [@Zhang2016] found that Diamond shape works best. found constant to work the best [@Guo2017]

**Choosing DNN learning hyperparameters.**
Loss function: logloss with $L_{2}$ penalty [@Song2018], [@Wang2017b], [@Zhang2016], [@Qu2016] mini-batch aware l2 for large inputs [@Zhou2017],
Batch size: 1024 [@Song2018] 512 [@Wang2017b]
Optimiser: Adam [@Song2018], [@Wang2017b]
Gradient clipping: [@Wang2017b]
Learning rate: 0.001-0.0001 [@Wang2017b], [1, 0.1, ..., 0.0001] [@Zhang2016]
Early stopping: [@Song2018], [@Wang2017b], [@Zhang2016]


## Still to categorise

[@Zhou2017] is very similar to the rest of these citations.
